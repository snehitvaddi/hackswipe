[
  {
    "title": "Raspberry Pi Music Player",
    "summary": "The Raspberry Pi Music Player is a versatile audio streaming solution that leverages the capabilities of Raspberry Pi to play music from various sources, including local files and online platforms like YouTube Music. By integrating Python scripts and shell commands, the player provides an accessible interface for users to manage and enjoy their music seamlessly.\n\nKEY ACHIEVEMENTS\nThis project distinguished itself by effectively combining multiple technologies and platforms to create a user-friendly music player that functions both locally and online. Winning the Emerging Talent Award highlights the innovative approach and technical prowess demonstrated by the team, showcasing their potential in the tech community.\n\nNotable technical implementations include the use of Python for backend scripting, ffmpeg for audio processing, and yt-dlp for fetching media from YouTube. The project also utilizes realvnc and SSH for remote access, enhancing usability and control over the music player environment. Furthermore, the integration with iOS Shortcuts allows for easy automation and enhanced user interaction.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to hardware limitations of the Raspberry Pi, such as processing power and audio output quality. Additionally, ensuring compatibility with various audio formats and online streaming protocols may have posed technical hurdles. Furthermore, creating a smooth user experience and interface across different platforms could have been a complex task.\n\nFUTURE POTENTIAL\nThe Raspberry Pi Music Player could evolve by incorporating additional features such as multi-room audio support, enhanced integration with smart home devices, and personalized music recommendations using machine learning algorithms. Expanding support for more streaming platforms and developing a dedicated mobile app could also broaden its user base and functionality.\n\nWhat it does: The Raspberry Pi Music Player is a voice-controlled music system that enables hands-free music playback through Siri integration. Here's what it can do: Voice Activation: Play music by simply saying \"Hey Siri, play my night music\"\nDual Music Sources: Local playback from USB storage (10 pre-loaded songs)\nCloud streaming from YouTube Music Automatic Timer: Music stops automatically after a set duration (perfect for sleep listening)\nHands-Free Operation: No need to touch your phone or computer\nRemote Control: Uses SSH commands triggered by iOS Shortcuts app\n\nInspiration: Each night I‚Äôd repeat the same annoying process of unlock‚ÄÇphone, connect to Bluetooth speaker, open my music app, find list of music etc. This was a process of around 5-6 steps, just to get music playing, and I didn‚Äôt like having the brightly lit screen of my phone while trying to drift off to sleep. I said to myself: ‚ÄúThere has got to be an easier way.‚Äù I wanted to make a tool that would be useful and solve an actual‚ÄÇproblem in my life. At that point I‚ÄÇknew what could solve it: A voice-controlled music player using a Raspberry Pi that plays music with just a simple Siri command.Yeah, instead of multiple steps we are now reduced to only one voice direction.\n\nHow it was built: Hardware Setup\nThe physical setup consists of:\nRaspberry Pi 3 ‚Üí Audio Cable ‚Üí Amplifier ‚Üí Wired Speaker\nInitial attempt: I tried using a Bluetooth speaker for wireless convenience, but the connection kept dropping. Solution: Switched to a wired setup for stable, reliable audio output.\nSoftware Implementation\nStep 1: Local Music Library\nCreated a bash script for playing local music files:\nbash#!/bin/bash\ntimeout 1800 cvlc --play-and-exit /home/pi/Music/*.mp3 Copied 10 favorite songs via USB\nRenamed files systematically: 01.mp3, 02.mp3, ..., 10.mp3 to avoid typos\nUsed timeout command for automatic shutoff after 30 minutes (( 30\\ times 60 = 1800 \\ ) seconds) Step 2: YouTube Music Integration\nCreated a Python script (yt-music-tunes.py) for cloud streaming:\npython#!/usr/bin/env python3\nimport s\n\nChallenges: Bluetooth Connectivity Issues\nProblem: Bluetooth speaker randomly disconnected, even when appearing connected.\nSolution: Switched to wired connection. Sometimes the simpler solution is the better one‚Äîstability beats wireless convenience.\nFixed Duration Settings\nProblem: Changing the timer duration required manually editing code. To change from 30 to 45 minutes, I had to: Open the script\nChange 1800 to ( 45 \\times 60 = 2700 )\nSave and exit Impact: Not convenient for varying listening times. YouTube Download Lag\nProblem: Streaming from YouTube causes 10-20 second startup delay while audio downloads.\nTrade-off: Accepted this as the cost of accessing unlimited music library vs. instant playback from local files.\nManual URL Entry Per Song\nProblem: Each different song requires editing the Python\n\nAccomplishments: Successfully reduced complexity: Transformed 5-6 phone operations into a single voice command.\nAchieved true hands-free control: No need to touch phone or computer to play music.\nBuilt a hybrid system: Combined local storage (instant playback) with cloud streaming (unlimited variety).\nImplemented automatic timer: Perfect for nighttime listening without worrying about manually stopping music.\nLearned hardware integration: Successfully connected and configured audio hardware components.\nMastered remote execution: Integrated multiple technologies (Raspberry Pi, iOS, SSH, Python, bash) into one cohesive system.\nCreated a practical solution: The system works reliably and has become part of my daily routine. The most rewarding moment was the first time I said \"Hey Siri, play my night music\" and\n\nWhat we learned: Technical Skills Hardware Integration: How to connect audio components and work with amplifiers\nBash Scripting: Writing shell scripts for system automation\nPython Programming: Using subprocess module and working with command-line tools\nNetwork Communication: Setting up SSH connections and executing remote commands\nAPI Integration: Connecting iOS Shortcuts with Raspberry Pi via SSH Problem-Solving Approaches Flexibility is key: When Bluetooth failed, switching to wired was the right choice\nSimplicity over complexity: Sometimes the most straightforward solution (wired connection, numbered file names) works best\nTrade-offs are necessary: Accepted download lag for YouTube streaming in exchange for unlimited music access\nIterative development: Started with local playback, then added streaming f\n\nWhat's next: Dynamic Voice Timer Control\nInstead of editing code, implement Siri-based duration input: User says: \"Play music for 45 minutes\"\nShortcut extracts the number (45)\nCalculates seconds: ( 45 \\times 60 = 2700 )\nPasses as parameter to the script Interrupt Capability\nCreate a \"Stop music\" Siri command:\nbashssh pi@192.168.x.x \"killall vlc\"\nDynamic URL Input\nSiri shortcut prompts: \"What's the YouTube URL?\" User provides URL via voice or paste\nShortcut passes URL as parameter to Python script\nNo more manual code editing Full Playlist Support\nModify Python script to handle YouTube playlist URLs: Automatically parse and download all songs in playlist\nPlay in sequence\nEnable \"shuffle\" option via voice command Advanced Features Web Interface\nBuild a simple web dashboard for easier configuration: Browse",
    "hackathon": null,
    "prize": "Winner Emerging Talent Award",
    "techStack": "bash, ffmpeg, ios, ios-shortcuts, python, raspberry-pi-os, realvnc, ssh, vlc, youtube-music, yt-dlp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=VIDEO_ID",
    "demo": null,
    "team": "Rimi Yoshikawa",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/raspberry-pi-music-player"
  },
  {
    "title": "VoxSol",
    "summary": "VoxSol Project Summary\n\nVoxSol is an innovative project that leverages blockchain technology to create a decentralized platform for voice interactions and audio content sharing. By utilizing Solana's fast transaction capabilities, it aims to enable users to securely create, share, and monetize audio assets in a community-driven environment.\n\nKEY ACHIEVEMENTS: VoxSol stood out by demonstrating exceptional integration with the Solana blockchain, showcasing its scalability and speed for real-time audio transactions. It captured the attention of judges by combining advanced audio technology with user-friendly interfaces, leading to its recognition as both the \"Winner\" and \"Best Use of Solana.\"\n\nThe project features a robust tech stack including Next.js for server-side rendering, Tailwind CSS for responsive design, and Three.js for immersive 3D audio visualizations. It also integrates ElevenLabs for AI-driven voice synthesis and utilizes Solana's web3.js library for seamless blockchain interactions, ensuring a smooth user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to scalability as user adoption grows, ensuring low latency in audio interactions, and managing the complexities of integrating various technologies while maintaining a cohesive user experience. Additionally, navigating the regulatory landscape surrounding audio content and monetization may have posed hurdles.\n\nFUTURE POTENTIAL: VoxSol has significant potential for evolution by expanding its features to include collaborative audio creation tools, enhanced monetization options for creators, and integration with other decentralized platforms. The project could also explore partnerships with content creators and audio platforms to broaden its user base and enhance community engagement.\n\nWhat it does: VoxSol is a voice-first Solana wallet copilot. You can: Check your portfolio balance by speaking\nExecute real token swaps on Solana devnet\nView live price charts\nGet AI-powered responses that understand DeFi context Everything is non-custodial - you connect your own wallet and sign every transaction yourself.\n\nInspiration: Managing a crypto wallet shouldn't require staring at complex interfaces. I wanted to create something that felt as natural as asking Siri or Alexa for help - but for Solana DeFi. What if you could just say \"swap 0.5 USDC to SOL\" and have it happen?\n\nHow it was built: Frontend: Next.js 14 with Tailwind CSS for a sleek, dark-themed UI\nAI Chat: Gemini API processes natural language and understands DeFi commands\nVoice Input: Web Speech API for speech-to-text\nVoice Output: ElevenLabs API for natural text-to-speech responses\nBlockchain: Solana devnet with @solana/web3.js for real on-chain transactions\nWallet: Phantom, Solflare, Backpack integration via wallet adapter\nSwap Execution: Custom vault that handles USDC ‚Üî SOL swaps at market rates\n\nWhat we learned: How to integrate voice interfaces with Web3 applications\nBuilding AI agents that understand financial context\nWorking with Solana transactions and token transfers\n\nWhat's next: AI Agent Allowance: Set spending limits for autonomous trading\nYield Vaults: One-click deposits into yield strategies\nMainnet deployment with Jupiter integration",
    "hackathon": null,
    "prize": "Winner Best Use of Solana",
    "techStack": "css, elevenlabs, gemini, gsap, next.js, phantom, react, solana, solana/web3.js, tailwind, three.js, typescript",
    "github": "https://github.com/KaranSinghBisht/VoxSol",
    "youtube": "https://www.youtube.com/watch?v=3Mows_n952M",
    "demo": "https://vox-sol.vercel.app/",
    "team": "Karan Bisht",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/voxsol"
  },
  {
    "title": "ComLab",
    "summary": "ComLab is a data-driven application designed to leverage the Snowflake API for enhanced data analytics and visualization. By combining technologies like MongoDB, Next.js, and React, the project aims to provide users with an interactive platform to explore and manipulate large datasets seamlessly.\n\nKEY ACHIEVEMENTS\nComLab stood out for its innovative use of the Snowflake API, allowing for efficient data querying and real-time analytics. The project‚Äôs clean user interface and responsive design contributed to its recognition as the winner in both the overall category and for Best Use of Snowflake API at the hackathon.\n\nNotable technical implementations include the integration of Snowflake for data warehousing, enabling fast and scalable data retrieval. The use of Next.js and React for the front-end ensures a dynamic and responsive user experience, while TypeScript enhances code maintainability and reduces runtime errors. Additionally, MongoDB serves as an effective database solution for handling unstructured data.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating multiple technologies, particularly ensuring smooth communication between the front-end and the Snowflake back-end. Additionally, managing data security and user authentication, especially with sensitive datasets, may have posed significant hurdles. Performance optimization for handling large datasets could also have been a concern.\n\nFUTURE POTENTIAL\nComLab has the potential to evolve into a comprehensive data analytics platform that could incorporate machine learning capabilities for predictive analytics. Expanding its functionalities to support real-time collaboration among users and integrating additional data sources could enhance its value. Moreover, fostering a community around the platform could lead to user-driven features and improvements.",
    "hackathon": null,
    "prize": "Winner Best Use of Snowflake API",
    "techStack": "javascript, mongodb, next.js, react, snowflake, typescript, vercel",
    "github": "https://github.com/aadittalele/comlab_app",
    "youtube": "https://www.youtube.com/watch?v=KK5LTfyfq14",
    "demo": "http://comlab.aadittalele.com/",
    "team": null,
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/comlab-an1uem"
  },
  {
    "title": "Story.ai",
    "summary": "Story.ai Project Summary\n\nStory.ai is an innovative storytelling platform that leverages AI to generate and enhance narratives. By utilizing advanced language models and voice synthesis technologies, the project enables users to create immersive and interactive stories tailored to their preferences.\n\nKEY ACHIEVEMENTS: The project stood out due to its unique integration of ElevenLabs for voice synthesis, allowing for dynamic audio storytelling. It also won two awards, including the overall winner and the best use of ElevenLabs, highlighting its exceptional implementation of cutting-edge technology and user engagement.\n\nStory.ai employs ElevenLabs for realistic voice generation and Gemini for natural language processing, enabling it to craft compelling narratives. The use of MongoDB for data storage ensures efficient management of user-generated content and seamless access to a rich database of stories.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the quality and coherence of AI-generated narratives, managing the scalability of the platform, and addressing user feedback to improve the storytelling experience. Additionally, integrating multiple technologies while maintaining performance could have posed technical hurdles.\n\nFUTURE POTENTIAL: Story.ai has significant potential for evolution by incorporating user-driven customization features, expanding into various genres, and enhancing interactivity through user choices. Moreover, partnerships with educational and entertainment sectors could broaden its application, making storytelling accessible and engaging for diverse audiences.\n\nInspiration: I was inspired by the magic of audiobooks and theater, and wanted to create a tool that could bring stories to life automatically. Reading a story is one thing, but hearing characters speak in unique, expressive voices makes it immersive and engaging. I asked myself: ‚ÄúWhat if a PDF story could become a full audio performance, with each character voiced differently?‚Äù That idea sparked this project. I also noticed that most existing text-to-speech tools use a single voice for all characters, which makes stories feel flat and hard to follow. I wanted to explore whether AI could understand the story context and generate distinct voices for each character, creating a richer, more human-like experience.\n\nHow it was built: Story.ai takes a PDF story and produces a multi-character narrated audio file, all accessible through a web interface. Here‚Äôs how it works: I used Python and PyPDF2 to extract clean, structured text from PDFs.\nI integrated the Google Gemini API to analyze the story and identify who is speaking in each line, distinguishing dialogue from narration and grouping consecutive lines by speaker to ensure smooth audio.\nI used ElevenLabs‚Äô Text-to-Speech API to generate realistic, expressive voices for each character and the narrator.\nI stitched all the generated audio clips together using Python libraries like pydub, producing a seamless audiobook.\nThe web interface allows users to upload PDFs, generate audio, and play it directly in the browser.\nI stored uploaded PDFs and generated audio files in M\n\nChallenges: Over-segmentation: Initially, the AI split every sentence into separate chunks, making the audio choppy. I solved this by updating prompts to group consecutive lines by the same speaker.\nVoice assignment: Ensuring each character had a distinct, expressive voice while keeping the sequence natural.\nAudio stitching: Combining multiple audio files into a seamless story without gaps or mismatched timing.\nPDF variability: Handling different PDF formats and layouts while extracting clean text.\nWeb integration: Coordinating API calls and audio playback in a user-friendly web interface. Despite these challenges, Story.ai successfully transforms static stories into immersive, multi-character audio experiences powered entirely by AI using Python, Google Gemini, ElevenLabs, MongoDB Atlas, and a web in\n\nWhat we learned: How to extract structured text from PDFs and prepare it for AI processing.\nHow to prompt an LLM (Google Gemini) effectively to identify speakers while grouping consecutive lines together.\nHow to generate multiple AI voices and merge them into a coherent audio narrative using ElevenLabs.\nHow to think about user experience, including pacing, dialogue flow, and listener immersion.\nHow to leverage MongoDB Atlas for storage and retrieval, ensuring scalability and reusability.\nHow to integrate all components into a web interface, allowing users to interact with the system directly.\n\nWhat's next: Scene-level visuals: Adding illustrations for each scene to enhance immersion.\nSubtitle syncing: Highlighting dialogue in real-time for accessibility and engagement.\nCharacter consistency across stories: Maintaining the same voices for recurring characters.\nEnhanced web features: Letting users share generated stories directly from the web interface.",
    "hackathon": null,
    "prize": "Winner Best Use of ElevenLabs",
    "techStack": "elevenlabs, gemini, mongodb",
    "github": "https://github.com/Ayushsinghal05/Story.ai",
    "youtube": "https://www.youtube.com/watch?v=SccU1kl_7ng",
    "demo": null,
    "team": "Ayush Singhal",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/story-ai-9tl7xf"
  },
  {
    "title": "DiaDoc",
    "summary": "DiaDoc is a web application designed to streamline document management by leveraging the Gemini API for enhanced functionality. The project aims to simplify the process of creating, sharing, and collaborating on documents, making it easier for users to manage their documentation needs.\n\nKEY ACHIEVEMENTS\nDiaDoc stood out by effectively integrating the Gemini API, earning it recognition as both the overall winner and for the Best Use of Gemini API at the hackathon. Its user-friendly interface and seamless navigation, built using modern web technologies, contributed to its success, showcasing the team's ability to deliver a polished and functional product within a limited timeframe.\n\nThe project was built using a robust tech stack that includes React for building user interfaces, Tailwind CSS for styling, and Vite for fast development. The integration of the Gemini API enabled dynamic document handling, while the use of JavaScript and HTML ensured responsive and interactive features, enhancing the overall user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in effectively integrating the Gemini API with their application, ensuring smooth data flow and user interactions. Additionally, optimizing performance and ensuring scalability while maintaining a clean codebase could have posed difficulties, especially within the hackathon's time constraints.\n\nFUTURE POTENTIAL\nDiaDoc has significant potential for evolution, including the addition of collaborative features like real-time editing and commenting. The team could also explore integrating AI tools for document analysis and automation, expanding its capabilities. Furthermore, by targeting specific user groups such as businesses or educational institutions, DiaDoc could tailor its features to meet diverse user needs, enhancing its market appeal.\n\nWhat it does: DiaDoc is a tool meant to help documentation writers come up with diagrams for their project‚Äôs file structure and to help hackers understand the file structure of existing software projects. Users can manually make file diagrams with a drag and drop interface, generate a file diagram given a public existing GitHub repository link, a link to a webpage containing project documentation or given the text draft of documentation for a software project, and receive the resulting file diagram in ASCII art. Users who want to go back to their file structures later on can download a .diadoc file of their file structure draft which can be opened by DiaDoc in the Manual Builder for future edits.\n\nInspiration: A couple of months ago, Lani was installing Magma, an open-source mobile core network solution, to her computer via its quick start documentation. It took her nearly 2 weeks to install it because it mostly contained command lines and she found herself so confused about the file structure of the project. Leah and Lani have faced similar issues when referring to documentation to install software, notably when installing the Flutter framework for app development and when encountering environment variables for the first time. It made them wonder if there could be a tool to aid in understanding a project‚Äôs file structure either step-by-step in documentation or given a project‚Äôs GitHub repository.\n\nHow it was built: DiaDoc was built in HTML, CSS and JavaScript with the React framework and uses the Gemini API for file structure generation in its ‚ÄúFrom Link‚Äù and ‚ÄúDocumentation Text‚Äù options. It has two repositories, one that is public, attached to this submission, and contains source code that can run the web application locally, and the other is a private repository cloned off of the public one by Vercel that holds .env files and a vercel.json file for web app hosting. It uses REST API requests to converse with the Gemini API and makes heavy use of the lucide-react, axios, and react-dom libraries and packages for clean icons, HTTP client, and document object model functions for React.\n\nChallenges: Building the app was one thing, hosting the app was a whole other issue. In the past both of us have only ever hosted static websites, and with tools like GitHub Pages and surge.sh, there was hardly ever an issue getting our projects up. With an existing web application, Vercel creates a clone of its repository (but private) to hold additional information needed to host it publicly. While this is a great security practice, it inadvertently caused problems. Two notable errors include one where Gemini would work fine locally but output a single error supposedly caused by the server while hosted. It turned out that we only had the API key locally so adding it to Vercel‚Äôs settings fixed it. The other error came from a library (axios) that Leah was confident we had in our project but our produc\n\nAccomplishments: Making a custom file extension! Originally, we thought that file extensions had to have a direct correlation to how they were being used, but it turns out most files boil down to txt files or c files functionally and that file extensions can be thought of an addition to a file‚Äôs name. It was also satisfying to finally work with React and Vercel, as we have developed apps before in nearly any other framework or tool (Dart/Flutter, pure HTML/CSS/JS, Python/Flask, Java/Android etc.) but never what was mostly recommended to us: React. Finally getting to use it made us realize why it was recommended to us in the first place.\n\nWhat we learned: Learned how to make a custom file extension and what file extensions really are\nExplored ways on how to incorporate the Gemini API into a web application\nFigured out how to create a drag-and-drop user interface for diagramming\nHow to create and send a file to one‚Äôs Downloads folder\n\nWhat's next: The file structure generation output is pretty inaccurate (for example, by giving DiaDoc a link to its own public repository, it will generate a file structure output a lot closer to more popular repositories from an organization also called DiaDoc) so figuring out ways on how to fine-tune the API for ASCII diagramming specific purposes would be our next main goal. We also came across a thread on Reddit where people shared similar thoughts on wanting a tool like this: https://www.reddit.com/r/datacurator/comments/1dzqtlp/what_tool_to_visualise_folder_structure/ and people have mentioned desiring features like being able to add metadata in addition to files and folder, the ability to add short descriptions for folder contents, and the ability to build templates for new projects (the latter",
    "hackathon": null,
    "prize": "Winner Best Use of Gemini API",
    "techStack": "api, css, gemini, html, javascript, react, tailwind, vercel, vite",
    "github": "https://github.com/LaniW/DiaDoc",
    "youtube": "https://www.youtube.com/watch?v=gYa0wN5x0Us",
    "demo": "https://diadocdeploy.vercel.app/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/diadoc"
  },
  {
    "title": "Nexo: Turn Code into Stories You Can See and Hear",
    "summary": "Nexo is an innovative project that transforms code into engaging visual and auditory stories, allowing users to understand programming concepts through multimedia storytelling. By combining code analysis with visualization and audio narration, it aims to make coding more accessible and enjoyable for beginners.\n\nKEY ACHIEVEMENTS\nNexo distinguished itself by winning two awards: the overall winner and the Best Beginner Hack. Its unique approach to demystifying code through storytelling not only enhances learning but also engages users in a way that traditional coding tutorials do not, thereby appealing to a wider audience.\n\nNotable technical implementations include the use of FastAPI for backend development, which facilitates quick and efficient API creation, and ElevenLabs for generating realistic voiceovers. The integration of MongoDB allows for effective data storage, while React and TypeScript enhance the user interface, providing a seamless experience for users. The project employs Docker for containerization, ensuring consistency across development and deployment environments.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring the accuracy of code interpretation and the quality of the generated stories. Balancing the complexity of technical concepts with engaging storytelling could also have been a hurdle, as well as managing the performance of multimedia content in real-time.\n\nFUTURE POTENTIAL\nNexo has significant potential for evolution, including the incorporation of more programming languages and frameworks, expanding its user base. Future developments could involve integrating interactive elements where users can modify code snippets and see real-time visual/audio updates, as well as expanding the platform to support collaborative storytelling among users. Additionally, partnerships with educational institutions could enhance its reach in coding education.",
    "hackathon": null,
    "prize": "Winner Best Beginner Hack",
    "techStack": "cloudflare, css, docker, elevenlabs, fastapi, gemini, github, mongodb, python, react, typescript, vite",
    "github": "https://github.com/Hacktown-BSB/Nexo",
    "youtube": "https://www.youtube.com/watch?v=WJ31fUvhq2c",
    "demo": null,
    "team": "Pedro Garcia Vilanova, Matheus Lemes Amaral, Tiago Bittencourt",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/nexo-the-audio-visual-gps-for-your-codebase"
  },
  {
    "title": "EcoLens",
    "summary": "EcoLens Project Summary\n\nEcoLens is an innovative web application designed to promote environmental awareness by providing users with actionable insights on sustainability practices. It leverages data visualization to help individuals and organizations track their ecological footprint and offers personalized recommendations for reducing their environmental impact.\n\nKEY ACHIEVEMENTS: EcoLens stood out by effectively combining user engagement with impactful data analytics, resulting in an intuitive interface that encourages eco-friendly behaviors. Its recognition as the \"Best Hack for Hackers\" reflects not only its technical prowess but also its social relevance and potential to drive change in user habits.\n\nThe project employs a robust tech stack, including HTML, JavaScript, Node.js, and React, to create a dynamic and responsive user experience. Notable implementations may include real-time data processing for ecological metrics, interactive data visualization components, and a user-friendly dashboard that simplifies complex environmental data into actionable steps.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as integrating various data sources for accurate ecological metrics, ensuring scalability of the application to handle a growing user base, and addressing user privacy concerns related to data collection. Additionally, achieving a balance between technical complexity and user experience may have posed difficulties during development.\n\nFUTURE POTENTIAL: EcoLens has significant potential for evolution, including the incorporation of gamification elements to further engage users, partnerships with environmental organizations for resource-sharing, and the expansion of features to include community-driven initiatives. Additionally, machine learning could be utilized to provide smarter, personalized recommendations over time, enhancing the platform's impact on sustainability efforts.\n\nWhat it does: EcoLens is an AI-powered sustainability intelligence platform that helps individuals and organizations understand and reduce their carbon footprint. Analyzes user behavior such as travel, energy usage, and lifestyle patterns\nEstimates carbon emissions using intelligent modeling\nProvides personalized, actionable recommendations\nTracks progress through sustainability scores and visual dashboards\nEncourages eco-friendly habits through gamification and challenges\nIn short, EcoLens turns complex environmental data into simple, meaningful insights that drive real-world change.\n\nInspiration: Sustainability is often discussed, but rarely understood at a personal level.\nMost people want to reduce their environmental impact, yet they don‚Äôt know where they are contributing the most or how to make meaningful changes.\nWe realized that sustainability data exists ‚Äî but it‚Äôs scattered, complex, and inaccessible to everyday users. This gap between intention and action inspired us to build EcoLens ‚Äî a platform that makes sustainability visible, measurable, and actionable for everyone.\n\nHow it was built: EcoLens was built as a software-first, scalable prototype using:\nFrontend: A responsive web interface displaying dashboards, charts, and insights\nBackend Logic: Rule-based and AI-simulated models to estimate carbon emissions\nData Layer: Mock datasets simulating travel, energy, and consumption behavior\nAI Engine: Generates personalized recommendations and predicts emission trends\nGamification Module: Badges, streaks, and sustainability scores to drive engagement\nWe focused on clean architecture, clarity, and storytelling ‚Äî ensuring the demo is intuitive and impactful even without real-world data integrations.\n\nChallenges: Translating abstract sustainability metrics into something understandable\nDesigning an AI system that feels intelligent without relying on real user data\nBalancing technical depth with simplicity for a hackathon demo\nMaking sustainability feel empowering, not overwhelming\nEach challenge pushed me to think more creatively and design with the user in mind.\n\nAccomplishments: Built a complete end-to-end sustainability intelligence platform in a short time\nSuccessfully translated complex climate data into actionable insights\nDesigned a system that is scalable from individuals to institutions\nCreated a solution that blends AI, UX, and environmental impact seamlessly\nMost importantly, we built something that can realistically drive behavioral change.\n\nWhat we learned: Sustainability solutions must be intuitive to create real adoption\nAI is most powerful when it simplifies decision-making, not complicates it\nGamification significantly improves engagement in serious domains\nClear storytelling is just as important as strong technology\n\nWhat's next: Our vision for EcoLens goes far beyond the hackathon:\nIntegrating real-world APIs (Google Maps, utility data, carbon databases)\nAdding enterprise-grade ESG reporting tools\nExpanding into campuses, smart cities, and corporate sustainability programs\nIntroducing community challenges and global impact leaderboards\nDeveloping a public sustainability score API\nEcoLens aims to become the standard platform for understanding and improving environmental impact.",
    "hackathon": null,
    "prize": "Winner Best Hack for Hackers",
    "techStack": "html, javascript, node.js, react",
    "github": "https://github.com/sagnikchatterjee450/ecolens",
    "youtube": "https://www.youtube.com/watch?v=VjEPoZH7p3U",
    "demo": "https://ecolens-delta.vercel.app/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/ecolens-vyokrh"
  },
  {
    "title": "NexusWatch",
    "summary": "NexusWatch Project Summary\n\nNexusWatch is a monitoring and alerting tool designed to enhance system observability and manage performance metrics effectively. It leverages a combination of Flask for backend services and Node.js with TypeScript for dynamic frontend interactions, enabling users to receive real-time insights into their system's health.\n\nKEY ACHIEVEMENTS: NexusWatch stood out by seamlessly integrating multiple technologies, demonstrating exceptional user experience and responsiveness. Its recognition as the winner of the hackathon and the \"Best Use of Vultr\" prize highlights its innovative approach and effective deployment capabilities within cloud infrastructure.\n\nThe project showcases notable technical implementations, such as the use of Flask for RESTful API development, efficient data handling with Node.js, and a robust frontend built with TypeScript that ensures type safety and enhanced maintainability. Additionally, its deployment on Vultr demonstrates effective cloud resource utilization.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to real-time data processing and latency management, ensuring that the monitoring system can handle high volumes of metrics without degradation of performance. Additionally, integrating diverse technologies and maintaining compatibility across different platforms may have posed significant hurdles.\n\nFUTURE POTENTIAL: NexusWatch has substantial future potential, including the possibility of expanding its features to include AI-driven analytics for predictive monitoring and automated incident response. Furthermore, the project could evolve to incorporate user-customizable dashboards and integrations with other popular monitoring tools, broadening its appeal and usability within various IT environments.\n\nWhat it does: NexusWatch is a global performance intelligence platform that tests website performance from multiple regions across the world and provides AI-powered insights and recommendations. It allows users to: Test any website's performance from 6 global regions simultaneously\nVisualize network paths with animated connection lines on an interactive map\nReceive AI-generated performance scores and reliability metrics\nGet actionable recommendations for optimizing global performance\nUnderstand how their service performs from different user perspectives\nThe platform transforms raw performance data into strategic business intelligence, helping companies make informed decisions about their global infrastructure.\n\nInspiration: In today's digital-first world, website performance directly impacts user experience and business success. We were inspired by the challenge that companies face when trying to understand how their services perform across different geographical regions. Existing solutions are either expensive enterprise tools or simplistic single-point tests that don't capture the global picture. We wanted to create something that leverages Vultr's global infrastructure to provide comprehensive performance insights with the power of AI.\n\nHow it was built: We built NexusWatch using a distributed architecture that leverages Vultr's global infrastructure: Backend Infrastructure:\nDeployed 6 Vultr cloud instances across Sydney, Amsterdam, Chicago, Frankfurt, Los Angeles, and Tokyo\nCreated a central orchestrator in Sydney that coordinates all performance tests\nUsed systemd for robust service management to ensure reliability\nTesting Technology:\nSwitched from ICMP ping to HTTP-based testing for more accurate web performance measurements\nBuilt custom Python agents that measure Time to First Byte (TTFB) for any website\nImplemented proper error handling and timeout management\nFrontend Experience:\nDeveloped a React TypeScript application with a modern dark theme\nCreated an interactive map using Leaflet with animated connection lines\nBuilt responsive co\n\nChallenges: Challenges we ran into Service Reliability: Initially, our agent processes would terminate when we closed SSH connections. We solved this by implementing systemd services for robust process management.\nTesting Major Services: We discovered that many large websites like Netflix and Amazon block ICMP ping requests. We pivoted to HTTP-based testing which is more relevant for web services and works with virtually any website.\nAPI Integration: Integrating the Vultr Inference API required careful prompt engineering to ensure the AI provided structured, useful responses rather than generic text.\nUI/UX Design: Creating a visualization that was both informative and visually appealing required multiple iterations to get the right balance between data density and readability.\nCross-Region Communicati\n\nAccomplishments: Accomplishments that we're proud of Complete Distributed System: We successfully built and deployed a distributed system spanning 6 global regions in just a few hours.\nReal AI Integration: We went beyond hardcoded recommendations to implement genuine AI-powered insights using Vultr's Serverless Inference.\nProfessional UI: We created a polished, responsive interface with smooth animations and thoughtful interactions that rivals commercial monitoring tools.\nRobust Architecture: Our system is built with production-quality practices including systemd services, proper error handling, and fallback mechanisms.\nSolving a Real Problem: We created a tool that addresses a genuine business need that companies typically pay thousands of dollars for with enterprise solutions.\n\nWhat we learned: Global Infrastructure: We gained deep insights into how global infrastructure affects web performance and the importance of geographic proximity to users.\nDistributed Systems: We learned firsthand about the challenges and solutions for building reliable distributed systems.\nAI Integration: We developed skills in prompt engineering and integrating AI services into web applications.\nModern Web Development: We honed our React TypeScript skills and learned to create complex visualizations with libraries like Leaflet.\nSystem Reliability: We learned the importance of robust service management and the difference between a quick hack and a production-ready system.\n\nWhat's next: Historical Tracking: We plan to add historical performance tracking to show trends over time and identify performance regressions.\nAlert System: We want to implement a notification system that alerts users when performance drops below specified thresholds.\nExpanded Testing: We're looking to add more testing locations and additional metrics beyond just latency, such as uptime monitoring and route tracing.\nCustom AI Models: We plan to fine-tune AI models specifically for performance analysis to provide even more accurate and domain-specific insights.\nIntegration with CI/CD: We envision integrating NexusWatch into development pipelines to automatically test performance during deployments.\nPublic API: We're considering offering a public API so other services can integrate NexusWatch's global p",
    "hackathon": null,
    "prize": "Winner Best Use of Vultr",
    "techStack": "flask, linux, node.js, typescript",
    "github": "https://github.com/YuvrajSHAD/NexusWatch",
    "youtube": null,
    "demo": "https://nexuswatch.yuvrajsingh302.workers.dev/",
    "team": null,
    "date": "2026-01-03",
    "projectUrl": "https://devpost.com/software/pentestlab"
  },
  {
    "title": "Current AI",
    "summary": "Current AI is a project designed to integrate AI capabilities into Discord, allowing users to interact with advanced AI models through a bot. It leverages various APIs to enhance user experiences, providing intelligent responses and functionalities that cater to the needs of the Discord community.\n\nKEY ACHIEVEMENTS\nThis project stood out by winning the \"Best Use of MongoDB Atlas\" prize, showcasing its effective utilization of database technology for scalable data management. Its overall innovation and user engagement strategies likely contributed to its recognition as a winner in the hackathon.\n\nKey technical implementations include the use of the discord-api and discord.py for seamless Discord integration, as well as the incorporation of Google Gemini AI for advanced AI capabilities. The project also employs MongoDB for robust data handling and Next.js for a responsive front-end experience, with Python and TypeScript ensuring efficient backend and frontend development.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to API integrations, particularly ensuring smooth communication between Discord and the various AI and database services. Additionally, managing user data privacy and ensuring compliance with Discord's policies would have been critical hurdles.\n\nFUTURE POTENTIAL\nCurrent AI has the potential to evolve into a comprehensive AI assistant for Discord, offering personalized experiences and additional features such as game integrations, moderation tools, or community engagement metrics. Future iterations could explore other social platforms, expanding its user base and functionality further.\n\nWhat it does: Current-AI is an AI-powered project management system that lives inside your Discord server. Just chat naturally, and watch the magic happen:\n\nInspiration: After attending over 21 hackathons, I've witnessed the same problem again and again: teams spend more time organizing than building. Picture this: It's 2 AM at a hackathon. Your team is exhausted. Someone asks, \"Who's working on the database?\" Silence. Another person chimes in, \"I thought you were doing that? I've been building the API!\" Now you have duplicate work, wasted hours, and mounting frustration. Sound familiar? The reality is brutal: üî¥ 6+ hours wasted just figuring out task assignments\nüî¥ Critical features forgotten because nobody tracked them\nüî¥ Team members duplicate work due to poor communication\nüî¥ Discord messages buried in endless scrolling\nüî¥ \"Who's doing what?\" becomes the most asked question I realized that Discord is where hackers live, but it's terrible for project ma\n\nHow it was built: Current-AI is a full-stack monorepo combining cutting-edge technologies:",
    "hackathon": null,
    "prize": "Winner Best Use of MongoDB Atlas",
    "techStack": "discord-api, discord.py, github-api, google-gemini-ai, mongodb, next.js, python, typescript",
    "github": "https://github.com/sherwinvishesh/Current-AI",
    "youtube": "https://www.youtube.com/watch?v=Y9Xt2FuiO6U",
    "demo": null,
    "team": "Sherwin Vishesh Jathanna",
    "date": "2026-01-04",
    "projectUrl": "https://devpost.com/software/current-ai"
  },
  {
    "title": "Renal Wellness:  3-Day Student Detox",
    "summary": "The \"Renal Wellness: 3-Day Student Detox\" project aims to promote kidney health and overall wellness among students through a structured detox program that leverages artificial intelligence. By integrating personalized nutrition and preventive care strategies, the initiative seeks to empower students to make informed lifestyle choices that support renal health.\n\nKEY ACHIEVEMENTS\nThis project stood out as a winner for its innovative approach to addressing student health issues related to renal wellness, combining technology with practical health solutions. Its recognition as the Renal-AI Grand Prize Winner highlights its effectiveness in utilizing AI to tailor health and nutrition advice, showcasing a significant potential impact on student health outcomes.\n\nThe project likely features advanced AI algorithms that analyze student health data to generate personalized detox plans. It may also incorporate user-friendly interfaces for students to track their health metrics and receive real-time feedback on their dietary choices, enhancing engagement and compliance with the detox program.\n\nPOTENTIAL CHALLENGES\nThe team may have faced challenges in ensuring the accuracy and reliability of the AI recommendations, particularly given the diversity of student health profiles. Additionally, recruiting participants and maintaining engagement throughout the 3-day program could have posed hurdles, especially in a busy student lifestyle.\n\nFUTURE POTENTIAL\nLooking ahead, this project could evolve into a comprehensive wellness platform that expands beyond a 3-day detox to include ongoing health monitoring, community support features, and partnerships with healthcare providers. By developing a scalable app, it could reach a broader audience and incorporate more health metrics, making it a vital resource for preventive care in student populations.",
    "hackathon": null,
    "prize": "Winner Renal-AI Grand Prize Winner",
    "techStack": "ai, artificialintelligence, healthcare, nutrition, preventivecare, renal-ai, studenthealth, wellness",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=QdbRx0ztb6U",
    "demo": "https://drive.google.com/drive/folders/1xWBm5AtiAPSJVm3yiFBrjXjbJFM8vJdx?usp=drive_link",
    "team": "Ayesha Riaz",
    "date": "2025-12-30",
    "projectUrl": "https://devpost.com/software/renal-wellness-3-day-student-detox"
  },
  {
    "title": "CultureAlive",
    "summary": "CultureAlive Project Summary\n\nCultureAlive is a web application designed to foster cultural exchange and awareness by showcasing diverse cultural practices, traditions, and stories from around the world. By connecting users with authentic cultural experiences, the project aims to promote understanding and appreciation of global diversity.\n\nKEY ACHIEVEMENTS: CultureAlive stood out by winning the overall hackathon and the Excellence in Social Impact Award, highlighting its significant potential for social impact. The project effectively addresses cultural preservation and education, resonating with judges who value initiatives that promote inclusivity and understanding.\n\nThe application is built using Python Flask for the backend, ensuring a robust and scalable framework. It incorporates modern web technologies such as Bootstrap for responsive design, CSS for styling, and JavaScript for interactivity. The use of dotenv for environment variable management enhances security, while groq and groq-llm suggest advanced data handling capabilities, potentially integrating AI for personalized cultural recommendations.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in curating and authenticating cultural content, ensuring it is representative and respectful of the various cultures featured. Additionally, scalability and user engagement would have been considerations as the application grows, requiring ongoing content updates and user feedback integration.\n\nFUTURE POTENTIAL: CultureAlive could evolve by expanding its features to include user-generated content, allowing individuals to share their own cultural stories and practices. Future iterations could also incorporate interactive elements such as virtual reality tours, workshops, or language exchange programs, further enhancing user engagement and cultural immersion.\n\nWhat it does: CultureAlive is a web application that uses AI-powered storytelling to explain cultural topics in simple and easy-to-understand language.\nUsers can enter a topic (such as a temple, festival, tradition, or historical concept), and the AI responds as a cultural storyteller, making complex information approachable and meaningful.\n\nInspiration: Culture is the soul of any community, yet many local traditions, temples, festivals, and stories remain undocumented or difficult to understand for younger generations. I was inspired to build CultureAlive to preserve and present cultural knowledge in a simple, engaging, and accessible way using AI.\nThe idea was to combine technology with heritage‚Äîallowing users to explore cultural topics through natural language explanations.\n\nHow it was built: Backend: Python with Flask AI Model: Groq LLM (LLaMA 3.1) Frontend: HTML, CSS, JavaScript Deployment: Render Environment Management: dotenv The Flask backend handles API requests and sends user input to the Groq LLM.\nThe AI processes the prompt using a cultural storytelling context and returns a clear, human-like explanation.\nThe project is deployed on Render, making it accessible publicly via the web.\n\nChallenges: Deployment issues: Handling environment variables securely on Render API authentication errors: Debugging invalid API key issues Template handling: Fixing Flask template path errors (TemplateNotFound) Error handling: Ensuring smooth user experience when the AI API fails Each challenge helped me better understand backend debugging, deployment workflows, and production-level configuration.\n\nWhat we learned: How to integrate LLMs into real-world applications Deploying Flask apps on cloud platforms Managing environment variables securely Debugging production errors Designing AI prompts for meaningful, user-friendly responses\n\nWhat's next: Multilingual cultural storytelling Voice-based narration User-contributed cultural stories Image and map integration for cultural sites",
    "hackathon": null,
    "prize": "Winner Excellence in Social Impact Award",
    "techStack": "backend-python-flask, bootstrap, css, dotenv, groq, groq-llm, html5, javascript, render",
    "github": "https://github.com/prajktanandurkar09/CultureAlive_Hack",
    "youtube": "https://www.youtube.com/watch?v=8nXVZgDNt1g",
    "demo": "https://culturealive.onrender.com/",
    "team": "Prajkta Nandurkar",
    "date": "2025-12-30",
    "projectUrl": "https://devpost.com/software/culturealive"
  },
  {
    "title": "Scholarship Finder",
    "summary": "Scholarship Finder Project Summary\n\nThe Scholarship Finder is a web application designed to help students locate and apply for scholarships that match their qualifications and needs. By leveraging voice recognition technology, it streamlines the search process, making it more accessible and user-friendly.\n\nKEY ACHIEVEMENTS: The project won the hackathon due to its innovative use of the Google Web Speech API, which allows users to search for scholarships using voice commands. This unique feature enhances user experience and accessibility, setting it apart from traditional scholarship search tools.\n\nThe implementation of the Google Web Speech API stands out as a notable technical achievement, enabling seamless voice recognition. Additionally, the use of HTML, JavaScript, JSON, and Python allows for effective front-end and back-end integration, ensuring smooth data retrieval and user interaction.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating voice recognition with the scholarship database, ensuring accurate understanding of diverse speech patterns, and addressing potential issues with data privacy and security.\n\nFUTURE POTENTIAL: The Scholarship Finder could evolve by incorporating machine learning algorithms to personalize scholarship recommendations based on user profiles. Expanding the platform to include features like application tracking, reminders, and a community forum for peer support could further enhance its utility and user engagement.\n\nChallenges: Unreliable AI output formatting\nSolved using strict JSON Schemas and JSON-only prompts. Alert fatigue\nSolved by classifying change severity and filtering low-impact updates. AI overreach\nSolved by limiting the model to recommendations only; execution remains in n8n.\n\nWhat we learned: How to safely integrate LLMs into automated workflows\nThe importance of schema validation for AI reliability\nHow MCP improves explainability and auditability\nHow orchestration tools turn AI agents into production systems\n\nWhat's next: Dashboard for historical change tracking\nSlack or webhook notifications\nMulti-language email support\nUser-defined alert rules\nDatabase-backed scholarship history",
    "hackathon": null,
    "prize": "Winner Browser Use",
    "techStack": "google-web-speech-api, html, javascript, json, python",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": "Moksha Shah",
    "date": "2025-12-23",
    "projectUrl": "https://devpost.com/software/scholarship-finder-59vkwp"
  },
  {
    "title": "Guidely",
    "summary": "Guidely is an innovative project designed to leverage the capabilities of the Gemini API alongside Google Cloud to provide personalized guidance and recommendations for users. It utilizes a React frontend to create a dynamic and user-friendly interface, enhancing the user experience and making the delivery of insights seamless.\n\nKEY ACHIEVEMENTS\nGuidely stood out by effectively integrating the Gemini API to deliver tailored recommendations, which impressed the judges and led to its recognition as the winner of the hackathon. Its successful implementation of cutting-edge technology and user-centric design contributed to its distinction, particularly in the category of Best Use of Gemini API.\n\nThe project showcases notable technical implementations, including:\n- Gemini API Integration: Leveraging advanced capabilities of the Gemini API for personalized content delivery.\n- Google Cloud Utilization: Employing Google Cloud for scalable backend services, ensuring reliability and performance.\n- React Frontend: Utilizing React to build a responsive and interactive user interface, enhancing user engagement.\n\nPOTENTIAL CHALLENGES\nThe team likely faced several challenges, such as:\n- API Limitations: Navigating potential limitations and constraints of the Gemini API while ensuring robust functionality.\n- Data Privacy: Addressing user data privacy concerns while handling personalized recommendations.\n- Scalability: Ensuring that the application remains performant and scalable as user demand grows.\n\nFUTURE POTENTIAL\nGuidely has strong potential for future development, including:\n- Feature Expansion: Adding more sophisticated algorithms for deeper insights and broader recommendation categories.\n- User Feedback Integration: Implementing user feedback mechanisms to refine recommendations and improve user satisfaction.\n- Market Adaptation: Expanding its applicability across different industries, such as education, healthcare, and personal finance, to reach a wider audience.\n\nWhat it does: Guidely is an AI-powered career mentor app that helps users build CVs, find jobs matching their skills, connect with nearby mentors, track active learning time, and complete daily tasks and challenges. It provides both text and voice guidance, gamifies learning experiences, and motivates users to achieve their career goals.\n\nInspiration: Many young professionals and students in Kigali struggle to find mentors, career guidance, and personalized learning opportunities. I wanted to create an AI-driven platform that empowers users to grow their careers through actionable guidance, mentorship, and skill development.\n\nHow it was built: We used Google AI Studio and integrated multiple AI models: Gemini 3 Pro for guidance and mentorship, Imagen 3 for visual assets, and Nanobana for task and challenge generation. The app supports voice and text recognition, social login, user profiles, and a personalized task system to guide users step by step.\n\nChallenges: Designing an intuitive UI for complex AI features, ensuring relevant mentor and job matches, integrating real-time voice guidance, and gamifying career development were the main challenges.\n\nAccomplishments: We created a platform that acts like a personal mentor, offering actionable guidance, gamified challenges, skill tracking, and mentorship connections‚Äîall in one app.\n\nWhat we learned: We gained experience integrating AI models, designing user-friendly interfaces for complex features, and creating systems that motivate and engage users effectively.\n\nWhat's next: We plan to expand mentor and job networks, add more gamified learning experiences, and enhance AI personalization to make the platform even more effective in guiding users‚Äô careers.",
    "hackathon": null,
    "prize": "Winner Best Use of Gemini API",
    "techStack": "gemini, google-cloud, react",
    "github": "https://github.com/aibrahiam/Guidely",
    "youtube": "https://www.youtube.com/watch?v=NfyhKd3D2to",
    "demo": null,
    "team": "Ahmed Ibrahim, Mr Adwok",
    "date": "2025-12-23",
    "projectUrl": "https://devpost.com/software/guidely"
  },
  {
    "title": "InterLink",
    "summary": "InterLink Project Summary\n\nInterLink is a collaborative platform designed to facilitate real-time communication and project management among users. It harnesses various web technologies to enable seamless interactions, allowing teams to work together effectively regardless of their physical locations.\n\nKEY ACHIEVEMENTS:  \nInterLink stood out by winning both the overall competition and the People's Choice award, indicating strong community support and recognition for its innovative approach to collaboration. Its user-friendly interface and robust functionality likely contributed to its appeal, showcasing its potential to enhance teamwork.\n\nThe project leverages a modern tech stack, including Node.js and Express.js for backend development, and React and Next.js for a responsive frontend. The use of Socket.io enables real-time communication, while MongoDB serves as a flexible database solution. Tailwind CSS enhances the user interface design, ensuring a visually appealing experience.\n\nPOTENTIAL CHALLENGES:  \nThe team likely faced challenges related to integrating various technologies smoothly, ensuring real-time data synchronization, and maintaining performance under load. Additionally, user experience testing to refine the interface and workflow could have posed hurdles during development.\n\nFUTURE POTENTIAL:  \nInterLink has significant potential for evolution, including the addition of features such as enhanced project analytics, integration with third-party tools, and expanded user customization options. Exploring mobile app development could also broaden its accessibility, making it a versatile tool for remote teams.",
    "hackathon": null,
    "prize": "Winner People's Choice",
    "techStack": "css, express.js, gemini, javascript, mongodb, next.js, node.js, radix, react, socket.io, tailwind, typescript",
    "github": "https://github.com/neil-agarwal24/InterLink",
    "youtube": null,
    "demo": null,
    "team": "Neil Agarwal, Ayush Iyer",
    "date": "2025-12-21",
    "projectUrl": "https://devpost.com/software/interlink-md3ouz"
  },
  {
    "title": "FluxDiagram",
    "summary": "FluxDiagram Project Summary\n\nFluxDiagram is a web-based application designed to create and visualize dynamic flow diagrams. Leveraging modern web technologies, it allows users to easily design, edit, and share complex diagrams, making it a valuable tool for brainstorming and project planning.\n\nKEY ACHIEVEMENTS: The project stood out for its intuitive user interface and seamless user experience, which facilitated quick diagram creation. Its innovative use of real-time collaboration features likely impressed judges, contributing to its recognition as a winner.\n\nBuilt with a robust tech stack including React and TypeScript, FluxDiagram utilizes CSS3 for responsive design and antigravity for enhanced visualization capabilities. The integration of these technologies ensures smooth performance and a visually appealing layout.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to ensuring cross-browser compatibility and performance optimization, especially with complex diagrams. Additionally, implementing real-time collaboration features could have posed synchronization and data integrity issues.\n\nFUTURE POTENTIAL: FluxDiagram has significant potential for evolution, such as incorporating AI-driven suggestions for diagram layouts or integrating with existing project management tools. Expanding features to support various diagram types and enhancing user collaboration can further increase its utility and market reach.\n\nWhat it does: FluxDiagram is a native VS Code extension that transforms your editor into a powerful flowchart builder. Core Features:\nüé® Intuitive Canvas ‚Äî Drag-and-drop nodes with smooth pan & zoom\nüîó Smart Connections ‚Äî Create edges between nodes with automatic path routing\nüìê Auto Layout ‚Äî Automatically organize messy diagrams with one click\nüó∫Ô∏è Minimap Navigation ‚Äî Navigate large diagrams effortlessly\nüåó Theme Aware ‚Äî Automatically adapts to your VS Code light/dark theme\nüì§ Multi-format Export ‚Äî PNG, SVG, and JSON export for any use case\n‚Ü©Ô∏è Full Undo/Redo ‚Äî Configurable history up to 200 steps\n‚å®Ô∏è Keyboard First ‚Äî Power-user shortcuts for every action\nDiagrams are saved as .fluxdiagram files that live alongside your code and can be version-controlled with Git.\n\nInspiration: Developers constantly need to visualize their logic ‚Äî whether it's documenting an algorithm, planning a system architecture, or explaining a workflow to teammates. But the current options are frustrating: switch to a browser-based tool like Lucidchart, lose context, and break your flow. Or use clunky desktop apps that don't integrate with your dev environment. We asked ourselves: Why can't we build flowcharts where we already spend most of our time ‚Äî inside VS Code? FluxDiagram was born from the belief that diagramming should be as seamless as writing code. No context switching. No exports to manage. Just open a file and start creating.\n\nHow it was built: FluxDiagram is built entirely with TypeScript and leverages the VS Code Custom Editor API for deep integration.\nArchitecture Highlights:\nLayer                                         Technology\nExtension Host                         VS Code Extension API\nState Management                   Custom EventBus + StateManager pattern\nCanvas Rendering                           HTML5 Canvas with custom renderers\nLayout Engine                          Graph-based auto-layout algorithm\nBuild System                                    esbuild for fast bundling\nTesting                                        Jest with TypeScript support Key Design Decisions:\nSeparation of concerns ‚Äî Core graph logic is decoupled from VS Code APIs, making it testable and portable\nEvent-driven architecture ‚Äî All state chan\n\nChallenges: VS Code Webview Sandboxing\nWebviews run in an isolated context with strict CSP policies. We had to carefully architect the communication bridge between the extension host and the canvas without compromising security.\nCanvas Performance at Scale\nRendering hundreds of nodes with smooth 60fps panning required implementing viewport culling ‚Äî only drawing elements visible on screen ‚Äî and optimizing our render loop.\nUndo/Redo with Graph State\nImplementing reliable undo/redo for a mutable graph structure was tricky. We solved it with immutable state snapshots and a custom StateManager that handles deep cloning efficiently.\nAuto-Layout Algorithm\nCreating a layout engine that produces visually pleasing results for any graph topology took multiple iterations. We implemented a layered approach inspir\n\nAccomplishments: ‚úÖ Zero external runtime dependencies ‚Äî The extension is lightweight and fast to load ‚úÖ Seamless UX ‚Äî Opening a .fluxdiagram file feels as natural as opening a code file ‚úÖ Production-ready code quality ‚Äî Full TypeScript, ESLint, Prettier, and Jest test coverage ‚úÖ Minimap implementation ‚Äî A fully functional minimap that mirrors the main canvas in real-time ‚úÖ Export flexibility ‚Äî Users can export diagrams as PNG (for docs), SVG (for web), or JSON (for processing) ‚úÖ First-class keyboard support ‚Äî Every action has a shortcut, making it accessible and efficient\n\nWhat we learned: VS Code's Extension API is incredibly powerful ‚Äî Custom editors open up possibilities far beyond text editing\nState management matters ‚Äî Investing early in a clean EventBus pattern saved us from countless bugs\nPerformance requires intention ‚Äî Canvas rendering at scale doesn't \"just work\" ‚Äî you have to design for it\nDeveloper experience is UX ‚Äî Small touches like grid snapping and keyboard shortcuts dramatically improve usability\nTesting graph logic is non-trivial ‚Äî We developed patterns for testing stateful, interconnected data structures\n\nWhat's next: üöÄ Roadmap:\nFeature                             Description\nCollaboration               Real-time multi-user editing via VS Code Live Share\nAI-Powered Diagrams     Generate flowcharts from natural language or code analysis\nSwimlanes                       Organize nodes into lanes for process diagrams\nImport Support              Import from Mermaid, Draw.io, and other formats\nCode Integration                Link nodes to specific lines of code for documentation We believe FluxDiagram can become the go-to diagramming tool for developers who live in VS Code. The journey is just beginning! üéØ",
    "hackathon": null,
    "prize": "Winner CERTIFICATE",
    "techStack": "antigravity, css3, html5, react, typescript",
    "github": "https://github.com/sitharaj88/flux-diagram",
    "youtube": "https://www.youtube.com/watch?v=FNBSyVYvidk",
    "demo": "https://marketplace.visualstudio.com/items?itemName=sitharaj.flux-diagram",
    "team": null,
    "date": "2025-12-29",
    "projectUrl": "https://devpost.com/software/fluxdiagram"
  },
  {
    "title": "HOUP APP",
    "summary": "HOUP APP Project Summary\n\nThe HOUP App is an innovative application designed to leverage artificial intelligence and antigravity technology to provide users with a unique experience. Although specific functionalities are not detailed, the combination of these technologies suggests a focus on enhancing user interaction and exploring new dimensions of mobility or content delivery.\n\nKEY ACHIEVEMENTS: The project stood out due to its pioneering use of antigravity concepts combined with AI, setting it apart from typical applications. Winning multiple prizes, including InterviewCake licenses and .xyz domains, indicates strong innovation, effective problem-solving, and potential market appeal, showcasing the team‚Äôs ability to capture judges‚Äô attention with a forward-thinking idea.\n\nNotable technical implementations likely include advanced AI algorithms for user interaction and experience personalization, alongside the integration of antigravity technology, which may involve novel physics simulations or user interface designs that defy conventional applications. This combination could lead to groundbreaking user experiences.\n\nPOTENTIAL CHALLENGES: The team likely faced several challenges, including the feasibility of integrating antigravity technology into a practical app, ensuring user safety, and addressing potential regulatory hurdles. Additionally, there may have been complexities in developing a user-friendly interface that effectively utilizes AI capabilities.\n\nFUTURE POTENTIAL: The HOUP App has significant potential for evolution into various sectors, such as entertainment, education, or mobility solutions. Future iterations could explore partnerships with tech companies, expand its feature set, or refine its core functionalities based on user feedback, ultimately leading to a commercially viable product that pushes the boundaries of current app capabilities.\n\nWhat it does: HOUP helps you record what you're working on every hour and automatically saves it to Google Sheets. Think of it as:\nA digital work diary that reminds you to update your progress\nAn automatic timesheet that logs your daily activities\nA productivity tracker that keeps a record of everything you accomplish\n\nHow it was built: using antigravity ai\n\nChallenges: many bug so i tripleshoot\n\nWhat we learned: AI and promting ,hoe apk work,how puplish apk",
    "hackathon": null,
    "prize": "Winner Participation; Winner InterviewCake Licenses; Winner .xyz Domains",
    "techStack": "ai, antigravity",
    "github": "https://github.com/naveenrajgit23/houp.git",
    "youtube": "https://www.youtube.com/watch?v=gGEAa8aBSek",
    "demo": "https://expo.dev/artifacts/eas/mH14uN4DWGFwjpCne9TFKD.apk",
    "team": "Naveen raj Malaikannan",
    "date": "2025-12-20",
    "projectUrl": "https://devpost.com/software/houp-app-xgeq9k"
  },
  {
    "title": "Sous Chef",
    "summary": "Sous Chef Project Summary\n\nSous Chef is an innovative cooking assistant application designed to enhance the culinary experience by providing users with personalized recipe suggestions, cooking tips, and ingredient management. It leverages advanced AI and machine learning to adapt to user preferences and dietary restrictions, making meal preparation more efficient and enjoyable.\n\nKEY ACHIEVEMENTS  \n   The project stood out due to its unique integration of AI technology, allowing for real-time recipe customization and user interaction. Winning both the overall prize and the third prize emphasizes its high level of innovation, usability, and impact in the culinary tech space.\n\nNotable technical implementations include the use of Claude for natural language processing to understand and respond to user queries, Memmachine for managing recipe data and user preferences, and a robust front-end built with React and TypeScript for a responsive and interactive user interface. Vite enhances performance by providing fast builds and hot module replacement.\n\nPOTENTIAL CHALLENGES  \n   The team likely faced challenges related to integrating multiple technologies seamlessly, ensuring the AI provides accurate and relevant suggestions, and managing user data privacy and security. Additionally, creating an intuitive user experience while handling complex functionalities could have posed design challenges.\n\nFUTURE POTENTIAL  \n   Sous Chef has significant growth potential, including the addition of features like meal planning, grocery list generation, and social sharing of recipes. Expanding the platform to include partnerships with grocery stores for ingredient delivery or integrating with smart kitchen devices could enhance user engagement and utility.\n\nWhat it does: Bridges the gap between student struggles and instructors field of view into how well their students are ingesting knowledge.\n\nInspiration: Had a discussion with whom i'd like to call Mentor. The Education Vertical is underserved as it is. We are preparing for our future, why not equip them with the best shot at making a better future.\n\nHow it was built: We built it with Claude Code, React, typescript and vite. Claude AI for Chat and MemMachine for persistent memory.\n\nChallenges: Implementing the real-time streaming response and speed while saving/pulling memory at the same time.\n\nAccomplishments: Creating this vision for students to get more personalized not generalized education and the instructors can save time and be able to assist all of their students and see where the hidden gaps are to provide a more robust curriculum.\n\nWhat we learned: That Memory can be applied and 100x your workflows.\n\nWhat's next: The Ability to upload curriculum uploads and an area for the students to keep structure notes from what they spoke to sous about plus their personal notes.",
    "hackathon": null,
    "prize": "Winner Third Prize",
    "techStack": "claude, memmachine, react, typescript, vite",
    "github": "https://github.com/MuddySheep/SousChef",
    "youtube": "https://www.youtube.com/watch?v=et9tQk2GUis",
    "demo": null,
    "team": null,
    "date": "2025-12-18",
    "projectUrl": "https://devpost.com/software/sous-chef-5czyd0"
  },
  {
    "title": "CleanMCP - MCP and Context Management made Easier",
    "summary": "CleanMCP is a project designed to simplify the management of Model-Context Protocol (MCP) and contextual data in applications. It aims to streamline the development process by providing an intuitive interface and efficient data handling capabilities, enabling developers to easily integrate and manage context within their applications.\n\nKEY ACHIEVEMENTS\nCleanMCP stood out by effectively addressing a common pain point in application development‚Äîthe complexity of managing context and state. Its innovative approach not only enhanced usability but also improved performance, leading to its recognition as a winner and second prize in the hackathon.\n\nThe project leverages a robust tech stack, including Next.js 16 and Node.js 20+, ensuring high performance and scalability. It incorporates advanced libraries like GSAP for animations and Radix UI for accessible components, while also utilizing Neo4j for efficient data management. The integration of TypeScript enhances code reliability and maintainability.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring seamless integration of various technologies and maintaining compatibility across different frameworks. Additionally, optimizing performance while handling complex data structures and providing a user-friendly interface could have posed significant hurdles.\n\nFUTURE POTENTIAL\nCleanMCP has the potential to evolve into a fully-fledged context management solution, possibly expanding to include features like real-time collaboration and enhanced data visualization. As the demand for efficient state management grows in web applications, CleanMCP could become a go-to tool for developers, fostering a community around its use and further innovation.\n\nWhat it does: Clean is an MCP server that intercepts tool calls from your other MCP server calls from coding agents mid-request. Every response gets captured and stored in a Neo4j graph database, tagged by team, user, MCP, and timestamp. The frontend then visualizes it and programmers can now have clean context provided straight from a history of MCP toolcalls saved in a vector DB. On top of that, we built a full dashboard using Reagraph in 2D mode where you can visualize your MCPs as nodes. Click on any MCP and see all the context it has fetched. The Context page lets you search, filter, and clean up outdated entries. We also integrated Mem Machine to power a chatbot that can answer questions about your stored context. Ask it things like \"What MCPs have we used on this project?\" or \"What context do we\n\nInspiration: When we started using AI coding agents like Claude Code and Cursor, we fell into a rabbit hole. There are over 12,000 MCP servers out there, and we had no idea which ones to use or what context our agents were actually fetching. But the bigger problem hit us when we worked as a team. One person's agent was using Tailwind v4, another's was on v3. Styles clashed. Context was fragmented. There was no shared memory and no visibility into what anyone's agent had actually seen. We knew we had to fix that.\n\nHow it was built: Clean runs as an MCP server that intercepts other MCP tool calls mid-request. Every response is captured and tagged with team ID, user ID, MCP name, tool name, and timestamp. The normal agent-to-MCP flow stays intact; Clean just sits in the middle and mirrors everything. All captured context is stored in Neo4j as a graph: Team ‚Üí User ‚Üí MCP ‚Üí Tool Call ‚Üí Context. This gives us a single source of truth for what context has ever been fetched across the team. We built a 6-page dashboard in Next.js: Home, MCP, Context, Team, Settings, Chat. The Home page uses Reagraph in 2D mode to show a central MCP node with individual MCP nodes around it. Clicking an MCP reveals the context nodes associated with that MCP. The Context page has search, freshness indicators, and bulk cleanup tools. The MCP page\n\nChallenges: Claude Code does not let you simply route MCPs through a proxy. We had to figure out how to actually intercept tool calls in the middle of a request, which required a lot of trial and error since MCP documentation is sparse. Translating Neo4j graph nodes into something that renders well in the browser was painful. We had to map relationships carefully so the 2D visualization made sense and performed smoothly. Wiring Mem Machine to the right slice of context from Neo4j took iteration. We needed the chatbot to stay relevant without dumping the entire database into every query. Smithery was not working, and there is no official API for scraping MCP directories. We ended up building our own MCP catalog from scratch in Neo4j. MCP servers are bleeding-edge technology. Learning how they work, bui\n\nAccomplishments: Got mid-request interception working so Clean actually captures tool calls as they happen\nBuilt our own MCP catalog from scratch in Neo4j instead of relying on external directories\nShipped a polished 6-page dashboard with Reagraph 2D visualization that we would actually want to use\nIntegrated Mem Machine into a chatbot that can answer questions about captured context\nTested cross-team context sharing on our own project and it works: one person fetches, everyone sees it\nLearned Neo4j, Mem Machine, Reagraph, and MCP architecture from scratch as freshmen in two days\n\nWhat we learned: MCP tooling is young and documentation is thin. We had to reverse-engineer patterns from examples and just try things until they worked.\nDesigning a graph schema in Neo4j that maps cleanly to a product UI takes thought. We spent hours on architecture before coding and it saved us from rewriting everything later.\nInterception is harder than proxying. Claude Code does not want you in the middle, so you have to work around constraints instead of fighting them.\nWith AI assistance, nothing is too far-fetched. We built something we did not think was possible at the start of the weekend.\n\nWhat's next: Token optimization is first. We want to use cosine similarity to fetch only relevant context per prompt instead of everything, and summarize stored context to reduce token costs. We also want smarter MCP recommendations that suggest which MCPs to install based on project type and what the team is working on. We are already talking to a YC company who wants to be our first customer. Goal is to ship, get paying users, and apply to the next YC batch.",
    "hackathon": null,
    "prize": "Winner Second Prize",
    "techStack": "claude-code, cypher-query-language, date-fns, eslint, framer-motion, gemini-api, gsap, lucide, mcp-(model-context-protocol), mem-machine, neo4j, next.js-16, node.js-20+, prettier, radix-ui, react-19, react-hook-form, reagraph, shadcn/ui, tailwind-css-v4, tanstack/react-table, typescript, vercel, websocket, zod",
    "github": "https://github.com/clarsbyte/cleanMCP-Hack.git",
    "youtube": "https://www.youtube.com/watch?v=1h5pDpASSlU",
    "demo": null,
    "team": "Pavan Kumar NY, Clarissa Saputra, Nikhil Prabhu",
    "date": "2025-12-19",
    "projectUrl": "https://devpost.com/software/clean-mcp-management-and-context"
  },
  {
    "title": "LifeGraph",
    "summary": "LifeGraph Project Summary\n\nLifeGraph is an innovative application designed to visualize and manage personal memories and experiences in a dynamic graph format. Utilizing a combination of advanced database technologies and user-friendly interfaces, it allows users to create, store, and explore their life events and milestones in an interconnected manner.\n\nKEY ACHIEVEMENTS: The project stood out due to its unique approach to personal memory management, integrating advanced graph databases to enhance user experience and data retrieval. Winning the First Prize indicates strong execution, innovative design, and effective problem-solving capabilities that resonated with the hackathon judges.\n\nLifeGraph leverages several cutting-edge technologies, including FastAPI for building a robust backend, Neo4j for graph database management, and React for an interactive frontend. The integration of TailwindCSS ensures a responsive and aesthetically pleasing design, while Vercel and Vite enhance deployment and development efficiency.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data structuring in the graph database, ensuring seamless user experience across devices, and maintaining performance as the dataset scales. Additionally, navigating user privacy concerns while handling personal information could have posed significant challenges.\n\nFUTURE POTENTIAL: LifeGraph could evolve into a comprehensive life documentation tool that incorporates AI-driven insights to help users reflect on their experiences. Future enhancements may include social sharing features, integration with other life management apps, and personalized memory recommendations based on user behavior and preferences.\n\nWhat it does: LifeGraph is a habit intelligence platform with two superpowers: 1. Graph-Based Pattern Discovery (Neo4j) Your daily habits (health, learning, creativity) are stored as connected nodes in a knowledge graph\nThe SAME_DAY_AS relationship links activities that happened together\nThe AI agent traverses these connections to find non-obvious correlations:\n\n\"On days with HRV > 55, your learning focus is 23% higher\"\n\"Heavy lifting + 8hrs sleep predicts creative satisfaction\"\n\"Afternoon coffee correlates with -23% sleep quality\" 2. Time-Traveling Memory (MemVerge) At strategic moments (Day 15, Day 45), the agent saves memory checkpoints\nUsers can \"restore\" to any checkpoint and the agent shows only what it knew then\nThe Day 45 discoveries (cold exposure, protein optimization) literally disappear whe\n\nInspiration: We've all been there: you track your sleep in one app, your workouts in another, your learning in a third. Each app shows you isolated metrics, but none can answer the questions that actually matter: \"Why was I so productive last Tuesday?\"\n\"Does my morning routine actually help my afternoon focus?\"\n\"What patterns predict my best creative sessions?\" The insight is obvious: your life is interconnected, but your tools aren't. Then we thought about AI assistants. They're great at conversations, but they have amnesia - every session starts fresh. What if an AI could not only see connections across your life domains, but also remember what it learned about you and even travel back in time to understand how its knowledge evolved? That's when LifeGraph was born: an AI agent that models your life a\n\nHow it was built: Architecture: React Dashboard (Vercel)\n        ‚Üì\nFastAPI Backend (GCP Cloud Run)\n        ‚Üì\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\nNeo4j Aura   MemVerge MMCloud\n(Graph DB)   (Memory Checkpoints) Neo4j Graph Schema: 5 node types: Streak, Log, Day, Concept, Sentiment\n9 relationship types including the crucial SAME_DAY_AS\n215 nodes, 432 relationships representing 90 days of synthetic habit data MemVerge Integration: Checkpoints store: discovered patterns, confidence scores, agent thoughts, active node count\nRestore operation returns the agent's state at that exact moment\nFrontend replaces the \"Agent Thoughts\" panel with checkpoint-specific insights AI Agent Logic: MATCH (health:Log)-[:SAME_DAY_AS]-(learn:Log)\nWHERE health.sleep_hours >= 8 AND health.workout = 'Zone 2 Cardio'\nRETURN AVG(learn.focus_score) as avg_focus\n\nChallenges: 1. Designing the Graph Schema\nHow do you model \"life\" as a graph? The breakthrough was the SAME_DAY_AS relationship - it connects activities across different domains without complex temporal joins. This simple edge enables powerful cross-streak queries. 2. Defining \"AI Agent Memory\"\nMemVerge typically checkpoints VMs. But what IS an AI agent's memory? We defined it as: Discovered patterns (with confidence scores)\nAgent thoughts (the reasoning stream)\nActive knowledge (which nodes it's aware of)\nNOT just raw data - the derived understanding 3. Making Time Travel Feel Real\nThe UI had to clearly show that the agent \"forgot\" later discoveries. We solved this by: Completely replacing the Agent Thoughts panel (not just filtering)\nShowing different confidence levels at different checkpoints\nAddin\n\nAccomplishments: 1. The Time Travel Actually Works\nWhen you restore to Day 15, the cold exposure insight genuinely disappears. The agent shows lower confidence scores. It feels like you're looking at a younger, less experienced version of the AI. 2. Cross-Streak Insights Are Real\nThe graph structure enables discoveries that spreadsheets can't: \"Frustrated tech sessions ‚Üí darker haiku themes same evening\". The SAME_DAY_AS edge makes this query trivial. 3. Clean Separation of Concerns Neo4j = What happened (data)\nMemVerge = What the agent knew (memory)\nThis pattern could apply to ANY AI agent application 4. Production-Ready Demo\nLive at lifegraph-hackathon.vercel.app with real API calls to Neo4j Aura and MemVerge-style checkpoints.\n\nWhat we learned: About Neo4j: Graph databases make relationship queries first-class citizens\nCypher pattern matching is incredibly intuitive for discovery\nAura's free tier handled our 215-node graph effortlessly About MemVerge: Memory checkpointing applies beyond VMs - it's a pattern for any stateful system\n\"Time travel\" is a compelling UX metaphor that users immediately understand\nAgent state includes derived knowledge, not just raw data About AI Agents: Agents need TWO types of memory:\n\nWorking memory: Current context and recent interactions\nEpisodic memory: What they learned and when they learned it\n\nGraphs are natural for agent reasoning about interconnected domains\nPersistent memory transforms agents from assistants to companions\n\nWhat's next: Short Term: Connect to real data sources (Apple Health, Whoop, Notion)\nNatural language queries (\"What makes me productive?\")\nMore granular checkpoints (weekly, monthly)\nAutomatic checkpoint creation when significant patterns emerge Long Term: Predictive insights (\"Based on today's data, try X tomorrow\")\nMulti-user anonymized patterns (collective intelligence)\nLLM integration for conversational graph exploration\nMobile app for daily logging\nExport/share your graph with coaches or doctors The Vision:\nEvery person deserves an AI that truly knows them - not just their last conversation, but their patterns, their growth, their journey. LifeGraph is a step toward AI agents with genuine long-term memory.",
    "hackathon": null,
    "prize": "Winner First Prize",
    "techStack": "fastapi, memory-machine, memverge, neo4j, neo4j-aura, python, react, recharts, tailwindcss, typescript, vercel, vite",
    "github": "https://github.com/ooiyeefei/lifegraph-hackathon",
    "youtube": "https://www.youtube.com/watch?v=Jwn7kOJdvsw",
    "demo": "https://lifegraph-hackathon.vercel.app/",
    "team": "Yee Fei Ooi",
    "date": "2025-12-18",
    "projectUrl": "https://devpost.com/software/lifestreak"
  },
  {
    "title": "Upside down survival",
    "summary": "\"Upside Down Survival\" is an innovative project designed to simulate survival scenarios in an inverted world, where traditional survival strategies are challenged. The project leverages advanced AI to create dynamic environments, requiring users to adapt their approaches in real-time, ultimately enhancing critical thinking and problem-solving skills in unconventional situations.\n\nKEY ACHIEVEMENTS\nThis project stood out for its unique concept that combines survival mechanics with an imaginative twist on reality. Winning both the overall hackathon and the \"Best use of Gemini AI\" prize highlights its effective integration of cutting-edge AI technology, allowing for immersive and adaptive gameplay, which captivated both judges and participants alike.\n\n\"Upside Down Survival\" was built using a robust tech stack including Express.js for backend services, Node.js for server-side logic, and React with TypeScript for a responsive user interface. The use of Python facilitated AI-driven components, particularly in generating adaptive scenarios based on player actions, showcasing a seamless blend of front-end and back-end technologies.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in balancing the complexity of AI algorithms with user experience, ensuring that gameplay remained engaging without becoming overly complicated. Additionally, implementing real-time adaptability in a survival scenario could present technical hurdles, such as maintaining performance and responding promptly to user inputs.\n\nFUTURE POTENTIAL\nThis project has significant potential for evolution into a full-fledged game or educational tool. Future iterations could expand on the AI capabilities, introducing multiplayer modes or incorporating virtual reality elements for a more immersive experience. Additionally, partnerships with educational institutions could leverage the game's critical thinking aspects for training programs in survival and crisis management.\n\nWhat it does: Upsidometer monitors temperature, air quality, and sound to detect abnormal environmental conditions linked to Vecna‚Äôs presence. It converts this data into a real-time danger level, triggers visual alerts on the screen, and plays music when the user is in serious danger to help them escape.\n\nInspiration: In Stranger Things, danger in the Upside Down is sensed through environmental changes rather than direct sight. We were inspired by this idea and wanted to build a system that could detect those subtle signs early and help people react before it‚Äôs too late.\n\nHow it was built: We used multiple sensors to collect real-time data, processed each input to detect anomalies, and combined them into a single danger index. This index is displayed visually through the Upsidometer and controls alerts like screen flashes and music playback. We used react, node, express,typescript to make this.\n\nChallenges: The main challenge was balancing sensitivity and accuracy while avoiding false alerts from normal environmental changes.It was difficult to integrate the Gemini Api Chatbot. It was also a challenge to integrate the music when the danger was critical.\n\nAccomplishments: The successful integration of Gemini Api Chatbot, inclusion of music when the person is in critical danger, real time sensor reading such as audio, temperature and air quality index. ALso very proud\n\nWhat we learned: Learned how to integrate Gemini Api Chatbot. Learned how to compute real time values using sensors.We learned the importance of sensor fusion, real-time data processing, and designing systems that are easy to understand during high-stress situations.\n\nWhat's next: Next, we plan to improve accuracy with adaptive thresholds, add more sensors, and expand the system into a portable or wearable solution for continuous protection.",
    "hackathon": null,
    "prize": "Winner Best use of Gemini AI",
    "techStack": "ai, express.js, node.js, python, react, typescript",
    "github": "https://github.com/alwinalbert/gorgan",
    "youtube": "https://www.youtube.com/watch?v=MFh-un-HZPs",
    "demo": null,
    "team": "harshadithyan, Devika Vinod, Alwin Albert, Sreeram Athrij",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/upside-down-survival"
  },
  {
    "title": "Compass",
    "summary": "Compass is a data-driven application designed to provide users with insights and recommendations based on complex datasets. By leveraging machine learning and advanced data visualization, it aims to guide users in decision-making processes across various domains.\n\nKEY ACHIEVEMENTS\nCompass stood out by effectively integrating multiple technologies to create a seamless user experience. Its use of advanced analytics and predictive modeling contributed to its recognition as a winner in the hackathon, demonstrating the project‚Äôs potential impact in real-world applications.\n\nNotable technical implementations include the use of FastAPI for efficient backend development, Next.js for responsive frontend design, and machine learning frameworks like NumPy and NumPyro for data processing and predictive analytics. The project also showcased impressive visualizations through Recharts, enhancing user interaction and understanding of data trends.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to data integration and ensuring the accuracy of predictions. Additionally, optimizing performance for real-time analytics and maintaining user engagement through intuitive design would have required significant effort.\n\nFUTURE POTENTIAL\nCompass could evolve into a comprehensive platform by expanding its data sources and integrating more sophisticated algorithms for enhanced predictive capabilities. Additionally, incorporating user feedback mechanisms and scaling to support larger datasets could significantly broaden its application across industries.\n\nWhat it does: Compass is a daily check-in and simulation app that is a personal model of how your habits influence your mood and then lets you interact with that model. Each day, you log a compact set of signals. You log your mood score, how long you slept, how much you exercised, how much time you spent with other people, how much you were on screens late at night, and an optional free-text journal. Behind the scenes, Compass treats that as a time series and learns how ‚Äúyesterday‚Äù predicts ‚Äútoday.‚Äù It then surfaces the three habits that appear to move your mood the most for you which turn coefficients into human-readable statements like ‚Äúfor you, each extra hour of sleep is associated with about +0.6 mood tomorrow‚Äù or ‚Äúextra late-night screen time tends to pull mood down the next day.‚Äù The app includes\n\nInspiration: We kept coming back to one simple but important question: ‚ÄúFor me, personally, what actually improves my mood?‚Äù Most mental health apps give the same advice. They tell you to sleep more, exercise, go outside, and reduce screen time. Helpful, but vague. None of them can answer whether sleep sleep actually the strongest lever for me? None of them can answer whether how much doomscrolling after 11pm hurt my next-day mood? Compass was created from the idea of precision mental health. No more generic tips, but a personal map of cause‚Äìeffect patterns learned from your own life. We wanted something that feels like a flight simulator for behavior change, where you can test ‚Äúwhat if I slept 8h and halved my screen time?‚Äù before you actually do it.\n\nHow it was built: Compass is a three-part system. It is a Next.js frontend, a Supabase/Postgres data layer, and a Python/FastAPI.\n\nChallenges: One of the biggest challenges was designing a model that felt both ‚Äúsmart‚Äù and interpretable. A deep neural net on the time series would have been a black box, but a naive ‚Äújust linear regression‚Äù felt too weak. Therefore, we had to find middle ground that stayed mathematically possible, explainable, and shippable. We also had to solve the cold-start problem, that is where new users have almost no history, so a purely personal model is too noisy, yet we still want to give them useful feedback. Another subtle challenge was handling the time-series and ‚Äúcausal‚Äù story responsibly. Through using lagged predictors (yesterday ‚Üí today), the model is more causal-ish than same-day correlations, but it‚Äôs still observational. On top of all that we had to make sure everything fit properly across Next.\n\nAccomplishments: I'm proud that Compass is a real end-to-end system. I'm equally proud that the stack (Next.js, Supabase, FastAPI, scikit-learn, Hugging Face transformers) is actually deployable and debuggable. Lastly, I'm grateful that this is something that people can use to help themselves.\n\nWhat we learned: I primarily learned that large language models are strongest as interpreters, not oracles. I constrained the LLM to operate only on structured summaries of the numeric model. Thus, the prediction is within reproducible math while using the model‚Äôs strengths to generate explanations, contextual guidance, and gentle disclaimers. That separation gave us both safety and clarity.\n\nWhat's next: I see Compass as a foundation for more modeling, including fully Bayesian hierarchical and time-varying coefficient models, so every effect size comes with uncertainty and the system can track how your sensitivity to different habits changes over time. We also want to embed explicit N-of-1 experiments, carefully integrate passive signals from wearables and phones, add clinician-facing and exportable views, and extend beyond mood to related dimensions like anxiety, energy, and stress resilience.",
    "hackathon": null,
    "prize": null,
    "techStack": "fastapi, lucide, next.js, numpy, numpyro, pandas, recharts, scikit, supabase, tailwind, transformers, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=6ADpWIHV5Q4",
    "demo": "https://compass-3.vercel.app/",
    "team": "Murari Ambati",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/compass-tmyfx6"
  },
  {
    "title": "10xCoders",
    "summary": "10xCoders is a web application designed to enhance the coding skills of users by providing interactive coding challenges and collaborative learning environments. It connects developers of varying expertise levels, facilitating mentorship and skill development through gamified experiences.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its innovative approach to coding education, focusing on community engagement and real-time collaboration. Winning the hackathon demonstrates its potential impact, as it effectively addresses a common need among developers for accessible, engaging, and practical learning resources.\n\nThe application leverages a modern tech stack, including Express.js for the backend, MongoDB for data storage, and React.js for a responsive frontend. The use of Tailwind CSS allows for rapid UI development with a focus on customizable design, enhancing user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring real-time collaboration features functioned seamlessly, particularly in handling multiple users. Additionally, creating a scalable architecture to support future growth and managing user data securely would have posed significant technical hurdles.\n\nFUTURE POTENTIAL\n10xCoders has the potential to evolve into a comprehensive platform that not only offers coding challenges but also integrates features like peer reviews, personalized learning paths, and partnerships with educational institutions. Expanding into mobile applications or incorporating AI-driven tutoring could further enhance user engagement and learning outcomes.\n\nWhat it does: 10xCoders is an all-in-one platform designed to help users learn, practice, and grow in the world of technology. It offers: Personalized Learning Paths ‚Äî Tailored roadmaps to fit each learner‚Äôs goals.\nCoding Practice ‚Äî Hundreds of real-world coding challenges to sharpen problem-solving skills.\nCareer Agent ‚Äî Smart recommendations and job matches based on user skills and interests.\nInterview Preparation ‚Äî Practice real interview questions from top tech companies.\nKanban Board ‚Äî Stay organized with a personal task manager.\nResume Maker & Enhancer ‚Äî Create and optimize tech-focused resumes with AI-powered feedback.\nTyping Speed Enhancer ‚Äî Build faster coding speed with interactive exercises. Everything you need ‚Äî from learning to landing your dream job ‚Äî is in one place.\n\nInspiration: The journey to becoming a great programmer can often feel overwhelming ‚Äî scattered resources, unclear learning paths, and lack of mentorship make it hard for aspiring coders to grow consistently. We wanted to create one unified platform that bridges the gap between learning, practicing, and advancing a tech career ‚Äî helping learners become true 10x developers with structured guidance and real-world practice.\n\nHow it was built: We built 10xCoders using: Frontend: React.js with Vite for lightning-fast performance and responsiveness.\nBackend: Node.js and Express.js for efficient API handling and authentication.\nDatabase: MongoDB for storing user data, progress, and resources.\nUI/UX: Tailwind CSS and ShadCN/UI for a modern, minimal, and adaptive design.\nAI Integration: OpenAI API for resume enhancement, personalized learning insights, and interview preparation. The platform‚Äôs architecture focuses on scalability, speed, and personalization.\n\nChallenges: Integrating multiple features while maintaining a smooth user experience.\nDesigning a personalized roadmap system that adapts to different user goals.\nEnsuring accurate AI-driven feedback for resumes and interview prep.\nBalancing data storage efficiency with real-time updates for progress tracking.\n\nAccomplishments: Successfully developed a modular and scalable platform integrating learning, career, and practice tools.\nBuilt a clean and responsive UI that works seamlessly across all devices.\nImplemented AI-powered features that add real-world value to users‚Äô growth.\nCreated an ecosystem that motivates learners to level up daily and stay consistent.\n\nWhat we learned: How to merge education, productivity, and career development into one cohesive platform.\nThe importance of personalization and user experience in edtech tools.\nLeveraging AI and automation to deliver intelligent career insights and faster learning outcomes.\nTeam collaboration and version control using GitHub and agile workflows with Kanban boards.\n\nWhat's next: Launching mobile app versions for Android and iOS.\nAdding community discussions and mentorship programs for peer learning.\nIntroducing real-time code collaboration and AI-powered coding tutor.\nPartnering with tech companies for direct job placement opportunities.\nExpanding to include Web3, Data Science, and DevOps career tracks.",
    "hackathon": null,
    "prize": "Winner 2nd Runner up",
    "techStack": "express.js, javascript, mongodb, node.js, react.js, tailwind",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=L7xEhdAI4Lo",
    "demo": "https://10x-coders-main.vercel.app/",
    "team": "Sameer .",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/10xcoders-yx83dt"
  },
  {
    "title": "CppShell",
    "summary": "CppShell Project Summary\n\nCppShell is a command-line interface (CLI) application built in C++ that likely provides a shell-like environment for users to execute commands, manage files, or interact with system processes. Its design may emphasize performance and user experience through features like command history and auto-completion.\n\nKEY ACHIEVEMENTS: CppShell stood out in the hackathon by winning the 1st Place ‚Äî Build From Scratch Champion award. Its success can be attributed to a well-executed concept that showcases advanced C++ programming skills, effective use of system resources, and an intuitive user interface that enhances user interaction.\n\nThe project utilizes C++ for its performance and efficiency, integrates Git for version control, and is developed on a Linux platform, indicating a focus on a robust environment. The implementation of the readline library suggests advanced features like command line editing and history management, enhancing the usability of the shell.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as managing memory effectively in C++, ensuring cross-platform compatibility, and implementing advanced features like parsing commands and handling errors gracefully. Debugging complex interactions in a shell environment could also present significant hurdles.\n\nFUTURE POTENTIAL: CppShell could evolve by incorporating additional functionalities such as scripting capabilities, plugin support for extensibility, and integration with cloud services for remote command execution. It could also explore graphical enhancements or web-based interfaces to broaden its user base and applications.\n\nWhat it does: CppShell is a fully functional command-line interpreter that replicates core shell behaviors: REPL Cycle: Processes user input in real-time.\nInfinite Pipelines: Chains commands (e.g., cmd1 | cmd2 | cmd3) via kernel pipes.\nI/O Redirection: Supports output redirection (>) and append mode (>>).\nPersistent History: Saves and loads session history automatically.\nAdvanced Parsing: Handles single quotes, double quotes, and backslash escapes correctly.\n\nInspiration: I wanted to stop trusting the \"magic\" of the terminal and understand the machinery. For the Build From Scratch hackathon, I challenged myself to build a POSIX-compliant shell without high-level shortcuts like system(). My goal was to use the raw system calls that power the OS.\n\nHow it was built: Built in C++ on Linux using three main architectural pillars: The Parser: A custom state-machine tokenizer that handles complex quoting and escape sequences character-by-character.\n Process Management: Uses fork() to clone the shell and execvp() to perform a \"brain transplant,\" replacing the process memory with the target program.\n Pipeline Logic: A \"bucket brigade\" loop that creates pipe() channels and uses dup2() to surgically rewire STDOUT and STDIN between independent processes.\n\nChallenges: Zombie Processes: Commands would finish but remain in the process table. I had to implement careful waitpid loops to reap them.\nDeadlocks: In multi-stage pipelines, forgetting to close a single write-end caused the reader to hang forever.\nParsing: Distinguishing between literal strings inside 'single quotes' vs escapable strings inside \"double quotes\" required multiple rewrites.\n\nAccomplishments: Zero High-Level Dependencies: Built entirely with raw POSIX syscalls (unistd.h, sys/wait.h, fcntl.h).\nRobust Pipelines: Can handle arbitrarily long command chains.\nUsability: With persistent history and auto-complete, it feels like a real tool.\n\nWhat we learned: Files are Integers: I gained a deep appreciation for how Linux treats File Descriptors.\nCost of Forking: Learned how expensive yet powerful cloning a process state is.\nComplexity of Text: Interpreting a simple string correctly is harder than it looks.\n\nWhat's next: Signal Handling: Implementing Ctrl+C handling.\nEnvironment Variables: Adding export and variable expansion ($VAR).\nScripting: Adding support for .sh script execution.",
    "hackathon": null,
    "prize": "Winner 1st Place ‚Äî Build From Scratch Champion",
    "techStack": "c++, git, linux, readline",
    "github": "https://github.com/vb8146649/codecrafters-shell-cpp",
    "youtube": "https://www.youtube.com/watch?v=W3uO1kWf1Rg",
    "demo": null,
    "team": "Vishal Vishal",
    "date": "2025-12-25",
    "projectUrl": "https://devpost.com/software/linux-terminal"
  },
  {
    "title": "Wastewise",
    "summary": "Wastewise Project Summary\n\nWastewise is an innovative project designed to help users manage waste more effectively by providing insights into recycling and waste reduction practices. Utilizing artificial intelligence, it offers personalized recommendations based on user behavior and local waste management guidelines.\n\nKEY ACHIEVEMENTS: The project stood out for its user-centric approach and integration of AI, which enables tailored insights and suggestions. Its comprehensive platform not only educates users about waste management but also actively engages them in sustainability efforts, earning it the title of winner at the hackathon.\n\nKey technical implementations include the use of AI algorithms to analyze user data and generate personalized waste management tips, as well as a responsive front-end developed with HTML, CSS, and JavaScript. The integration with Gemini technology likely enhances user interaction and experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as data accuracy in waste classification, user engagement in adopting new habits, and ensuring the platform's adaptability to different local regulations and recycling capabilities. Building a scalable model that effectively serves diverse communities could also present hurdles.\n\nFUTURE POTENTIAL: Wastewise could evolve into a comprehensive platform that incorporates community features, allowing users to share tips, successes, and challenges. Future iterations could also integrate partnerships with local governments and recycling organizations to enhance data accuracy and provide users with real-time updates on local waste policies and events.\n\nWhat it does: WasteWise is a comprehensive AI-powered platform that transforms waste management into an engaging, rewarding experience:\n\nInspiration: In a world drowning in electronic waste and struggling with proper waste disposal, we saw an opportunity to make a difference. Every year, 50 million tons of e-waste are generated globally, yet only 20% is properly recycled. This results in $62.5 billion worth of recoverable materials lost annually, while toxic materials contaminate our soil and water. WasteWise was born from a simple question: What if recycling could be as easy as shopping online? We envisioned a platform that doesn't just tell you to recycle ‚Äì it shows you how much your waste is worth, connects you with verified recyclers, educates you about environmental impact, and rewards you for making sustainable choices. Our mission is to bridge the gap between environmental responsibility and everyday convenience, making it easier\n\nHow it was built: Frontend Stack: Pure HTML5, CSS3, and vanilla JavaScript for lightweight performance\nResponsive design with custom animations and modern card-based UI\nGlassmorphism effects and animated loading states for enhanced UX\nDynamic form validation and progressive enhancement patterns AI & Machine Learning: Google Gemini API (gemini-2.0-flash-exp model) via REST endpoints\nImage recognition for waste classification\nNatural language processing for chatbot interactions\nCustom prompt engineering for accurate waste identification and pricing insights Data Architecture: LocalStorage for user preferences, chat history, and estimate tracking\nSessionStorage for form state management\nComprehensive pricing database with 50+ item categories\nHigh-value items database with per-piece pricing logic\nCondition mult\n\nAccomplishments: ‚úÖ Fully Functional AI Integration: Three distinct AI-powered features (classifier, estimator, chatbot) working seamlessly with real Gemini API calls and intelligent fallbacks ‚úÖ Sophisticated Pricing Engine: Realistic calculations with 50+ categories, high-value item recognition, condition assessment, and quantity bonuses ‚Äì featuring proper number formatting with thousands separators ‚úÖ Resilient Architecture: Built-in rate limiting, error handling, and local fallback responses ensure users always get value even when APIs are unavailable ‚úÖ Exceptional UX: Animated loading states, typing indicators, floating chatbot buttons, dynamic form validations, and glassmorphism effects create an engaging, modern experience ‚úÖ Production-Ready Code: Clean event handling, proper state management, comprehe",
    "hackathon": null,
    "prize": null,
    "techStack": "ai, css, gemini, html, javascript",
    "github": "https://github.com/Ranojitdas/Wastewise-Ecommerce-Website",
    "youtube": "https://www.youtube.com/watch?v=ey3tgQPmqYI",
    "demo": "https://edunest.me/Wastewise-Ecommerce-Website/index.html",
    "team": "Ranojit Das",
    "date": "2025-12-13",
    "projectUrl": "https://devpost.com/software/wastewise-eaws74"
  },
  {
    "title": "Location Scout",
    "summary": "Location Scout is a web application designed to help users discover and recommend ideal locations for various activities, utilizing Yelp's extensive database of businesses and services. By combining user preferences with location data, it aims to streamline the process of finding suitable venues for events, outings, or gatherings.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its intuitive user interface and effective use of Yelp's API to deliver personalized location recommendations. Its innovative approach to location scouting and ability to cater to specific user needs contributed to its success, securing both the first and second place wins at the hackathon.\n\nBuilt with Next.js and TypeScript, Location Scout leverages modern web development practices, ensuring a responsive and maintainable codebase. The integration with Yelp allows real-time data access, enabling users to filter locations based on various criteria such as ratings, type of venue, and distance.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to API limitations and data accuracy, particularly in ensuring that location suggestions were both relevant and up-to-date. Additionally, implementing a seamless user experience while handling large datasets from Yelp would have required careful optimization.\n\nFUTURE POTENTIAL\nLocation Scout could evolve by incorporating user-generated content, such as reviews and photos, to enhance its recommendations further. Expanding the platform to include additional services like event planning tools or partnerships with local businesses could also create more value for users, making it a comprehensive solution for event scouting and planning.\n\nWhat it does: Location Scout helps small business owners identify strong brick-and-mortar locations. Users enter a business type and city, such as ‚Äúcoffee shop‚Äù and ‚ÄúBoston, MA.‚Äù The app analyzes Yelp data to map competitor density, scores neighborhoods using AI for saturation and market gaps, and ranks top opportunities. It also gives detailed analysis of recommended neighborhoods with interactive maps, demographic insights, competitor comparisons, review summaries, and service recommendations. This turns complex data into a clear checklist for differentiation.\n\nInspiration: Location Scout was inspired by how messy and guesswork-heavy site selection is for small businesses. Most business owners rely on intuition, scattered Google searches, or expensive enterprise tools that are not practical for a single-location cafe, restaurant, or retail shop. The goal was to create a focused, self-serve tool that uses Yelp‚Äôs business data to give clear guidance on where to open and how to stand out.\n\nHow it was built: The app was built with: Frontend: Next.js, React, TypeScript\nMaps: Leaflet for pins, clustering, and interactive bounds\nAPIs: Yelp Business Search API and Yelp AI Search API\nAI Processing: Nvidia LLM for parsing and structuring data\nUI: shadcn/ui and Tailwind CSS\nPerformance: Client-side caching to keep exploration fast\n\nChallenges: Post-processing AI responses for the UI\nThe app needed structured outputs such as neighborhood scores and gap summaries, so I created parsing and validation steps to keep results consistent across different business types.\nOptimizing API calls and caching\nEach analysis required multiple Yelp and Yelp AI calls. Local caching with smart invalidation helped maintain a balance between speed, freshness, and API usage limits.\n\nAccomplishments: A smooth flow from data to decisions\nUsers can move from a single search to neighborhood scoring, competitor mapping, and clear action steps within minutes.\nTurning Yelp AI into practical intelligence\nSimple API calls now produce insights that support real location decisions for small business owners.\nA fully responsive design\nThe app works just as well for detailed desktop analysis as it does for on-the-go exploration on mobile.\n\nWhat we learned: Simple APIs can create meaningful impact\nWith the right structure, Yelp AI can provide the type of insights that were once available only to larger companies.\nClarity matters more than volume\nPulling large datasets is easy, but turning that information into useful guidance takes careful formatting and prioritization.\nThoughtful UX ties everything together\nThe flow from the landing page to the city overview, then into neighborhood detail views and personalized insights, feels natural even though the underlying logic is complex.\n\nWhat's next: Custom analysis filters\nOptions for saturation, competitor density, foot traffic, budget ranges, and lease preferences.\nUser accounts and saved analyses\nA logged-in experience with history tracking and cross-city comparisons.\nAI-powered launch playbooks\nAutomatically generated 90-day plans for hours, pricing, marketing, and service strategy based on neighborhood needs.\nTeam collaboration\nShared analyses, comments, and collaborative launch planning.\nMobile-first exploration\nNative iOS and Android apps with geolocation to support real-time scouting. Yelp AI API client ID - I7BEU_lXBeNf7EfUKEbCvA",
    "hackathon": null,
    "prize": "Winner Second Place",
    "techStack": "nextjs, typescript, yelp",
    "github": "https://github.com/rahuls98/location-scout",
    "youtube": "https://www.youtube.com/watch?v=x1utF9G-AHE",
    "demo": null,
    "team": "Rahul Suresh",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/location-scout"
  },
  {
    "title": "AuditShield.ai",
    "summary": "AuditShield.ai Summary\n\nAuditShield.ai is a comprehensive platform designed to streamline and enhance the auditing process by leveraging artificial intelligence. It aims to automate data collection and analysis, providing organizations with real-time insights and compliance monitoring to improve their audit efficiency and accuracy.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative use of AI to minimize manual efforts in the audit process, resulting in faster and more accurate outcomes. Winning the hackathon demonstrates the team's ability to effectively solve a real-world problem with a practical and scalable solution.\n\nNotable implementations include the use of Node.js and PostgreSQL for robust backend development, React for a responsive frontend, and TanStack Query for efficient data fetching. The integration of Supabase for real-time databases and Tailwind CSS for a sleek user interface contributed to a seamless user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data privacy and security, as handling sensitive audit information requires rigorous compliance with regulations. Additionally, ensuring the AI algorithms deliver accurate insights and can adapt to various auditing scenarios could have posed significant technical hurdles.\n\nFUTURE POTENTIAL: AuditShield.ai could evolve by incorporating machine learning models to further enhance predictive analytics and risk assessment capabilities. Expanding its features to include customizable reporting tools and integrations with other enterprise systems could broaden its market appeal and user base. Furthermore, the addition of collaborative features for audit teams could improve workflow and communication.",
    "hackathon": null,
    "prize": "Winner 1st",
    "techStack": "gemini, google, node.js, postgresql, react, react-router, shadcn/ui, supabase, tailwind-css, tanstack-react-query, typescript, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=3_9FBeWEHX8",
    "demo": "https://fair-charge-ai.vercel.app/",
    "team": null,
    "date": "2025-12-12",
    "projectUrl": "https://devpost.com/software/auditshield-ai"
  },
  {
    "title": "HYROX Pulse",
    "summary": "HYROX Pulse is a fitness-oriented digital application that leverages augmented reality and interactive elements to enhance user engagement in workout regimes. By integrating advanced technologies, it aims to provide users with real-time performance metrics and personalized training experiences.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively merging augmented reality with fitness tracking, creating an immersive user experience that motivates individuals to achieve their fitness goals. Its innovative approach and user-centric design likely contributed to its recognition as the winner of the hackathon.\n\nBuilt using Lens Studio and TypeScript, HYROX Pulse incorporates cutting-edge AR capabilities to visualize workout metrics in real-time. The application may utilize sophisticated algorithms for data analysis and visualization, providing a seamless and interactive interface for users.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring compatibility across various devices, optimizing performance for real-time data processing, and maintaining user engagement in a crowded fitness app marketplace. Additionally, integrating AR features while keeping the user experience intuitive could have posed significant hurdles.\n\nFUTURE POTENTIAL\nHYROX Pulse has the potential to evolve into a comprehensive fitness platform that includes social features, such as community challenges and leaderboards, to enhance user motivation and interaction. Future iterations could also incorporate machine learning to personalize workout plans further based on user data and preferences, positioning it as a leader in the fitness technology space.\n\nWhat it does: HYROX Pulse adopts Spectacles technology to digitize the physical grind. We turn any open space into a smart, connected arena‚Äîno judges or training partners required. KEY FEATURES Autopilot Coaching (The Use Case): Take Station 4 (80m Burpee Broad Jumps). The AR layer actively counts every rep and tracks distance covered. The moment you hit 80m, the Spectacles \"nudge\" you immediately into the run transition. No counting, no cheating‚Äîjust performance. Ghost Mode (The Competition): Leveraging HYROX's standardized format, we visualize the leaderboard. Train against AR Avatars of friends abroad, World Champion pros, or your own \"Personal Best\" shadow. Split time projection in your physical environment. THE IMPACT We bridge the gap between physical exertion and digital feedback, turning a solo\n\nInspiration: HYROX is the \"World Series of Fitness.\" It‚Äôs a standardized global race format blending functional strength and endurance. The formula is simple but brutal: 8km of running separated by 8 different functional workouts (like sled pushes, rowing, and wall balls). Because every race globally uses the same weights and distances, it creates a perfectly level playing field‚Äîa massive, unified leaderboard where seconds separate the amateur from the elite. Chandran, Roland and Min having a conversation about sports.. marathon running.. before we settled on HYROX. We wanted to introduce  AR  into, and with a bit of imagination, one can use Hyrox in a practice and training environment without a judge, to feel as though we are competing with others.\n\nHow it was built: Scripts with help of Github Copilot and Cursor\nLens Studio Lens for Snap Spectacles\n\nChallenges: Integration of different parts in the final project\n\nAccomplishments: Hand and head recording and replay\nSplit time projection integrated into your physical environment\nBurpee counter detects phases of your burpee broad jump exercise\nGPS + IMU based  performace indicators for long distance running\n\nWhat we learned: Working with mixed reality like Spectacles\nHow to mix physical with digital\n\nWhat's next: Supabase integration for persistent storage\nLeaderboards selection of people to train with\nMore complete body tracking including context replace of weights, slide etc.\nAutomated wall ball judging with sphere detection",
    "hackathon": null,
    "prize": "Winner 1st Place",
    "techStack": "lensstudio, typescript",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": "Roland Smeenk, Chandran N, min lai",
    "date": "2025-12-13",
    "projectUrl": "https://devpost.com/software/hyrox-ar"
  },
  {
    "title": "NutriCare Agents",
    "summary": "NutriCare Agents Project Summary\n\nNutriCare Agents is a digital platform that aims to provide personalized nutrition advice and meal planning through the integration of AI and location-based services. By leveraging user data and preferences, it delivers tailored dietary recommendations and connects users with nearby grocery options and food services.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative use of AI from OpenAI for personalized nutrition insights and the seamless integration of multiple APIs to enhance user experience. Winning the hackathon indicates its potential impact and effectiveness in addressing real-world dietary challenges.\n\nNotable technical implementations include the use of Next.js for server-side rendering and optimized performance, Firebase for real-time database management, and integration with Google Maps API for location-based services. The use of React and Tailwind CSS also ensured a responsive and visually appealing user interface.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring data privacy and security while handling sensitive user information related to health and nutrition. Additionally, achieving accurate and relevant AI-generated recommendations may have required extensive training and fine-tuning of the model used.\n\nFUTURE POTENTIAL: NutriCare Agents could evolve by incorporating more advanced machine learning algorithms for improved personalization, expanding partnerships with local grocery stores or meal delivery services, and integrating social features that allow users to share their experiences and recipes. Potential expansion into mobile applications could also enhance accessibility and user engagement.",
    "hackathon": null,
    "prize": null,
    "techStack": "api, firebase, gemini, google, google-cloud, javascript, maps, next.js, openai, places, react, tailwind",
    "github": "https://github.com/technoob05/NutriCare_Agents",
    "youtube": "https://www.youtube.com/watch?v=6d3UbajcSWw",
    "demo": "https://nutricareagents.learningaiwithlosers.com/",
    "team": "HUY QU√ù MINH, Dao Sy Duy  Minh",
    "date": "2025-12-11",
    "projectUrl": "https://devpost.com/software/nutricare-agents-oefn4b"
  },
  {
    "title": "Mintly",
    "summary": "Mintly is a digital platform designed for seamless authentication and interaction using QR codes, enabling users to manage and verify their identities in real time. By leveraging a robust database and a user-friendly interface, Mintly aims to simplify user authentication processes and enhance security in various applications.\n\nKEY ACHIEVEMENTS\nMintly stood out in the hackathon due to its innovative use of real-time QR code scanning for secure authentication, which enhances user experience and security. Its victory was likely attributed to a combination of technical sophistication, user-centric design, and the effective implementation of cutting-edge technologies.\n\nThe project incorporates various technologies, including PostgreSQL for robust data management, React for a responsive user interface, and Tailwind CSS for streamlined styling. The use of real-time features and QR code scanning (via libraries like `yudiel/react-qr-scanner`) exemplifies the team‚Äôs commitment to creating a modern, interactive application. Additionally, the implementation of TypeScript and ESLint ensures code quality and maintainability.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring security during the authentication process, particularly around QR code generation and scanning. Moreover, integrating real-time functionalities while maintaining performance and user experience could have posed significant technical hurdles. Balancing feature richness with a clean, intuitive user interface may also have been a challenge.\n\nFUTURE POTENTIAL\nMintly has the potential to evolve into a comprehensive identity management solution, expanding beyond QR code authentication to include features like multi-factor authentication and integration with other digital identity platforms. Future iterations could explore scalability options, enabling businesses to adopt Mintly for diverse applications, thus broadening its user base and enhancing its impact in the security and authentication landscape.",
    "hackathon": null,
    "prize": null,
    "techStack": "authentication, context, css, database, definer, dom, eslint, figma, framer, functions, github, level, lovable, motion, postgresql, qrcode.react, query, react, realtime, rls), router, row, security, shadcn/ui, sql, tailwind, typescript, vite, yudiel/react-qr-scanner, zod",
    "github": "https://github.com/CristosMpi/mintlyfin.git",
    "youtube": "https://www.youtube.com/watch?v=T2XR2lIDmw8",
    "demo": "https://mintlyfin.lovable.app/",
    "team": "Christos Mpirmpos",
    "date": "2025-12-17",
    "projectUrl": "https://devpost.com/software/mintly"
  },
  {
    "title": "Aegis.ai",
    "summary": "Aegis.ai Project Summary\n\nAegis.ai is a project focused on leveraging natural language processing (NLP) to enhance user interactions, possibly through a web browser extension. The aim seems to be improving accessibility or information retrieval by interpreting and responding to user queries effectively.\n\nKEY ACHIEVEMENTS: Aegis.ai stood out in the hackathon by showcasing innovative use of NLP to create a seamless user experience. Its ability to understand and process natural language likely impressed judges, leading to its recognition as a winner. \n\nThe project is built with a combination of CSS3 and HTML5 for a modern user interface, while the core functionality relies on natural language processing techniques. The integration of these technologies into a web extension format highlights its practicality and user-friendliness.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in accurately processing and interpreting a wide range of natural language inputs, ensuring that the extension performs reliably across different contexts. Additionally, optimizing the performance and responsiveness of the extension while managing resource constraints could have posed significant hurdles.\n\nFUTURE POTENTIAL: Aegis.ai could evolve into a comprehensive tool for various applications, such as customer support, educational aids, or personal assistants. Future developments could incorporate machine learning to enhance its NLP capabilities, expand its functionalities, or integrate with other platforms for broader use cases.",
    "hackathon": null,
    "prize": null,
    "techStack": "css3, extension, html5, natural-language-processing",
    "github": "https://github.com/BruderTaubeWitchhut/aegis-ai.git",
    "youtube": "https://www.youtube.com/watch?v=HXqxVz5VN6I",
    "demo": null,
    "team": "Private user, Private user",
    "date": "2025-12-10",
    "projectUrl": "https://devpost.com/software/aegis-ai-8q3aph"
  },
  {
    "title": "Inkphony",
    "summary": "Inkphony Project Summary\n\nInkphony is an innovative lifestyle app that combines immersive technologies to enhance personal expression through digital art and music. It allows users to create, share, and experience personalized content in a collaborative environment, potentially transforming how individuals engage with creative outlets.\n\nKEY ACHIEVEMENTS: Inkphony stood out by winning two prestigious awards: the overall hackathon winner and the Best Lifestyle Experience. Its unique approach to merging art and music, combined with user-friendly features and engaging interactivity, resonated well with judges and participants alike.\n\nThe project was built using advanced tools including Depth for spatial awareness, Meta for augmented reality experiences, PCA for data analysis, Quest for VR capabilities, and Unity for game-like interactivity. This combination allowed for a rich, multi-sensory experience that enhanced user engagement.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in integrating multiple technologies seamlessly, ensuring a smooth user experience across devices, and managing the balance between creative freedom and user guidance. Additionally, addressing varying user skill levels in art and music creation may have posed design difficulties.\n\nFUTURE POTENTIAL: Inkphony has the potential to evolve into a comprehensive platform for creative collaboration, possibly incorporating AI-driven tools for personalized content suggestions. Expanding its user base could lead to partnerships with educational institutions or artists, further enriching the community and enhancing user engagement through workshops and events.\n\nWhat it does: Inkphony allows you to write music on any surface around you and create endless instruments that you can play your music with until you find the right sound. Easily iterate using the handtracking note writing and scrubbing gesture to delete.\n\nInspiration: From our musical and XR backgrounds, we decided to create a highly creative tool to learn and experiment with instruments generated from your environment, the world becomes an instrument!\n\nHow it was built: Using the latest Meta PCA (v81), which features time-stamped and stereo camera access together with Gemini API, ElevenLabs, and our advanced filtering/cleaning system, we were able to build an asynchronous, continuous background recognition that seamlessly generates icons and instruments dynamically from your surroundings.\n\nChallenges: The usual image recognition data-to-world aligning, which was solved by storing the depth frame together with the camera capture, to later align properly when the Gemini API result was ready. This allows us to get correct results aligned to the real world, even if we are moving our heads.\n\nAccomplishments: The seamless background object detection system, the smooth hand-tracking implementation that opens up many possibilities compared to being limited to the Logitech pen.\n\nWhat we learned: Depth-align a camera frame to a depth frame with timestamps, using stereo camera access to create mind-bending deformations in real time!\n\nWhat's next: The top 1 next feature that we are implementing before publishing is saving and loading your songs and instruments. \nThen, have the capability to have multi-instrument compositions: \"Make the bass with an apple sound, the rhythm with a bottle sound, and the melody with a cup!\"\nAfter that, the ability to share songs with other users, and being able to listen to other people's compositions and instruments!",
    "hackathon": null,
    "prize": "Winner Best Lifestyle Experience",
    "techStack": "depth, meta, pca, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=T6NyCAvU0sk",
    "demo": "https://www.meta.com/s/8YgVAmyJM",
    "team": "PCA-AI integration, stretching",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/inkphony"
  },
  {
    "title": "OpenSoundLab",
    "summary": "OpenSoundLab Project Summary\n\nOpenSoundLab is an innovative audio-focused application leveraging immersive technologies to create interactive sound experiences. By integrating real-time sound manipulation with immersive environments, it allows users to engage with audio in a dynamic and visually appealing way.\n\nKEY ACHIEVEMENTS: The project distinguished itself by winning both the overall prize and the Judge‚Äôs Choice award, indicating strong support from both judges and peers. Its unique approach to blending audio interactivity with immersive technology showcased creativity and technical proficiency, making it a standout in the hackathon.\n\nKey technical implementations include the use of C and C# for robust audio processing, integration with Unity for creating engaging user interfaces, and innovative use of passthrough technology to blend virtual and real-world audio experiences. The mirror component might suggest a unique way of reflecting sound dynamics based on user interaction.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in synchronizing audio and visual elements seamlessly, ensuring low latency for real-time interactions, and managing hardware compatibility, especially with passthrough technologies. Additionally, optimizing performance for various devices could have posed significant hurdles.\n\nFUTURE POTENTIAL: OpenSoundLab has the potential to evolve into a comprehensive platform for audio education, virtual concerts, or sound therapy applications. By expanding its features to include collaborative sound creation and enhanced user engagement tools, it could appeal to musicians, educators, and therapeutic professionals in various fields.\n\nWhat it does: OpenSoundLab (OSL) is a spatial modular synthesizer and creative toolkit for Meta Quest headsets, designed to facilitate immersive sound creation and exploration in virtual reality. It offers a range of features including over 30 modular devices and spatial sound design, enabling users to craft rich auditory experiences and music within mixed-reality environments.\n\nInspiration: My inspiration for this project lies in many years of producing music, audio installations, working with Eurorack modular synthesizers and teaching creative technologies at the Academy of Art and Design Basel (HGK). The start was the pandemic and a funding that I received for creating a VR sound laboratory based on SoundStage VR for educating students in sound design via video tutorials at home.\n\nHow it was built: It is an open-source fork of SoundStage VR since that app was abandoned in 2017. OpenSoundLab is built with Unity in C# and audio DSP codes in C/C++ with Meta APIs and Mirror Networking for the multiplayer.\n\nChallenges: Adding the multiplayer features were definitely the most challenging aspects, especially syncing the audio engine in low latency over the network.\n\nAccomplishments: Offering a mixed-reality first, colocative, spatialized sound design and media arts experience that now offers hands as an input modality thanks to this hackathon!\n\nWhat we learned: Too much to list... :)\n\nWhat's next: Store release in Q1 2026, adding more modules and more video tutorials, looking forward to seeing what people do with it! And providing instructions for other developers to create their own modules and modify OpenSoundLab, with a special focus on agentic coding. We will also continue to play gigs with OpenSoundLab and enhance the tool for new art pieces, now with a focus on hands interactions. http://www.sphericals.io",
    "hackathon": null,
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "c, c#, mirror, passthrough, unity",
    "github": "https://github.com/SphericalLabs/OpenSoundLab/tree/feature/hands",
    "youtube": "https://www.youtube.com/watch?v=230hncihTxI",
    "demo": null,
    "team": "Ludwig Zeller",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/opensoundlab"
  },
  {
    "title": "The Tarot Experience VR - AI Edition",
    "summary": "The Tarot Experience VR - AI Edition is an innovative virtual reality application that combines tarot card reading with artificial intelligence, providing users with personalized tarot insights and immersive experiences. By leveraging AI-driven interactions, the project enhances traditional tarot readings, making them more dynamic and accessible in a virtual environment.\n\nKEY ACHIEVEMENTS\nThis project stood out by seamlessly integrating immersive VR technology with AI capabilities, allowing for a unique and engaging user experience that resonates with both tarot enthusiasts and newcomers. Its recognition as the winner and runner-up for Best Lifestyle Experience reflects its ability to blend entertainment, spirituality, and cutting-edge technology effectively.\n\nNotable technical implementations include the use of Unity for VR development, integration of AI models like ChatGPT for generating personalized narratives and interpretations, and the incorporation of passthrough capabilities to create an engaging mixed-reality experience. The project also utilizes C# for scripting and leverages the MetaXR platform for enhanced VR functionalities.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring a smooth user experience in VR, managing the complexities of AI-generated content, and balancing the traditional aspects of tarot with modern technology. Additionally, optimizing performance for various hardware, especially in a resource-intensive VR environment, could have posed difficulties.\n\nFUTURE POTENTIAL\nThe Tarot Experience VR - AI Edition has significant potential for evolution by expanding its features, such as incorporating multi-user experiences for group readings, adding a wider variety of tarot decks, or even offering guided meditation sessions. Future iterations could also explore partnerships with wellness platforms or incorporate machine learning to enhance personalization further, making the application a holistic tool for self-discovery and personal growth.",
    "hackathon": null,
    "prize": "Winner Best Lifestyle Experience Runner-up",
    "techStack": "ai, ar, c#, chatgpt, immersive, llm, metaxr, openai, passthrough, quest, unity, vr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=DnuDY5wk3rE",
    "demo": null,
    "team": "Adam James Malone",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/the-tarot-experience-vr-ai-edition"
  },
  {
    "title": "Wishy Washy",
    "summary": "Wishy Washy is a creative project that likely focuses on the visualization or modeling of artistic concepts, possibly in the realm of 3D design or animation, given the tools used. The project may integrate portrait mode features, enhancing user interaction or visual appeal, which aligns with its recognition in best implementation awards.\n\nKEY ACHIEVEMENTS\nWishy Washy distinguished itself by winning multiple awards, including \"Winner\" and \"Best Portrait Mode Implementation.\" These accolades suggest exceptional execution and innovation in user interface or experience design within the context of portrait mode, indicating a well-thought-out approach to user engagement.\n\nThe project utilizes a diverse array of advanced tools, including Blender for 3D modeling, ZBrush for detailed sculpting, and Substance Painter for texturing, showcasing a high level of craftsmanship. The use of Horizon Desktop Editor and Noesis may indicate a focus on interactive elements or user interface design, enhancing the project's overall quality and functionality.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in integrating various software tools effectively, ensuring compatibility between different formats and workflows. Additionally, achieving a seamless user experience in portrait mode could have required extensive testing and iteration to optimize performance and visual fidelity.\n\nFUTURE POTENTIAL\nWishy Washy has the potential to evolve into a more comprehensive platform for 3D art creation and sharing, possibly incorporating features like collaborative design or interactive storytelling. Expanding its functionalities could also include support for augmented reality (AR) or virtual reality (VR) experiences, tapping into emerging technologies and broader user engagement.\n\nInspiration: I set out to build something fast, simple to start, and instantly fun, a vertical, portrait-mode experience that anyone could jump into in seconds. I learned how crucial small feedback loops are in a mobile-friendly game: every movement, tap, and reward needed to feel immediate and satisfying. Throughout development, I also deepened my understanding of Noesis UI, custom scripting workflows, and integrating AI-assisted tools without losing the handmade charm.\n\nChallenges: The biggest challenge was time. Building a fast-paced game requires iteration, but limited playtesting windows made it tough to refine mechanics, observe players, and adjust systems on the fly.\n\nWhat's next: I plan to release seasonal updates, expand progression systems, and introduce a customization store where players can personalize their characters. I‚Äôll continue refining window-cleaning and row-clearing rewards as the world grows.",
    "hackathon": null,
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "blender, horizon-desktop-editor, noesis, photoshop, substance-painter, vs-code, zbrush",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=XxgwNhRKAm8",
    "demo": "https://horizon.meta.com/world/10237615339870207/?locale=en_US",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/wishy-washy"
  },
  {
    "title": "Pack Attack",
    "summary": "Pack Attack Project Summary\n\nPack Attack is a social game designed to engage players in an interactive and collaborative environment, where they can strategize and work together to achieve common goals. The game likely emphasizes teamwork and communication, making it appealing for social play.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative gameplay mechanics and strong emphasis on social interaction, earning it the title of Winner and Honorable Mention for Best Social Game. Its unique approach to fostering collaboration among players set it apart from other entries.\n\nPack Attack was built using advanced tools such as Unreal Engine 5, which allowed for high-quality graphics and immersive gameplay experiences. The use of software like Adobe, Blender, Photoshop, and Substance Painter contributed to the game's visual appeal and detailed asset creation.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to balancing gameplay mechanics to ensure fairness and fun for all players. Additionally, integrating social features while maintaining a smooth user experience could have presented technical hurdles.\n\nFUTURE POTENTIAL: Pack Attack has significant potential for evolution, including the possibility of expanding gameplay with new modes, additional social features, or seasonal content updates. Furthermore, it could leverage community feedback to refine gameplay and enhance user engagement, positioning it well for growth in the multiplayer gaming market.\n\nWhat it does: Pack Attack is a cooperative party game that encourages players to work as a single unit. It brings people together by removing the friction of traditional multiplayer setups. The core loop is simple but challenging: you have to manage a futuristic delivery line. \nThe MR player checks what the delivery robots need, tells it to their couch-colleagues who have to pack the order and send it away. The parcel will land in the MR drop zone so the player in the headset can pick it up, drop it into the canon and shoot it towards the correct robot.\nIt encourages constant (and often loud) communication between all participants. It‚Äôs designed to be chaotic, but when you finally get into a flow state with your team, that shared sense of triumph is unmatched.\n\nInspiration: We‚Äôve always been huge fans of couch co-ops and social games, nothing beats the feeling of screaming instructions at your friends, whether you‚Äôre in the same room or on a call. We wanted to capture that frantic, high-stakes energy found in games like Overcooked or Keep Talking and Nobody Explodes, where the gameplay is 10% mechanics and 90% pure communication. The specific theme came from watching the absolute chaos of the holiday season. We started wondering: with Christmas delivery times getting tighter and logistics getting crazier, what is actually happening behind the scenes? And more importantly, what will the chaotic, automated warehouse of the future look like? That‚Äôs where the idea for Pack Attack was born.\n\nHow it was built: We wanted the game to look stylized and with history, so we started by creating all of the models in Blender and texturing them in Substance Painter to give them that vibrant look. The heavy lifting is done by Unreal Engine 5, which handles the game logic and environment. To ensure a low barrier to entry, we didn't want players to have to download a companion app. Instead, we built a responsive web interface using React. Players simply join the game via a website on their phones, turning their mobile device into a controller instantly: https://pack-attack.app !\n\nChallenges: The biggest hurdle was definitely the \"invisible\" stuff: networking and UX. Onboarding: We wanted it to be seamless. Ensuring data was sent instantly between the mobile web client and the headset without lag was tricky.\nControl Feel & Asymmetry:\nDesigning for asymmetric gameplay on the Quest was a unique challenge. We spent a lot of time iterating on how the MR player interacts with the world versus how the mobile players assist them. The Quest controls needed to feel physical and immersive through actually lifting boxes and firing cannons, while remaining intuitive enough to coordinate with teammates on 2D screens. If the MR mechanics fight the player, the game stops being \"fun-chaotic\" and just becomes frustrating.\n\nAccomplishments: We are incredibly proud of the accessibility. The fact that the MR player can allow their friends to join a session with ease, and be playing inside the game session within seconds without installing anything is a huge win for us. Seeing the seamless session setup and reliable performance between a high-fidelity engine like UE5 and a standard mobile browser feels like magic every time it works.\n\nWhat we learned: Cross-device communication: We learned a massive amount about WebSockets and optimizing data packets to keep latency low between the website and the main MR game.\nThe art of chaos: We learned that chaos needs to be designed. You can't just throw random things at players; you have to give them tools to solve problems, then slowly turn up the heat.\n\nWhat's next: We have a big roadmap ahead! This simple game loop has potential for a lot of exciting directions: Story Mode: We want to add a single-player, story-driven mode to explore the lore of this futuristic delivery company.\nMore Chaos: We plan to implement events, think machinery breaking down, conveyor belts reversing, or power outages that force players to adapt on the fly.\nMobile Depth: We want to give the mobile players more specific, tactile tasks on their screens (like rewiring circuits or stamping packages) to make their role feel even more distinct.",
    "hackathon": null,
    "prize": "Winner Best Social Game Honorable Mention",
    "techStack": "adobe, blender, photoshop, substance-painter, ue5, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=YkWO2qdyeME",
    "demo": "https://pack-attack.app/",
    "team": "Andrew Douglas, Ines Hilz",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/pack-attack"
  },
  {
    "title": "Fluxglove and VR",
    "summary": "Fluxglove and VR is an innovative project that integrates a gesture-controlled glove with virtual reality experiences. By utilizing sensors and programming, it allows users to interact with virtual environments in a more immersive and intuitive way, enhancing user engagement and interactivity.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its unique combination of hardware and software, providing a seamless interface between physical hand movements and virtual actions. Winning the hackathon indicates its potential impact and relevance, showcasing a strong user experience and effective functionality that resonated with judges.\n\nNotable technical implementations include the integration of the MPU6050 motion sensor for accurate gesture detection, the use of Arduino for hardware interfacing, and a robust software stack using HTML5, CSS, and JavaScript for a responsive user interface. The utilization of Python for backend processing also highlights versatility in programming languages.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring accurate sensor calibration and data processing to translate physical gestures into virtual commands reliably. Additionally, maintaining low latency between gesture input and virtual response would be critical for user satisfaction. Balancing the complexity of the system while ensuring ease of use could also pose difficulties.\n\nFUTURE POTENTIAL\nFluxglove and VR could evolve into a commercial product aimed at gaming, training simulations, or therapeutic applications. Future enhancements might include refining the glove‚Äôs design for comfort, expanding compatibility with various VR platforms, and incorporating machine learning algorithms for improved gesture recognition and adaptability.\n\nWhat it does: FluxGlove is a low-cost, haptic-enabled smart glove that translates natural hand movements into digital actions in real-time. Gestural Control: Users can control a mouse cursor, navigate web pages, or play games (like BGMI) simply by moving their hand in the air and bending their fingers.\nHaptic Feedback: The glove vibrates to simulate touch when interacting with virtual objects, adding a layer of immersion.\nAccessibility Mode: It acts as a Human-Computer Interface (HCI) for users who cannot grip a traditional mouse, allowing them to execute complex commands with simple finger flexes.\nWeb Integration: Through our custom platform, the glove connects directly to the browser for calibration and testing without heavy driver installation.\n\nInspiration: The inspiration for FluxGlove (born from our initiative Sparsh Mukthi) came from two starkly different realities. On one hand, we saw the explosion of VR and the Metaverse, locked behind paywalls of hardware costing thousands of dollars. On the other, we witnessed the daily struggles of individuals with motor impairments who find standard keyboards and mice nearly impossible to use. We asked ourselves: Why can't the same technology used for gaming be used to give freedom to those with disabilities? We wanted to build a bridge‚Äîa device that offers \"Touchless Freedom.\" Our goal was to democratize motion capture, bringing high-end haptic feedback and precise tracking down to a sub-$50 price point, making it accessible for both the gamer in a dorm room and the patient undergoing rehabilitation\n\nHow it was built: We adopted a hybrid hardware-software approach, leveraging our background in Electronics & Communication Engineering (ECE).\n\nChallenges: The \"Jitter\" Problem: The raw values from the flex sensors and MPU6050 were incredibly noisy. The cursor would shake uncontrollably. We spent days tuning the Complementary Filter and adding dead-zones in the code to stabilize the input.\nHardware Constraints: Mounting sensors on a flexible glove without breaking connections was a challenge. We had to innovate with cable management to ensure the user had full range of motion without snapping wires.\nLatency: Minimizing the delay between a physical hand movement and the screen action was critical for gaming. We had to optimize our baud rate and streamline the serial packet parsing logic.\n\nAccomplishments: The Price Point: We successfully built a functional prototype for under $50, whereas industry standards cost hundreds or thousands.\nWeb Integration: Getting the glove to talk to a web browser via WebSerial was a huge technical win for us.\nTeam Synergy: As a team of first-year students, coordinating hardware (soldering, circuit design) and software (full-stack web dev) remotely and efficiently was a massive achievement.\nLive Demo: Creating a site where judges can visually see the glove's tracking logic in real-time.\n\nWhat we learned: Sensor Fusion: We gained a deep understanding of how to mathematically combine data from different sensors to create a reliable \"truth.\"\nProduct Design: We learned that \"functional\" isn't enough; the device has to be comfortable. The physical form factor is just as important as the code.\nEmbedded Communication: We mastered UART and serial communication protocols, which are foundational for our ECE degree.\n\nWhat's next: Wireless Evolution: Migrating from Arduino Uno to ESP32 to introduce Bluetooth Low Energy (BLE) and remove the cables entirely.\nAI Integration: Implementing an LSTM (Long Short-Term Memory) neural network to recognize complex, custom gestures (like sign language) automatically.\nGame Development: Building a dedicated Unity SDK so game developers can easily add FluxGlove support to their titles.\nClinical Trials: Partnering with rehabilitation centers to test the \"Sparsh Mukthi\" accessibility features with patients recovering from hand injuries.",
    "hackathon": null,
    "prize": null,
    "techStack": "arduino, css, flex, html5, javascript, mpu6050, python",
    "github": "https://github.com/anuragkr-14/FLUXGLOVES",
    "youtube": "https://www.youtube.com/watch?v=AncF9D27v9A",
    "demo": "https://fluxglove.vercel.app/",
    "team": "Keshav Agrawal, Saket Kumar, Anshul Kumar, Anurag Kumar, Mohak Gupta",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/fluxglove-and-vr"
  },
  {
    "title": "Into The Mind",
    "summary": "\"Into The Mind\" is an immersive virtual experience developed to explore and visualize complex mental landscapes, allowing users to navigate the intricacies of thought processes and emotions. By utilizing advanced technologies, the project aims to create a unique interactive journey that enhances understanding of mental health and cognitive experiences.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its innovative approach to representing abstract mental concepts in a visually engaging way, seamlessly blending art and technology. Its win as both a general winner and Judge‚Äôs Choice highlights its exceptional creativity, technical execution, and the impactful message it conveyed about mental health.\n\n\"Into The Mind\" leveraged Blender for high-quality 3D modeling and animation, while C++ was utilized for performance optimization and gameplay mechanics within Unreal Engine. The project likely incorporated advanced rendering techniques and real-time interactivity to enhance user immersion, providing a dynamic and responsive experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to accurately representing complex psychological concepts in a user-friendly manner. Balancing performance and visual fidelity in a real-time environment, as well as ensuring the experience remained engaging without overwhelming the user, were probably significant hurdles.\n\nFUTURE POTENTIAL\nThe project has substantial potential for evolution into a therapeutic tool or educational platform, possibly integrating features like guided meditation or cognitive behavioral therapy modules. Additionally, expanding the experience to include multiplayer elements or community-driven content could enhance engagement and broaden its impact in mental health awareness and education.",
    "hackathon": null,
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "blender, c++, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=A2s7b85T348",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/into-the-mind"
  },
  {
    "title": "Hardy World Poker",
    "summary": "Hardy World Poker is a digital poker platform designed to enhance the gaming experience through innovative technology and engaging features. Leveraging the Noesis framework, it aims to create a more interactive and immersive poker environment for players.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively integrating advanced gameplay mechanics and user interface elements that enhance player interaction. Its recognition as a hackathon winner and for leveraging Noesis showcases its originality and technical prowess in the competitive landscape.\n\nBuilt with TypeScript, Hardy World Poker likely features strong typing and modularity, improving code quality and maintainability. The use of Noesis suggests advanced rendering techniques and user experience enhancements, contributing to a smooth and visually appealing interface.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to real-time gameplay dynamics, ensuring a seamless user experience under varying network conditions. Additionally, balancing game mechanics to maintain fairness and engagement among players would have required careful consideration.\n\nFUTURE POTENTIAL\nThe project could evolve by integrating AI for personalized gaming experiences, expanding to include additional card games, or incorporating blockchain technology for secure transactions and ownership of in-game assets. This could significantly broaden its appeal and user base in the competitive online gaming market.\n\nWhat it does: Hardy World Poker is a complete Texas Hold'em poker game featuring: Multi-tiered Buy-In System: Players can choose from 6 different cities with varying buy-in amounts (1K, 5K, 10K, 25K, 50K, 100K), each representing a major world city (Las Vegas, New York, London, Tokyo, Dubai, Monaco)\nFull Poker Gameplay: Complete Texas Hold'em implementation with betting rounds, blinds, side pots, all-in scenarios, and hand evaluation\nAI Opponents: Intelligent AI players with different personalities (Tight, Aggressive, Loose, Balanced) that make realistic decisions based on hand strength\nPersistent Statistics: Player stats are tracked and saved, including hands played, wins, losses, and chip counts\nGlobal Leaderboard: Top 10 players are ranked globally based on chip count, stored in world-level persisten\n\nInspiration: Hardy World Poker was inspired by the desire to create an immersive, multiplayer poker experience within Horizon Worlds. The goal was to build a fully-featured Texas Hold'em poker game that combines classic card game mechanics with modern UI/UX design, complete with persistent player statistics, leaderboards, and multiple buy-in tiers representing major cities around the world.\n\nHow it was built: The project was built using: TypeScript: Core game logic, state management, and AI decision-making\nNoesisGUI/XAML: UI framework for creating responsive, data-bound interfaces\nHorizon Worlds API: Network events, persistent storage, and world management\nObject-Oriented Architecture: Modular design with separate classes for:\n\nPokerOverlay: Main game controller and UI management\nPokerGameState: Game state management (players, pot, betting rounds)\nPokerActions: Hand evaluation and game controller logic\nPokerAI: AI decision-making with personality-based behavior\nServerManager: Server-side logic for persistent storage and leaderboard management\nPokerImageLoader: Optimized parallel image loading with exponential backoff\nPokerTimer: Turn timer with countdown and expiration handling Key Technical F\n\nAccomplishments: Complete Poker Implementation: Successfully built a full-featured Texas Hold'em game with all standard rules and edge cases handled\nPerformance Optimizations: Achieved significant performance improvements through algorithm optimization and lazy loading\nRobust State Management: Implemented reliable game state management that handles complex scenarios like multiple all-ins and side pots\nBeautiful UI/UX: Created an intuitive, visually appealing interface with smooth animations and clear feedback\nPersistent Data System: Built a reliable system for saving player stats, chips, and leaderboard data that survives world restarts\nAI Intelligence: Developed AI players that feel realistic and challenging without being predictable\nCrash Protection: Implemented a robust system to prevent data loss even\n\nWhat we learned: Performance Matters: Pre-computation and caching can dramatically improve performance in computationally intensive operations like hand evaluation\nType Safety is Critical: Careful type validation prevents subtle bugs like string concatenation instead of numeric addition\nNetwork Optimization: Debouncing and batching network events significantly reduces server load and improves user experience\nState Management Complexity: Poker game state management requires careful handling of edge cases, especially with all-in scenarios and side pots\nUI/UX Design: Creating intuitive interfaces requires balancing information density with clarity, and animations can greatly enhance user experience\nPersistent Storage Patterns: World-level vs player-level storage requires different strategies, and careful desi\n\nWhat's next: Tournament Mode: Implement tournament-style gameplay with multiple tables and progressive elimination\nPlayer Customization: Allow players to customize their table themes, and card designs\nAchievement System: Add achievements and badges for various accomplishments (winning streaks, big pots, etc.)\nSocial Features: Implement friend lists, private tables, and chat functionality\nAdvanced Statistics: More detailed stats tracking including VPIP, PFR, aggression factor, and hand history\nMobile Optimization: Optimize UI for mobile VR devices\nSound Design: Add immersive sound effects and background music\nTutorial System: Interactive tutorial for new players to learn poker rules\nDaily Challenges: Daily challenges and missions to keep players engaged\nCross-World Leaderboards: Expand leaderboard syste",
    "hackathon": null,
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=i31rOC24T1k",
    "demo": "https://horizon.meta.com/world/25301194902846864/?locale=en_US",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/hardy-world-poker"
  },
  {
    "title": "The Nanauts",
    "summary": "The Nanauts is an immersive experience project that leverages advanced web technologies to create an engaging environment using hand tracking and passthrough camera access. By integrating the Immersive Web SDK and WebXR, it allows users to interact with virtual elements in a seamless manner, enhancing the overall user experience.\n\nKEY ACHIEVEMENTS\nThis project stood out by winning two awards: the overall Winner and Best Immersive Experience Built with Immersive Web SDK. Its innovative use of interactive elements and real-time user engagement through hand tracking made it a compelling and memorable experience for users.\n\nNotable technical implementations include:\n- Hand Tracking: Enables users to interact with the virtual environment naturally and intuitively.\n- Passthrough Camera Access: Provides a mixed-reality experience by blending the real world with virtual elements.\n- WebXR Integration: Allows for cross-platform compatibility and accessibility through web browsers.\n- Web Audio API: Enhances the immersive experience with spatial audio, creating a more engaging sound environment.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance across different devices, ensuring accurate hand tracking in various lighting conditions, and maintaining a seamless user experience while integrating multiple complex technologies. Additionally, debugging immersive experiences can be more complicated than traditional applications.\n\nFUTURE POTENTIAL\nThe Nanauts could evolve into a platform for educational experiences, virtual tourism, or gaming, expanding its audience and applications. Future iterations could incorporate social features, allowing users to interact with each other in shared virtual spaces, or enhance AI integration for dynamic content generation.\n\nWhat it does: As earth‚Äôs goodwill ambassador to The Nanaunts, you help three cute little robot explorers understand our culture and way of life by exploring your environment in mixed reality, answering their questions, and aiding in their quest for understanding, with the Professor guiding your experience.\n\nInspiration: My game was inspired by Walt Disney‚Äôs example of trying to harness the latest in emerging technology to create characters and stories with real heart. Therefore my goal lately has been to incorporate AI services for unique gameplay.\n\nHow it was built: from a base Immersive Web SDK project for its many common XR components\nused Immersive Web Emulator for rapid iteration in the browser\nused Meta Quest scene understanding to dynamically construct a navmesh for the robot agents to navigate according to crowd sim, flocking and game rules\nused Passthrough Camera Access to generate custom environment maps\nGave the player the ability to speak directly to the characters, using a serverless Meta Llama 3.3 endpoint hosted on AWS, which can judge the intent and emotional sentiment of speech given certain game conditions\nTwo interaction modes using either controllers or hand tracking with natural microgestures.\nUsed  the WebXR Hit Test API to get real depth to open portals on tracked surfaces.\nNavigation-driven procedural animations to impart charac\n\nChallenges: I attempted to leverage SAM 3 and SAM-3D-Objects AI models (released mid-November) to recognize and react to more objects in the real-world (https://www.linkedin.com/feed/update/urn:li:activity:7399101129554419712/) but camera intrinsics on Quest browser are still experimental, and I ran into challenges with accuracy, and on a short timescale, I ultimately pivoted the concept to rely on core Quest technology + the more reliably and widely-available Llama.\n\nAccomplishments: I completed all technical and creative work on my own in only three weeks, starting November 21. I‚Äôm proud of not only have having that design and creative intuition, but being an engineer who can see it through.\n\nWhat we learned: I learned more about how to develop and design AI services for innovative gameplay versus solely production efficiencies.\n\nWhat's next: I‚Äôd love to win the contest and have a good reason to keep investing in the technology and title, as I see tremendous possibilities for work and play in interacting with increasingly-intelligent virtual beings in your real-world environment as new AR glasses and computer vision solutions become available in coming years. I could see this becoming a virtual pet title, like a \"mixed reality Tamagotchi,\" encouraging replayability. I want to design for that future.",
    "hackathon": null,
    "prize": "Winner Best Immersive Experience Built with Immersive Web SDK",
    "techStack": "handtracking, hittest, iwsdk, llama3.3, passthroughcameraaccess, webaudioapi, webxr",
    "github": "https://github.com/jameskane05/nanauts",
    "youtube": "https://www.youtube.com/watch?v=7r-29fp4Nbg",
    "demo": "https://jameskane05.github.io/nanauts/",
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/nanauts"
  },
  {
    "title": "Falcon",
    "summary": "Falcon is an innovative social gaming project that leverages advanced technology to create an engaging and interactive gaming experience. It focuses on fostering community interaction and collaboration among players, potentially integrating features that enhance social connectivity within the gaming environment.\n\nKEY ACHIEVEMENTS\nFalcon stood out by winning the Best Social Game award, highlighting its exceptional design and gameplay that effectively promote social interaction. Its unique mechanics and user engagement strategies likely resonated with both judges and participants, showcasing its potential to redefine social gaming experiences.\n\nThe project utilized a combination of C++, Unreal Engine, and EOS (Epic Online Services), indicating robust performance and high-quality graphics. The use of blueprints within Unreal Engine likely enabled rapid prototyping and flexible game mechanics, contributing to a polished final product that engages players visually and interactively.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as balancing gameplay mechanics to ensure fairness while fostering competition, addressing potential technical bugs during development, and designing a seamless user experience that integrates social features without overwhelming players. Additionally, ensuring scalability for a growing user base could have posed significant hurdles.\n\nFUTURE POTENTIAL\nFalcon has the potential to evolve into a comprehensive social gaming platform by incorporating additional features like player-driven content creation, expanded multiplayer capabilities, and cross-platform integration. By continuously engaging with its community and adapting to player feedback, it could further enhance its social dynamics, thereby solidifying its position in the gaming market.\n\nChallenges: Achieving stable colocation for multiple players in small physical spaces.\nBuilding engaging controls for many diffrent categories of vehicles.\nEnsuring synchronized physics across multiplayer sessions.\nCreating track-building tools that remain accessible to casual players.\n\nAccomplishments: A highly responsive MR flight system that feels natural and tactile.\nDistinct gameplay modes offering both competitive and cooperative experiences.\nA creative system for building custom race tracks inside the headset.\nA polished, social experience supported by expressive avatars and intuitive interactions. What we learned We learned how crucial spatial consistency is for social MR experiences, and how small variations in real-world environments can influence perceived gameplay quality.\nWe also gained valuable experience in building scalable multiplayer architecture, designing intuitive hand-based controls, and optimizing physics for mixed-reality conditions.\nMost importantly, we saw how creativity tools empower players and significantly increase engagement. What's next for FALCON We plan t",
    "hackathon": null,
    "prize": "Winner Best Social Game Runner-up",
    "techStack": "blueprints, c++, eos, meta, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=CwuS1JffbFs",
    "demo": null,
    "team": "Wild Vision Games Wild Vision Games",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/falcon-shgxn9"
  },
  {
    "title": "Boopi",
    "summary": "Boopi Project Summary\n\nBoopi is an innovative project that leverages artificial intelligence and generative AI technologies to enhance portrait photography. By utilizing advanced algorithms and APIs, it aims to provide users with tools for creating stunning portrait images, likely with automated enhancements and artistic features.\n\nKEY ACHIEVEMENTS:  \n   Boopi's standout quality lies in its exceptional portrait mode implementation, which earned it recognition as the Best Portrait Mode Implementation at the hackathon. This achievement indicates a high level of technical finesse and user experience, showcasing the project's ability to effectively utilize AI for creative purposes.\n\nThe project is built using a combination of cutting-edge technologies, including AI and generative AI frameworks, along with Horizon APIs for enhanced functionality. The use of TypeScript suggests a focus on maintainable and scalable code, allowing for robust application development and integration of complex features.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges related to algorithm optimization for real-time processing of images, ensuring high-quality output while minimizing processing time. Additionally, user interface design and experience would have been critical to make the technology accessible and engaging for users.\n\nFUTURE POTENTIAL:  \n   Boopi has significant potential for growth by expanding its feature set to include more advanced editing tools, integration with social media platforms for easy sharing, and personalized user experiences through machine learning. Future iterations could also explore collaborative features, allowing multiple users to edit and enhance portraits together in real-time.\n\nWhat it does: In this endless runner, you help Boopi the frog jump from lily pad to lily pad by swiping up on your screen. Eat flies you find along the way to purchase cosmetics in the shop. Don‚Äôt fall into the water though, scary things lurk beneath, waiting for a yummy little snack!\n\nInspiration: When I was a kid, the most memorable game I played on my first-ever iPod Touch was those paper-throw games where you flick your finger to toss a crumpled paper ball into a trash bin. I‚Äôve always wanted to recreate that concept with my own twist, but this time, with a frog.That idea gave me the freedom to go wild with cute and funny creature animations, and to build a childlike, dreamy world that captures the nostalgia I associate with those early gaming memories.\n\nHow it was built: We set out to make a stylized game with simple shapes and memorable characters. Since we wanted to appeal to casual players and teens, we focused on creating vivid, dreamy visuals to grab the viewer‚Äôs attention and a simple yet replayable game loop that keeps them coming back.\n\nChallenges: We mostly faced technical challenges along the way. For example, importing 3D assets and dynamically switching between animation states wasn‚Äôt fully supported, so we had to rely on a hacky workaround. Additionally, a few recent Horizon Worlds Editor updates broke some of our logic with Noesis and TypeScript, which forced us to rework parts of the code.\n\nAccomplishments: We were super excited after figuring out how to implement the hop mechanism, which is core to our gameplay. We managed to factor in the swipe angle and accurately translate it into the frog‚Äôs jump direction. I‚Äôm also proud of the visuals we developed using Blender‚Äôs texture-baking techniques to store realistic light and shadow information directly onto our textures. Early on, we also figured out instantiation for spawning endless scene elements, which helped shape the foundation of our gameplay loop.\n\nWhat we learned: We learned a lot about properly importing assets, textures, and animations from other 3D apps into the editor. We also gained valuable experience with the Noesis editor‚Äîsetting up leaderboards, dynamic quests, and the in-game store, all featuring playful animations.\n\nWhat's next: 1 - We really want the game to feel more social, so we plan to introduce a proper lobby system where players can walk around and hang out before the game begins.\n2 - Additionally, along with a game-changing multiplayer mode where you compete with friends to see who can hop the farthest, we also plan to add multiple biomes, with unique enemy types,  that you can seamlessly travel through. This keeps the visuals constantly interesting and makes players want to keep hopping for more!\n3 - We also plan to add a more fleshed out cosmetic store, add more quest variety, introduce more gameplay altering mechanics through obstacles, power ups, skill tree.",
    "hackathon": null,
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "ai, genai, horizon-apis, noesis, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=sv3uZyZ-_TA",
    "demo": "https://horizon.meta.com/world/902445966279753/?locale=en_GB",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/boopi"
  },
  {
    "title": "MarketView VR",
    "summary": "MarketView VR Project Summary\n\nMarketView VR is an innovative Android application designed to provide users with immersive virtual reality experiences for market analysis and visualization. By leveraging VR technology, it allows users to explore and interact with market data in a dynamic and engaging environment, enhancing decision-making processes in business and finance.\n\nKEY ACHIEVEMENTS: The project distinguished itself by winning the overall hackathon and the Best Android Utility App award. Its success can be attributed to its unique integration of VR technology with practical market analysis tools, offering a novel solution that combines entertainment with utility, thus appealing to a wide audience.\n\nMarketView VR was developed using Android Studio with Kotlin and Compose, showcasing modern Android development practices. The use of VR capabilities enhances user interaction, while the efficient architecture ensures smooth performance, even in resource-intensive scenarios.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in optimizing VR performance on various Android devices, ensuring compatibility and usability across different screen sizes and hardware capabilities. Additionally, integrating complex data visualization techniques within a VR environment may have posed significant technical hurdles.\n\nFUTURE POTENTIAL: MarketView VR has the potential to evolve into a comprehensive market analytics tool, incorporating machine learning algorithms for predictive analytics and personalized insights. Expanding its functionalities to include collaborative features or integration with other financial platforms could further enhance its value proposition, making it indispensable for market analysts and investors.\n\nWhat it does: MarketView VR is a VR‚Äënative stock analysis workspace for Meta Horizon OS. The current P0 build focuses on three panels: a Portfolio Overview Panel showing daily price and percentage changes across tracked stocks, a Stock Detail Chart Panel for exploring intraday and historical trends, and a News Panel surfacing relevant headlines in context. There is no trading or execution; the app is intentionally research‚Äëonly, designed for calm, focused market monitoring in VR.\n\nInspiration: Stock analysis has always been a multi‚Äëscreen activity: serious investors juggle charts, news, and multiple tickers at once, but are constrained by flat monitors and limited screen space. MarketView VR explores how virtual reality can turn that workload into an immersive, spatial environment, giving portfolio management the same ‚Äúmission control‚Äù feel that professionals get from multi‚Äëmonitor desks‚Äîwithout the hardware overhead.\n\nHow it was built: Started with Figma Make for initial mockups, refined in Figma for detail. Connected to Meta Horizon MCP server to incorporate all VR design recommendations. Built as an Android app targeting SDK 36 (compatible with Android 24‚Äì36), using Jetpack Compose for UI. Divided into three layers: UI, business logic, and data‚Äîfetching real stock data via Alpha Vantage (historical prices) and Finhub (stock info/news). Multiple panels implemented via Android activities with default/recommended sizes, plus responsive resizing that auto-adjusts layout. Tested on Meta Quest 3S using Meta Quest Developer Hub. Future versions will migrate data logic to our own backend.\n\nChallenges: The challenge we ran into was designing specifically for VR, which we found very important and thus spent a lot of time on. Balancing information density with VR comfort was tough‚Äîtraditional finance tools compress too much text and data, which doesn't translate well to a headset. It took several iterations to perfect panel sizing, text scale, and contrast for readability without overwhelming users, while also resisting feature creep to stay disciplined for a true P0 release.\n\nAccomplishments: The result is a minimal yet capable P0 app that feels ‚Äúnative‚Äù to VR rather than a flat app ported into 3D. The three‚Äëpanel layout already supports a meaningful daily workflow for monitoring a stock portfolio, and the design leaves clear room for future expansion. The app respects VR ergonomics and interaction patterns while still feeling like a professional‚Äëgrade finance tool.\n\nWhat we learned: Designing for VR forced a rethinking of common finance UI patterns: typography, spacing, and hierarchy all need to be more generous, and interactions must work equally well with hands and controllers. It also highlighted how powerful spatial memory can be‚Äîusers quickly associate specific information with specific positions in their virtual workspace, which is a strong advantage over traditional setups.\n\nWhat's next: The next planned release will add portfolio filters (such as top movers and worst performers) and additional chart types, including candlestick, bar, and comparison views. Future iterations will introduce historical performance tracking based on purchase price and quantity, plus a voice‚Äëdriven AI assistant so users can ask questions like ‚ÄúWhat caused this drop in June?‚Äù while looking at a chart. Over time, the goal is for MarketView VR to grow from a focused research tool into a full spatial ‚ÄúBloomberg of VR‚Äù for individual investors.",
    "hackathon": null,
    "prize": "Winner Best Android Utility App",
    "techStack": "android, android-studio, compose, kotlin, mcp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=XFBR9wV8kLA",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/marketview-vr"
  },
  {
    "title": "‚Äî Mise ‚Äî",
    "summary": "Mise is an immersive experience project designed to leverage spatial computing technologies to create engaging virtual environments. Utilizing the Meta Spatial SDK and Oculus hardware, the project integrates advanced AI capabilities through OpenAI, aiming to enhance user interaction and exploration within these digital spaces.\n\nKEY ACHIEVEMENTS\nMise stood out by winning both the overall hackathon and the award for Best Immersive Experience Built with Spatial SDK. Its unique combination of immersive virtual reality and AI-driven interactivity likely contributed to its recognition, showcasing innovative user engagement and a seamless integration of technologies.\n\nThe project utilized the Meta Spatial SDK to create rich, interactive environments, while the Oculus platform provided an intuitive user interface and immersive experience. The incorporation of OpenAI allowed for dynamic interactions, enabling users to engage with the environment in meaningful ways, such as through conversational AI or adaptive storytelling.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance for real-time interactions in a virtual environment, ensuring compatibility across different devices, and effectively integrating AI functionalities without compromising user experience. Additionally, creating compelling content that fully utilizes the immersive capabilities of the technology may have posed further hurdles.\n\nFUTURE POTENTIAL\nMise has significant potential for evolution into various sectors such as education, entertainment, and training simulations. Future developments could include expanding the platform to support more users simultaneously, incorporating additional AI features for personalized experiences, or even integrating augmented reality elements to blend the virtual and physical worlds seamlessly.\n\nInspiration: We have all had that moment: opening the fridge, staring at a random assortment of ingredients, and thinking, \"I have nothing to eat.\" I wanted to eliminate that decision fatigue entirely. I also wanted to solve a problem every home cook faces: trying to scroll through a recipe on a phone while your hands are covered in flour or sauce. Mise (short for Mise en place) bridges the gap between your physical kitchen and digital guidance. By leveraging the Meta Spatial SDK, I built an assistant that lives inside your kitchen without taking up physical space, making cooking effortless, clean, and genuinely fun.\n\nHow it was built: We built Mise as an AR-first experience tailored for the Meta Quest 3/3S. We heavily utilized the Meta Spatial SDK to ensure high performance (60 FPS+) and reliable tracking.\n\nChallenges: Spatial UI Alignment: Getting VR UI panels to feel naturally placed in a real kitchen without obstructing the user's view of dangerous items (knives/stoves) required careful design and calibration.\nLatency vs. UX: AI recipe generation and visual analysis can be slow. We optimized caching and \"debouncing\" AI calls so the user never feels stuck waiting inside the headset.\nInstruction Translation: Translating flat text recipes into 3D animated guidance was a major UX hurdle. We iterated several times to make the instructions feel native to spatial computing rather than just floating text.\n\nWhat's next: We are just getting started. Here is our roadmap for the next updates: [ ] Smart Grocery Lists: Auto-generation based on low inventory.\n[ ] Hardware Sync: Integration with smart fridges and ovens for auto-timers.\n[ ] Community Cookbook: Share your generated \"Recipe Disks\" with other users. Cooking shouldn't be stressful. With Mise, it's the future.",
    "hackathon": null,
    "prize": "Winner Best Immersive Experience Built with Spatial SDK",
    "techStack": "metaspatialsdk, oculus, openai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=VQT4SE5cFgw",
    "demo": null,
    "team": "Private user",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/mise-9wvaib"
  },
  {
    "title": "AptX",
    "summary": "AptX is an innovative project designed to streamline user authentication and enhance online learning experiences through a seamless integration of various technologies. By leveraging Firebase and Firestore for real-time data management, AptX aims to provide a robust platform for users to access selective online courses efficiently.\n\nKEY ACHIEVEMENTS\nAptX distinguished itself by winning multiple awards at the hackathon, including the Leaf Courses prize and a finalist certificate from CodeCrafters. Its success can be attributed to its user-friendly interface, effective implementation of authentication features, and the ability to deliver personalized learning experiences, making it appealing to both users and judges alike.\n\nThe project incorporates advanced technologies such as Firebase for authentication and real-time database management, Firestore for scalable cloud database solutions, and React for building a responsive user interface. The use of Node.js enhances server-side functionalities, while Gemini and Google Cloud contribute to performance and scalability, ensuring a smooth user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring robust security measures for user data during authentication, especially given the increasing concerns over data privacy. Additionally, integrating multiple technologies and maintaining seamless functionality across various platforms could have posed technical hurdles during development.\n\nFUTURE POTENTIAL\nAptX has significant potential for growth, including expanding its course offerings and incorporating advanced features such as AI-driven personalized learning paths. Future iterations could also explore partnerships with educational institutions to broaden its reach, enhance user engagement through gamification, and leverage analytics to improve course effectiveness and user satisfaction.\n\nWhat it does: AptX is a holistic web application designed as a complete learning and support ecosystem for students with Down syndrome, their teachers, and their guardians.\n\nInspiration: Students with Down syndrome often thrive in learning environments that are visually engaging, patient, and present information in simple, single-concept steps. We noticed a gap in digital tools that are specifically designed to meet these needs from the ground up. Our mission began with a simple question: \"How can we use AI to create a truly adaptive and joyful learning experience for every child?\" We were inspired by: *Study that out of 7 children in a group 2 have down syndrome , only one or none can access standard education The unique learning profiles of students with Down syndrome, who benefit from repetition, multi-sensory input (visual, audio), and positive reinforcement.\n  The dedication of teachers and guardians who spend countless hours adapting standard curriculum.\n  The potent\n\nChallenges: Designing a UI that is extremely simple and accessible without feeling restrictive.\n  Fine-tuning AI prompts to generate content that is genuinely appropriate and encouraging for students with Down syndrome.\n  Integrating multiple user roles (Student, Teacher, Guardian) with distinct but interconnected experiences.\n  Ensuring real-time data synchronization between student actions and the guardian dashboard.\n\nAccomplishments: Building a truly multi-user platform that serves the entire support system around a student.\n  Creating a user experience that prioritizes calmness, clarity, and positive reinforcement.\n  Successfully integrating multiple GenAI capabilities (text simplification, image generation, TTS, chat) into a single, cohesive application.\n  Designing an application that goes beyond academics to include emotional well-being as a core feature.\n\nWhat's next: We plan to grow the prototype into a full learning companion with: Deeper personalization of lesson content based on individual progress.\n  Gamified learning milestones and rewards to boost engagement.\n  Teacher-facing analytics to identify classroom-wide learning trends.\n  Offline mode for lessons to ensure accessibility in all environments.\n  Integration with school information systems (SIS).",
    "hackathon": null,
    "prize": "Winner Leaf Courses - Selective online course; Winner Finalist Certificate; Winner CodeCrafters",
    "techStack": "authentication, css, firebase, firestore, gemini, google-cloud, javascript, node.js, react",
    "github": "https://github.com/QueenMary100/AptX_",
    "youtube": "https://www.youtube.com/watch?v=4GWKm7U9Auc",
    "demo": "https://apt-x-nu.vercel.app/",
    "team": "Mary Mbithe",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/aptx"
  },
  {
    "title": "BeanJam - Clinique Voudou",
    "summary": "BeanJam - Clinique Voudou Project Summary\n\nBeanJam - Clinique Voudou is an innovative project that combines artistic expression with interactive technology, likely focusing on themes related to culture and creativity. While specific details on functionality are not provided, the project seems to engage users in a unique and immersive experience, potentially through a game or interactive platform.\n\nKEY ACHIEVEMENTS:  \n   The project stood out by winning the title of \"Best Artistry in a project\" at HackSussex, showcasing its exceptional visual and artistic design. This recognition indicates a strong emphasis on aesthetic quality and creativity, which likely resonated with judges and participants alike.\n\nBuilt with tools like Gemini, GitHub, Godot, and Google Drive, the project likely incorporates advanced game development techniques and version control for collaborative work. Godot, a popular open-source game engine, suggests that the project may feature engaging gameplay mechanics and rich graphics.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges related to integrating artistic elements with technical functionalities, ensuring a seamless user experience. Additionally, managing collaboration and version control effectively on platforms like GitHub can present difficulties, especially in a hackathon environment with tight deadlines.\n\nFUTURE POTENTIAL:  \n   The project could evolve by expanding its interactive features or incorporating more complex storytelling elements, potentially turning it into a full-fledged game or educational tool. Collaborations with artists or cultural experts could enhance its depth, while introducing new technologies like augmented reality could further engage users and broaden its audience.\n\nWhat it does: Playing as Voudou Jacques the doctor, ghost patients arrive and ask for potion treatments. Potion ingredients appear on screen, Voudou Jacques has to collect the correct amount of each from the shelves and put it in the toad cauldron to create the potion. When the patient's potions are ready, they pay and leave. Rats‚Äô tails are a key ingredient which can be replenished by squishing rats which scurry across the screen.\n\nInspiration: Witch doctors practising magical voodoo medicine in the Louisiana Bayou in 1870s.\n‚ÄòOvercooked‚Äô game where the player has to prepare orders for customers using ingredients.\n\nHow it was built: Used GitHub and Google Drive to collaborate.\nGame created in Godot.\nAll art created by hand in Aseprite.\nMusic composed on Roland synthesiser using Logic.\nLore created by team imagination and research of the setting, with help from Gemini.\n\nChallenges: We wanted to have some more dialogue between Voudou Jacques and the ghost patients, but instead put more attention on smooth and immersive gameplay.\n\nAccomplishments: We have an energetic playable character in an immersive and graphically appealing setting\nAtmospheric music has been composed by the team\nRat squishing is fun for all!\n\nWhat we learned: As a cohesive and collaborative team, we understood each others‚Äô strengths from the outset and supported each other to build on these over 24 hours and bring everything together. \nRed Bull seems like a great idea at the time.\n\nWhat's next: Brackeys Gamejam in February, Uni of Kent hackathon in March 2026.",
    "hackathon": null,
    "prize": "Winner [HackSussex] The Palette of power!: Best Artistry in a project",
    "techStack": "gemini, github, godot, google-drive",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5AAHgG30zzc",
    "demo": "https://drive.google.com/file/d/1yIeAGW1tjxrFyPJhuisdUfIMWRaCDoIq/view?usp=drive_link",
    "team": "Amelia Parsons",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/beanjam-clinique-voudou"
  },
  {
    "title": "Mind Your Mana",
    "summary": "Mind Your Mana: Project Summary\n\nMind Your Mana is an engaging game developed using Unity that incorporates strategic resource management elements, where players must effectively manage their mana to navigate challenges and progress through levels. The use of navmeshplus enhances the game's navigation, allowing for more dynamic and realistic movement within the game environment.\n\nKEY ACHIEVEMENTS: The project stood out for its innovative gameplay mechanics that blend strategy and resource management, resulting in a captivating user experience. Its win as the Grand Prize highlights the team‚Äôs exceptional execution of the game concept, creativity in design, and the ability to engage players with compelling challenges.\n\nThe game leverages C# programming for robust game logic and scripting, while navmeshplus enhances pathfinding capabilities, allowing for smooth character interactions and navigation across complex terrains. The integration of Unity's features contributes to a visually appealing and interactive gameplay experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in balancing gameplay mechanics, ensuring that resource management was engaging without being overwhelming for players. Additionally, optimizing performance and addressing potential bugs, especially with the navmesh implementation, would have been critical to maintain a seamless user experience.\n\nFUTURE POTENTIAL: Mind Your Mana could evolve by incorporating multiplayer elements, allowing players to compete or collaborate in managing resources. Expanding the game with additional levels, varied challenges, and richer storytelling could enhance player engagement and longevity. Additionally, potential integration of AI-driven opponents or cooperative gameplay could further elevate the game's complexity and appeal.\n\nWhat it does: Mind Your Mana is primarily about utilising specific spells to clear sections of a dungeon, by drawing the correct sigils on a hexagonal grid using the mouse. To clear an area, the player must defeat all the enemies in the area, then use the sigil shown on the exit door to cast the correct spell, which opens the door and allows them to progress. There are four spells used - a bolt spell, an explosion spell, a healing spell, and a teleport spell. Each spell consumes mana, which regenerates over time. The explosion and healing spell are 'area of effect' spells - the explosion can deal damage to both the player and any enemies in the blast range, and the healing spell can heal both enemies and the player. The bolt spell is a ranged attack, which deals significant damage should it make contact\n\nInspiration: Mind Your Mana's general style was inspired by several top-down dungeon crawlers, with the artstyle influenced by games such as A Link to the Past. The specific idea of drawing sigils to cast different spells came from both the Minecraft Hex Casting Mod, as well as the 2016 Google Doodle Halloween game.\n\nHow it was built: One of our stretch goals which we did not have time to achieve was allowing for multiplayer. While we did not achieve this goal, this approach lead us to design a very modular game, which is well suited for a multiplayer implementation in the future. Enemy navigation was especially aided by the use of a small library named NavMeshPlus, which provides some convenience Unity convenience functions for generating navigation meshes from tilemaps. Everything else however, was of our own creation. The sigil drawing system uses some relatively simple mathematics and line rendering to draw a path, which is reduced down to a sequence of integers. This sequence of integers is then used to determine the spell which was intended. The enemies make use of an MMORPG style enmity system, implemented as par\n\nChallenges: The main challenges we faced were with setting up the spellcasting system, as well as the shaders and other effects used in the spellcasting. Luckily, these issues were overcome, which allowed us to continue with completing the game. The creation of the navigation meshes was also a challenge, as none of us have had much experience with that in the past!\n\nAccomplishments: We learned new skills in developing with navigation meshes.\nWe managed to develop what we think is a very well-rounded game, with good visuals, considering the time allocated to us.\nManaging to keep our development processes in check, so that we could even consider attempting our stretch goals.\n\nWhat we learned: We learned more about how to function effectively as a team, and manage difficult problems and situations. We were able to ensure we developed our code base in the correct way to keep it clean and maintainable. We also learned a number of new skills in the Unity Game Engine, such as navigation meshes, tilemapping, and more advanced particle effects.\n\nWhat's next: As mentioned, one of our stretch goals was to implement multiplayer. With the code base we have developed, this should not be too difficult to implement in the future. We also have a few ideas for more spells, such as a spell which allows you to swap positions with an enemy.",
    "hackathon": null,
    "prize": "Winner Grand Prize",
    "techStack": "c#, navmeshplus, unity",
    "github": "https://github.com/Nathcat/HackSussex-GameJam-2025",
    "youtube": "https://www.youtube.com/watch?v=2ZFRnfujm7k",
    "demo": null,
    "team": null,
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/mind-your-mana"
  },
  {
    "title": "SunShiftAI",
    "summary": "SunShiftAI is a project that utilizes artificial intelligence to optimize solar energy usage by analyzing weather data and user preferences. The application aims to enhance energy efficiency for households or businesses by providing tailored recommendations for solar energy management.\n\nKEY ACHIEVEMENTS\nThe project stood out by successfully integrating multiple APIs to deliver real-time weather insights and personalized energy solutions. Its recognition as a winner in multiple hackathon categories, including Leaf Courses and CodeCrafters, reflects its innovative approach and practical application in the renewable energy sector.\n\nKey technical implementations include the use of Dart and Flutter for cross-platform mobile app development, combined with the Gemini API for AI-driven insights and OpenWeatherAPI for accurate weather data. The architecture also leverages the Provider package for state management and SharedPreferences for efficient local data storage.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring accurate data integration from multiple APIs, maintaining app performance across devices, and creating a user-friendly interface that effectively communicates complex energy management concepts. Additionally, addressing data privacy concerns related to user information could have posed a significant hurdle.\n\nFUTURE POTENTIAL\nSunShiftAI could evolve into a comprehensive energy management platform by incorporating machine learning algorithms for predictive analytics and expanding its features to include energy cost tracking and savings forecasts. Collaborations with solar panel manufacturers and energy companies could also enhance its functionality and user adoption.",
    "hackathon": null,
    "prize": "Winner Leaf Courses - Selective online course; Winner Finalist Certificate; Winner CodeCrafters",
    "techStack": "dart, flutter, geminiapi, openweatherapi, provider, sharedpreferences",
    "github": "https://github.com/Ayushmaan-PCG",
    "youtube": "https://www.youtube.com/watch?v=0J-wbK8xSao",
    "demo": null,
    "team": "Ayushmaan Bellum",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/sunshiftai"
  },
  {
    "title": "Dear Diary",
    "summary": "Dear Diary is an innovative digital journaling application that allows users to document their thoughts and experiences in a seamless and engaging manner. By leveraging modern web technologies, it offers a user-friendly interface and interactive features that enhance the journaling experience.\n\nKEY ACHIEVEMENTS\nThe project stands out due to its unique combination of aesthetic design and functionality, providing a captivating user experience. Its ability to integrate data visualization through charts and user-friendly navigation likely contributed to its recognition as a hackathon winner.\n\nKey technical implementations include the use of Next.js for server-side rendering, which enhances performance and SEO, and Recharts for dynamic data visualization. The integration of Sanity as a headless CMS allows for flexible content management, while Framer and Shadcn contribute to a sleek and responsive design.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges regarding data privacy and user security, especially in handling personal journal entries. Additionally, maintaining a balance between performance and rich visual features could have posed technical hurdles.\n\nFUTURE POTENTIAL\nDear Diary could evolve into a comprehensive mental wellness platform by incorporating features such as mood tracking, mindfulness exercises, and community sharing options. Integrating AI-driven insights could also provide users with personalized reflections and suggestions based on their journaling patterns.",
    "hackathon": null,
    "prize": null,
    "techStack": "framer, grok, next.js, recharts, sanity, shadcn",
    "github": "https://github.com/somewherelostt/DiaryEmo",
    "youtube": "https://www.youtube.com/watch?v=7GwJDLrbnFM",
    "demo": "https://deardiary.maazx.dev/",
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/dear-diary-u7gt6v"
  },
  {
    "title": "OctoSense",
    "summary": "OctoSense is an innovative mobile application designed to enhance user engagement through real-time data insights. By leveraging APIs and a combination of modern programming frameworks, OctoSense provides users with an intuitive interface to access and analyze data relevant to their specific needs.\n\nKEY ACHIEVEMENTS\nThe project stood out by successfully integrating multiple technologies to create a seamless user experience, demonstrating exceptional functionality and performance. Its win at the hackathon reflects the team's ability to deliver a polished product within a limited timeframe, showcasing creativity and effective problem-solving skills.\n\nOctoSense utilizes Android Studio for native app development and employs Flutter and Dart for cross-platform functionality, allowing for a smooth user interface across devices. The integration with Supabase for backend services and data management, along with the use of APIs for real-time data access, highlights its robust technical architecture.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges around ensuring data security and privacy, especially in handling user data through APIs. Additionally, achieving optimal performance and responsiveness across different devices and screen sizes may have required significant testing and optimization efforts.\n\nFUTURE POTENTIAL\nOctoSense has the potential to evolve into a comprehensive analytics platform by incorporating machine learning algorithms for predictive insights. Future enhancements could include adding features for social sharing, personalized recommendations, or expanding its capabilities to integrate with various data sources, broadening its appeal and functionality.",
    "hackathon": null,
    "prize": null,
    "techStack": "android-studio, api, dart, flutter, gemni, supabase",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=OALGfSFFpWk",
    "demo": "https://drive.google.com/file/d/15NrOOLGn6vCuBxPlqEQqeNQhsgPZWb2H/view?usp=drivesdk",
    "team": "Private user",
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/octosense-af0vhw"
  },
  {
    "title": "Trash Talkers",
    "summary": "Trash Talkers Project Summary\n\nTrash Talkers is an innovative project aimed at addressing environmental concerns related to waste management. By leveraging geographic information systems, it provides users with interactive maps that highlight litter hotspots and promote community engagement in clean-up efforts.\n\nKEY ACHIEVEMENTS: The project stood out by effectively combining technology with community action, making it easier for users to visualize waste accumulation in their areas. Its win in the hackathon reflects the project's potential for real-world impact and community mobilization.\n\nTrash Talkers utilized HTML for the front-end interface, along with Leaflet for interactive mapping capabilities. The integration of vector layers and PostGIS allowed for robust geographic data handling, enabling real-time updates and detailed mapping of trash locations.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data accuracy and user engagement. Collecting reliable data on littering locations and ensuring consistent community participation in reporting and cleanup efforts could have been significant hurdles.\n\nFUTURE POTENTIAL: The project could evolve by incorporating machine learning algorithms to predict litter patterns, enhancing user engagement through gamification, or expanding to include partnerships with local governments and organizations for broader impact. Additionally, creating a mobile app could increase accessibility and participation from a wider audience.\n\nWhat it does: Trash Talker is a simple-to-use, appealing website designed to minimize trash and broken community equipment, keeping our streets safe and clean. The website is incredibly user-friendly, as volunteers can help out in three ways. Report an issue. If someone spots something that needs attention, they can capture a photo of the issue and select a category for it, followed by an optional description (e.g., ‚ÄúBroken glass near the playground‚Äù). They can use either their live location or select a location on our map to show us exactly where the issue is.\nJoin our crew of dedicated volunteers and make a direct impact on our community. They can use the Google Form linked on the website to sign up for community clean-up initiatives and choose the tasks, timings, and roles that work best for them. Th\n\nInspiration: We started our brainstorming ideas by reading the rules and reviewing the hackathon guide. We needed to choose a specific focus area for the project (that is solvable, of course), and find common ground since we both know different language models. We scrolled through a catalog of problems to find the 80,000 Hours website, but our personal fits varied at first. While one partner was excited about combating engineered pandemics using biotechnology, another was intrigued to study the moral status of digital minds. We wanted to create a project that aligned with our personal vision, so we kept going through the brainstorming process until we found something we both enjoyed‚Äîcleaning our communities. This problem is solvable, neglected, and is something that closely aligns with our social and m\n\nHow it was built: We used GitHub and followed a standard HTML5 structure. We used standard meta tags to make the site mobile-friendly as well (using viewport) and used Google Fonts (Poppins) by importing the font family. Font Awesome was used for icons like the camera. CSS Styles\nRoot variables in our CSS styles defined the basic color palette and organized it in one common place, so we can easily change it later. The Navbar is fixed to the top with a glass-like blur effect. There are four feature cards at the top (Report, Volunteer, Leaderboard, Donate) that animate/hover up when you move your mouse over them. The Navigation Bar\nTrash Talker is displayed in a stacked design, and each link has a unique hover color corresponding to its section in Nav Links (e.g., Pink for Report, Purple for Leaderboard). The\n\nChallenges: Our primary challenge while completing this project was time. We were challenged to brainstorm a problem, create a prototype, test our website, and present it as well‚Äîall in such a short time frame of a few days. We also used an API to track the location of the user, and this took a lot of time to design. We had to examine a lot of websites and their designs to draw inspiration for the design of our own. Finally, uploading the logo to our website was also a challenge. We designed the thumbnail for our project submission using Canva, and we were also planning on using Canva to make a logo to attach to our website. While we tried downloading both JPG and PNG files to upload to GitHub, neither worked, even though we had no errors in our code. This was a disappointing challenge for us that we\n\nAccomplishments: The interface allows users to report community civic issues in real-time, anywhere they are! It works in every area of the world (provided there is stable internet connectivity). We are proud of the responsiveness of our interface. It can be accessed on all devices, both desktop and mobile. It is a user-friendly public communication system that allows for real change to happen, relying on the power of numbers. It is also interactive. Buttons and cards lift up when hovered over, which creates real and engaging user interaction. Finally, the same colors and fonts defined in the CSS variables are consistently used throughout the page, giving the page a clean, aesthetic look.\n\nWhat we learned: We learned many useful things from this project. Talking to each other has improved our communication skills, as we have consistently needed to text and schedule times for calls this weekend. We also learned valuable copyright policies and figured out how to license our project under the MIT License. Finally, we have learned to find a balance between our technical skills and our presentational skills, as we need to excel in both areas for a successful submission.\n\nWhat's next: Trash Talkers needs a consistent supply of donors and volunteers to keep it running. While it is likely that there will be many issue submissions, we expect to have a lack of donors and volunteers. Currently, we have a leaderboard for top donors and top volunteers on a weekly, monthly, and all-time basis. We hope this system will incentivize more Trash Talkers to join our community. We have edited our ‚ÄúDonate‚Äù tab to automatically update the currency of money based on the location of the user to encourage global participation. However, our website‚Äôs data on leaderboards, as well as the donate tab, are currently just sample statistics used for demo purposes. We need a clearer, more efficient system to truly expand our interface globally across all communities.",
    "hackathon": null,
    "prize": "Winner Finalist Certificate",
    "techStack": "html, leaflet-vector-layers-postgis",
    "github": "https://github.com/Archit992008/TrashTalkers?tab=readme-ov-file",
    "youtube": "https://www.youtube.com/watch?v=LqLju7wULh4",
    "demo": null,
    "team": null,
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/trash-talkers"
  },
  {
    "title": "The Attack of the Spoopy Little Guys",
    "summary": "\"The Attack of the Spoopy Little Guys\" is a game that likely combines elements of traditional role-playing and magic systems within a whimsical, spooky theme. Players probably engage with charming yet mischievous characters, navigating challenges that involve strategy and creativity.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively blending traditional RPG mechanics with innovative gameplay elements that captivate players. It won the \"Best traditional/rpg magic\" award at HackSussex, highlighting its originality and execution in creating an engaging magical experience within a unique narrative.\n\nThe game was built using Blender for 3D modeling, C# for scripting, and Unity as the game engine. Notable technical implementations likely include well-designed character animations, a robust magic system, and a seamless integration of user-friendly gameplay mechanics that enhance the overall player experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in balancing gameplay mechanics, ensuring that the magic system was both fun and intuitive. Additionally, time constraints during the hackathon may have limited their ability to refine graphics and polish gameplay, requiring quick problem-solving and prioritization of features.\n\nFUTURE POTENTIAL\nThis project could evolve into a full-fledged game with expanded storylines, multiplayer capabilities, and more intricate magic systems. Future iterations could explore deeper character development, seasonal updates, and community-driven content, enhancing player engagement and longevity of the game.\n\nWhat it does: The player controls a wizard in third person, and has to fight different types of ghosts using spells that they unlock along the way. Damage dealt from each attack is dependant on the roll of a dice, similar to D&D. After each wave of ghosties, the player can choose to roll a buff dice, a d12 that randomly picks a buff or debuff for the player or all of enemies.\n\nInspiration: Our main inspiration was Google's 2016 Halloween game, which had a cat wizard fighting different ghosts, using the mouse to draw simple shapes. We then found some ghost assets on the Unity Store and decided to expand upon the game in 3D with a roguelike structure. As well as this, we were heavily inspired by Dungeons and Dragons (AKA D&D), and decided to use some of the systems in that game to make our project more complex. We were also inspired by survival.io with the ongoing battle element.\n\nHow it was built: We built our project in Unity, using assets and animations from the Unity Store. Only the dice were created by us - we do not claim any other art, assets, or animations as our own. We also used GitHub for version control and to share our project with each other whenever necessary.\n\nChallenges: A couple of group members had other plans on the Sunday so they had to leave early. This meant that we were on an extreme time-crunch! As well as this, our lack of an artist on the team meant that we had to find, download and figure out how to use free assets available on the Unity Store. Another challenge was getting the character and the terrain to interact correctly. We had created the player on a plane and stupidly created the terrain in a different scene, so when merging we had to recreate the player. However, we didn't realise the the terrain was way too big for the character, causing movement problems when scaling the player up, and looking slightly ridiculous when at the right size. We fixed this by trying to resize the terrain, but figuring this out and then fixing it took about 3\n\nAccomplishments: We completed this game in 3D, despite that being significantly harder and more work. Also, we managed to create our own dice asset in Blender for the combat system. We are also really proud of how pretty the environment turned out to be: we feel like we really encapsulated the enchanted and slightly spooky vibe that we were going for. The lighting really makes this project shine! No pun intended...\n\nWhat we learned: We learned how to better avoid merge collisions when pushing to GitHub, and generally improved our skills in Unity - especially debugging. In addition, we learned how to stay calm under pressure when working as a team with a strict deadline, successfully avoiding any fallings out. We also learned how to do a lot of things individually, most notably lighting, switching scenes, and Blender.\n\nWhat's next: During the summer, we may add some additional features like more spells, more complicated fighting systems, and cutscenes. Currently, we don't have a dedicated artist on this project, so creating cutscenes for a backstory is quite difficult. If we had more time, it would be great to create all the assets and models ourselves in Blender.",
    "hackathon": null,
    "prize": "Winner [HackSussex] Roll 20 for a win: Best traditional/rpg magic",
    "techStack": "blender, c#, unity",
    "github": "https://github.com/ChweeBee/GameJam25",
    "youtube": "https://www.youtube.com/watch?v=3w80mFvIFCU",
    "demo": null,
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/ghosts-in-the-woods"
  },
  {
    "title": "AI Code Mentor",
    "summary": "AI Code Mentor\n\nAI Code Mentor is an innovative application designed to assist developers by providing real-time coding guidance and mentorship through artificial intelligence. The platform supports multiple operating systems, allowing users to receive tailored coding advice, troubleshoot issues, and enhance their programming skills in an interactive manner.\n\nKEY ACHIEVEMENTS: The project won the hackathon due to its user-friendly interface and effective use of AI to deliver personalized coding support. Its cross-platform capability ensures accessibility for developers on various devices, making it a versatile tool for both novice and experienced programmers.\n\nThe project leverages Flutter for a seamless cross-platform experience, utilizing Dart for development and incorporating essential libraries like Riverpod for state management. The integration of AI technologies such as Gemini and OpenRouter enhances the application‚Äôs ability to provide intelligent code suggestions and mentorship.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring the AI's accuracy and relevance in code suggestions, as well as maintaining performance across different platforms. Additionally, integrating various technologies and managing state effectively in a real-time application could have posed significant technical hurdles.\n\nFUTURE POTENTIAL: AI Code Mentor could evolve by incorporating more advanced AI algorithms for deeper learning and improved suggestions. Future iterations might include features like collaborative coding or peer mentorship, as well as integrations with popular coding platforms to enhance the user experience further. Expansion into additional programming languages and support for more complex projects could also broaden its appeal.\n\nWhat it does: AI Code Mentor provides instant, intelligent code analysis across four domains: üêõ Bug Fixer\nDetects syntax errors, logic flaws, runtime exceptions\nSide-by-side diff view with beginner/intermediate/expert explanations\nüîí Security Scanner\nIdentifies SQL injection, XSS, hardcoded secrets\nOWASP-compliant fixes with risk scores\n‚ö° Performance Analyzer\nDetects O(n¬≤) algorithms, memory leaks, bottlenecks\nVisualizes complexity with charts\nüîß Code Refactor\nModernizes legacy code with clean architecture\nSuggests design patterns, removes code smells\nApp Screenshots\n\nInspiration: I've wasted countless hours debugging simple errors that AI could catch instantly. I noticed 70% of security vulnerabilities are preventable, yet most developers lack accessible tools. I wanted to create an intelligent coding companion that fixes bugs and teaches why they occurred‚Äîthe mentor I wish I had when learning to code.\n\nChallenges: Problem:\n\nAccomplishments: ‚ú® Multi-Provider Orchestration - Seamless failover between 3 AI providers\n‚ú® Educational Impact - 3-tier explanations adapt to skill level\n‚ú® Zero-Crash Architecture - Graceful error handling, no raw exceptions shown\n‚ú® Speed - Average analysis time: 2.1s (10x faster than manual review)\n‚ú® Cross-Platform - One codebase runs on Android, iOS, Web, Desktop\n\nWhat we learned: 1. AI Engineering ‚â† Prompt Engineering\nProduction AI apps need structured outputs, fallback parsing, and response validation‚Äînot just good prompts. 2. JSON Parsing Edge Cases\nDifferent AI models return wildly different formats. Built a universal sanitizer handling unescaped newlines, control characters, and invalid escapes. 3. Security First\nGitHub secret scanning taught me to use .env files properly: 4. UX > Technical Complexity\nRewrote error messages from:\n\nWhat's next: Q1 2026: VS Code Extension üîå",
    "hackathon": null,
    "prize": null,
    "techStack": "android, cross-platform, dart, flutter, flutter-dotenv, gemini, groq, ios, macos, material-design-3, openrouter, riverpod, sharedpreferences, website, windows",
    "github": "https://github.com/assassinaj602/ai_code_mentor",
    "youtube": null,
    "demo": "https://ai-code-mentor-my7etjczx-asadullahaj602-9700s-projects.vercel.app/",
    "team": null,
    "date": "2025-12-06",
    "projectUrl": "https://devpost.com/software/ai-code-mentor-ghpdf7"
  },
  {
    "title": "HandCraft XR: Winter Edition",
    "summary": "HandCraft XR: Winter Edition\n\nHandCraft XR: Winter Edition is an immersive experience designed to engage users in a virtual crafting environment during the winter season. It allows users to create and manipulate 3D objects using hand-tracking technology, enhancing the interaction with the virtual space and providing a unique, seasonal crafting experience.\n\nKEY ACHIEVEMENTS: The project stands out for its innovative use of hand-tracking technology and its seasonal theme, which resonates with users looking for engaging winter activities. Winning both the overall competition and the Judge's Choice indicates its exceptional execution and ability to captivate both judges and participants alike.\n\nNotable technical implementations include the use of Havok for physics simulations, enabling realistic interactions with crafted objects. The integration of WebXR allows for a seamless virtual reality experience across different devices, while the utilization of Three.js and TypeScript enhances the rendering of 3D graphics and improves code maintainability.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in optimizing the hand-tracking responsiveness and accuracy, particularly in a dynamic crafting environment. Additionally, ensuring compatibility across various devices and managing the complexity of 3D modeling in a winter-themed setting may have posed significant hurdles.\n\nFUTURE POTENTIAL: HandCraft XR could evolve into a broader platform that offers seasonal crafting experiences throughout the year, incorporating various themes and user-generated content. Future enhancements might include social features that allow users to collaborate, share creations, or participate in virtual crafting events, further increasing engagement and community building.\n\nWhat it does: HandCraft XR is a calm Mixed Reality sandbox where you use natural hand gestures to build a festive miniature Christmas Market on a virtual table. You can: Pick blocks from a floating tablet\nPinch to spawn them in your hand\nScale, rotate, and position them intuitively\nSnap pieces onto the table or attach decorations to other objects\nGrab and throw away unwanted pieces\nWatch snow fall and clouds drift overhead\nBuild a peaceful winter village that automatically saves and loads It‚Äôs simple, tactile, and relaxing - a tiny holiday world you create with your hands.\n\nInspiration: I love programming, VR, and building things. Naturally, I wondered: why not create a little thematic playground - something like virtual LEGO? A winter theme felt perfect: quiet snow, warm lights, and a tiny Christmas Market that you can bring to life piece by piece.\n\nHow it was built: The experience is built specifically for Meta Quest devices using Immersive Web SDK, Three.js, WebXR, and a fully hand-tracked interaction system. It runs in the Quest Browser without installation. All interactions use the WebXR Hand Tracking API, and the trailer footage was recorded directly from a Meta Quest 3 in a standalone mode. Core systems include: A custom pinch-based spawning mechanic\nTwo-hand scaling/rotation with visual outlines\nSnapping logic for table placement and decoration attachment\nA virtual tablet UI with paginated block icons\nA lightweight lighting setup optimized for standalone performance\nInstanced meshes, dynamic batching, BVH acceleration, and adaptive shadow update system for smooth WebXR performance\nAutomatic save/restore via local storage All assets and interacti\n\nChallenges: Performance in WebXR: Standard materials and shadows were too heavy, so lighting had to be redesigned around lighter, more stable options.\nMissing features in Immersive Web SDK: To achieve the interaction patterns and stability I needed, I studied, forked, and extended the SDK: https://github.com/evstinik/immersive-web-sdk\nLimited time for content: The block library is curated but small, so prioritizing which items give the most ‚Äúholiday feel‚Äù was important.\n\nAccomplishments: A fully hand-tracked building system that feels natural and intuitive\nA single-pass outline effect implemented via the stencil buffer\nEfficient shadows using an adaptive, on-demand shadow system with a dedicated micro shadow camera\nSmooth performance even with many objects, shadows, mixed reality, physics and UI\nA cozy, relaxing atmosphere\nGetting it all done within limited time, while keeping the core experience polished\nMade it from scratch in just a month during the competition\n\nWhat we learned: How to structure a WebXR project using ECS architecture\nHow stencil buffers work\nHow to identify CPU or GPU bottlenecks, and distinguish vertex-bound from fragment-bound performance issues\nHow to build a dynamic batching system with BatchedMesh and improve interactions using instance-based BVH structures\nHow to optimize shadows to be able to run on standalone Quest 3 device\n\nWhat's next: Future improvements I'm excited about: WebRTC-based multiplayer building with friends\nSharing worlds or exporting creations\nNight mode with warm lights and glowing decorations\nPeople and animated elements to bring the Christmas Market to life\nMore interaction types (stacking, attaching objects, advanced snaps)\nExpanded sound design with more satisfying feedback and ambience\nAdditional themed kits (Horror Mansion, City, etc.) This version lays the foundation, and I'm excited to grow it into a richer, more expressive Mixed Reality building experience.",
    "hackathon": null,
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "havok, iwsdk, three.js, typescript, webxr",
    "github": "https://github.com/evstinik/immersive-web-sdk",
    "youtube": "https://www.youtube.com/watch?v=4KbKIA9H5_Y",
    "demo": "https://handcraft-xr.netlify.app/",
    "team": "Nikita Evstigneev",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/handcraft-xr-winter-edition"
  },
  {
    "title": "Disco Flip",
    "summary": "Disco Flip is an innovative project designed to enhance user engagement through an immersive experience in a virtual environment. By integrating various technologies, it aims to create a seamless and enjoyable onboarding process for new users, facilitating their transition into the platform.\n\nKEY ACHIEVEMENTS\nDisco Flip stood out by winning two awards: the overall winner and the Best New User Onboarding Experience. This recognition is attributed to its effectiveness in simplifying user interaction and making the onboarding journey enjoyable, ultimately increasing user retention and satisfaction.\n\nThe project leverages several advanced technologies, including Horizon Worlds for creating immersive environments, Houdini for procedural content generation, and TypeScript for robust coding practices. The integration of tools like Noesis and TapToMove enhances user interactivity, enabling fluid navigation within the virtual space.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring compatibility across different devices and platforms, maintaining performance and responsiveness in a resource-intensive virtual environment, and balancing user engagement with complexity during the onboarding process.\n\nFUTURE POTENTIAL\nDisco Flip has significant potential for evolution by incorporating AI-driven personalization features that adapt the onboarding experience to individual user preferences. Additionally, expanding the platform to include social features could enhance community building, further driving user engagement and retention.\n\nWhat it does: The main puzzle concept of Disco Flip is to flip on and off the three music tracks, Drums, Bass, and Guitar/Keys. When all three tracks are on at the same time,  strings and synths also get added. \nWith all three tracks enabled, players must reach the Disco Ball goal to solve the puzzle, avoiding obstacles along the way Disco Flip features 50+ levels, an in-game Fashion store, Leaderboards and Quests, and an Explore mode where players can venture through the disco club and take in the vibes.\n\nInspiration: The inspiration for Disco Flip comes from a Mixed Reality prototype we once made in Unity, which would spawn turnstiles in the players living room and dynamically create a workable puzzle from their open floor space. It worked well, but we knew many players wouldn't have enough floor space to get a proper variety of puzzles, and the market for Mixed Reality exclusive games might be limiting, but from that experiment Disco Flip was born. Disco Flip takes the best parts of the idea and adds fun funky disco music to the mix, solidifying it as a great and addicting mobile title, where players can dig in to the tight puzzle game-loop, running time trials and competing for high scores.\n\nHow it was built: Horizon Worlds, Typescript, Noesis Studio, Houdini Indie (for texturing and game art).\nHorizon's GenAI Mesh Generation, and Sound Generation. Central to the game are the Tap-To-Move API, and Portrait API - the game couldnt exist without either of these two, they're terrific additions to the Horizon Worlds codebase. \nWe also used Meta's NPCs to serve as the games dance troupe.\n\nChallenges: Learning Noesis and its UI paradigms. Creating puzzles was a challenge, where puzzles need to be very dependable in their design and execution. A puzzle that doesnt work, or is too easy/exploitable makes or breaks the fun.\n\nAccomplishments: I'm proud of making a puzzle game with a lot of content, as its not a genre I've done before. I think i really succeeded in capturing the essence of 1970's Disco in the games look and feel which wasnt easy; theres a fine line between tacky and stylish.\n\nWhat we learned: I learnt all about UI programming from a Noesis standpoint, also using Portrait and Tap-To-Move features.\n\nWhat's next: The Puzzle/Music genre combo proved to be really engaging and immersive. We would love to add to the musical palette and explore other genres such as K-Pop either as DLC for Disco Flip or as K-FLIP or similar. We'd love to further enhance the game's social aspect further, join your friends and join in the dance troupe, or sabotage the puzzle player with additional obstacles. Or just hang out in the club together. We'd love to support VR properly, it works already experimentally but it wasnt a focus for this contest. With some minimal changes I think we could do it.",
    "hackathon": null,
    "prize": "Winner Best New User Onboarding Experience",
    "techStack": "horizonworlds, houdini, noesis, portrait, taptomove, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Cdc1boa_tjc",
    "demo": "https://horizon.meta.com/world/10172786784620181",
    "team": "Alexandre Dubois",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/disco-flip"
  },
  {
    "title": "TakeOver 1600-SF22",
    "summary": "TakeOver 1600-SF22 is an innovative mixed and virtual reality application that utilizes hand tracking technology to create immersive experiences. The project likely enables users to interact seamlessly within a virtual environment, leveraging the capabilities of advanced hand tracking to enhance user engagement and interaction.\n\nKEY ACHIEVEMENTS\nThe project distinguished itself by winning first place in both the Mixed & Virtual Reality Category and for Best Implementation of Hand Tracking by Meta. These accolades indicate exceptional execution, creativity, and effective use of technology, showcasing the team's ability to push the boundaries of user interaction in virtual spaces.\n\nTakeOver 1600-SF22 was built using C# and Unity, two powerful tools for game and application development. The integration of the Meta SDK and Photon for real-time networking likely facilitated high-quality multiplayer experiences, while the use of colocation technology ensured accurate tracking and interaction in shared virtual environments.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the precision and reliability of hand tracking, ensuring that the technology performed well across different user scenarios. Additionally, managing the complexities of real-time multiplayer interactions while maintaining a smooth and engaging user experience would have required careful optimization and testing.\n\nFUTURE POTENTIAL\nThis project has significant potential for evolution by expanding its functionalities to include more diverse virtual environments or applications, such as gaming, training simulations, or remote collaboration tools. Further developments could also explore enhanced AI integration for smarter user interactions and broader compatibility with various VR/AR hardware.\n\nWhat it does: Order‚Äôs Up transforms any room into a shared mixed-reality kitchen. When the game starts, ingredients appear around the room like tomatoes, bread, cheese, lettuce, and meats. Players use hand-tracking shortcuts to pick up utensils (a knife and a pan), prepare ingredients by chopping and frying them, and assemble fully custom burgers and sandwiches. Customers appear and place orders that print on virtual receipts. Players work together to find, prepare, and assemble ingredients as per the orders. When dishes are ready, the customers rate them. Your job is to: Read the order ticket\nAssemble the dish based on requirements\nMatch the dish to the correct customer\nSatisfy the picky customers Customer ratings depend on: Ingredient correctness\nPreparation level\nAssembly accuracy The fun isn‚Äôt just\n\nInspiration: We knew from the start that we wanted to build something co-located: an experience that feels better when people are physically together, not just online. For us, mixed reality isn‚Äôt just placing objects in a room. It‚Äôs about creating shared presence, laughter, and chaos, where the people in the room matter as much as the digital content. Early brainstorms ranged from a bomb defusal puzzle to a *stacking challenge, but the pattern was clear: we wanted something intuitive, physical, and instantly fun. We wanted a game anyone could understand in seconds, where the learning curve is tiny and the chaos curve is huge. A key inspiration came from a teammate who wanted something he could play with his kids. We needed something simple enough to learn instantly, playful enough to laugh over, and im\n\nHow it was built: We built Order‚Äôs Up using the Meta XR SDK and their Building Blocks as the foundation for mixed reality features: Colocation via shared spatial anchors so every player sees the same virtual kitchen mapped into their physical environment\nHand-tracking and pose detection for grabbing, slicing, flipping, and throwing without controllers\nMeta Spatial Audio for positional kitchen sounds: sizzling pans, chopping, and customer reactions\nPhoton Fusion Networking to synchronize ingredients, utensils, and interactions across headsets in real-time Each ingredient has around 3 states: raw, prepared, burnt. (Choppable ingredients do not get burnt!) Each state changes how the ingredient looks, scores, and reacts. Additional systems include: Physics-based ingredient throwing\nCollision-aware sandwich asse\n\nChallenges: The first major challenge was co-location. Early prototypes showed drift and anchor inconsistencies, which caused ingredients to misalign between players. We had to learn: Which objects required strict network synchronization\nWhich could live locally\nHow to minimize network traffic while staying responsive\nHow to pass a simple cube back and forth We also underestimated the difficulty of gesture reliability. Detecting the difference between a grab, slice, and throw requires tuned thresholds and noise smoothing. Hand-tracked design is only fun when interactions feel forgiving and responsive. Lastly, building an end-to-end experience under hackathon time meant stripping down mechanics to what was instantly readable: > ‚ÄúIf you can grab it, you can cook it.‚Äù TLDR: Simple design is the hardest d\n\nAccomplishments: A fully working co-located MR game built in a weekend supporting up to 20 users\nA complete game loop: prep ‚Üí assemble ‚Üí ticket match ‚Üí rating\nHand-only interactions: no controllers, no menus\nReal-time networking with Photon Fusion\nStable shared spatial anchor for a full session The moment someone catches a flying tomato and yells ‚ÄúCut this!‚Äù at their teammate is the exact magic we were aiming for.\n\nWhat we learned: We learned that co-location is the experience. When multiple players share a real space with virtual objects, the game stops feeling like VR and becomes a shared physical game that just happens to use digital ingredients. Building Order‚Äôs Up taught us practical lessons about mixed reality as a product and as a technical discipline. The experience required us to think about spatial computing as a system that blends human behavior, networking constraints, and real world environments. The strongest response came from shared moments. Two people arguing about how to cook virtual bacon created more engagement than any shader or physics improvement. Mixed reality is most compelling when it creates a feeling of being together in a world that exists between physical space and digital logic.\n\nWhat's next: We‚Äôre excited to continue development with features like: More utensils (spatula, grill, toaster, blender)\nProcedural customer personalities and orders\nIngredient physics ‚Äúchaos mode‚Äù\nMultiplayer scoring and leaderboards\nCustom dishes and community recipes\nCosmetic upgrades for chefs\nInteractions between chefs (like high fives!) Long term, we see Order‚Äôs Up as a platform for social mixed reality. The core loop, grab and prepare and deliver, is simple enough for anyone to learn in seconds, but the underlying systems have the depth to keep people laughing, arguing, and trying new strategies. Our goal is to grow the experience into something families and friends can pull out in any room for a shared moment, and to show that co-located MR can create real engagement in a casual setting where th",
    "hackathon": null,
    "prize": "Winner Mixed & Virtual Reality Category by Meta 1st Place; Winner Best Implementation of Hand Tracking by Meta 1st Place",
    "techStack": "c#, colocation, metasdk, photon, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=AKSFZrCOvos",
    "demo": "https://www.meta.com/s/6mEmA6Q65",
    "team": "Nishka Awasthi, Cyril Medabalimi, Michael Deering",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/dinerdash"
  },
  {
    "title": "700-SF15 SWANS",
    "summary": "The 700-SF15 SWANS project integrates AI technologies with camera access to enhance user interaction and experience, likely focusing on real-time image processing or data analysis. The project aims to create an innovative application that leverages computer vision and natural language processing to provide meaningful insights or functionalities.\n\nKEY ACHIEVEMENTS\nThis project stood out by effectively combining multiple advanced technologies, which not only showcased the team‚Äôs technical expertise but also resulted in a practical and user-friendly application. Winning the AI with Camera Access by Meta Runner-Up prize highlights its originality and potential impact in the field of AI and computer vision.\n\nNotable technical implementations include the use of Unity for interactive experiences, Flask for backend services, and OpenAI for natural language processing capabilities. The integration of various APIs, such as InsightFace for face recognition and Wit.ai for voice interaction, demonstrates a sophisticated approach to AI and machine learning.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the integration of multiple technologies, ensuring smooth communication between different APIs and frameworks. Additionally, achieving real-time processing and maintaining high accuracy in AI functionalities could have posed significant technical hurdles.\n\nFUTURE POTENTIAL\nThe 700-SF15 SWANS project has the potential to evolve into a full-fledged application or platform that can be used in various domains such as education, entertainment, or security. Future enhancements could include expanding the AI capabilities, optimizing performance, and exploring additional use cases that leverage the existing technology stack for broader applications.\n\nWhat it does: SWANS is an AI/AR copilot that follows a service worker across every gig, restaurant, lounge, bar, caf√©, or venue. It: Converts live table conversations into structured orders using AI Displays this data on a subtle AR HUD for the worker Tracks regulars and ‚ÄúSWANS‚Äù (VIPs) to elevate guest experience Helps workers avoid mistakes and reduce cognitive load Positions floating indicators above guests to guide re-engagement (WIP) Eliminates the need for pen+pad, POS terminals, or memorization Empowers workers to deliver premium service anywhere Helps workers increase tips, speed, and consistency The result:\nFaster service, fewer mistakes, higher tips, and premium hospitality delivered by anyone, anywhere.\n\nInspiration: Service work is the largest labor category on Earth ‚Äî over 100 million people worldwide. Most juggle multiple jobs, memorize dozens of menus, adapt to constantly changing environments, and are expected to deliver premium, personalized service every single shift.\nTurnover is sky-high, job stability is low, and yet guest expectations keep rising. We realized something: no one is building tools for the workers themselves.\nPOS systems serve owners. AI scheduling tools serve management.\nBut nothing exists to make workers faster, smarter, more confident, and more financially secure. So we built SWANS ‚Äî the Service Work Assistant & Navigation System.\nNot to replace workers.\nBut to make workers superhuman.\n\nHow it was built: Built a Unity-based AR HUD optimized for subtle, non-intrusive overlays Used live audio input ‚Üí AI ‚Üí structured JSON schema for orders (items, modifiers, quantity, category, VIP tag, special instructions) Displayed the parsed order in real time on the HUD Added the first version of S.W.A.N. Indicators ‚Äî small floating markers meant to attach to unique faces Created a prototype system for tracking regulars and classifying ‚ÄúSWANS‚Äù (our VIP tier) Attempted automatic table detection to place ‚Äúengagement markers‚Äù in AR space Experimented with face anchoring to position markers above individuals Everything was optimized around one principle:\naugment the human, don‚Äôt interrupt them.\n\nChallenges: Automatically detecting tables in complex lighting and seating layouts Getting floating engagement markers to anchor stably in physical space Face detection for positioning SWAN indicators above heads without drift Creating a clean, minimal UI that enhances service without distracting from it Ensuring structured order output stayed consistent across varied conversation styles Avoiding latency when updating the AR HUD with new order data Designing a worker-focused tool that respects privacy and portability\n\nAccomplishments: Built end-to-end ‚Äúvoice ‚Üí structured order ‚Üí HUD display‚Äù working inside AR Designed a consistent, extensible schema for order capture Implemented the first version of VIP/SWAN tracking Established a visual language for floating face indicators Made AR interfaces that preserve human connection by staying subtle and elegant Proved the viability of a worker-first service copilot Demonstrated a shift from enterprise tools ‚Üí personal augmentation tools\n\nWhat we learned: Workers don‚Äôt need automation ‚Äî they need augmentation Personal AI adoption is faster than enterprise AI adoption The best AR UI is nearly invisible and preserves eye contact Structured outputs + AR overlays = reliable, real-time utility Building a universal ‚Äúservice OS‚Äù requires supporting multi-job workflows The future of frontline labor belongs to tools that travel with the worker, not the employer\n\nWhat's next: Expand the worker profile system to follow workers across venues Improve marker stability with better face anchoring + depth understanding Build out a name-recognition & regular-guest memory system Add micro-gestures for confirmations/corrections Layer in tip-optimization & service-speed analytics Push toward a worker subscription model (Superhuman for service workers) Pilot with multi-job workers across restaurants, lounges, bars, and hotels Move to lightweight AR form factors and eventually to everyday wearables SWANS is just the beginning.\nWe‚Äôre building the world‚Äôs first AI exoskeleton for service workers ‚Äî\na universal copilot that democratizes premium service and helps workers earn more, stress less, and perform at a superhuman level.",
    "hackathon": null,
    "prize": "Winner AI with Camera Access by Meta Runner-Up",
    "techStack": "c#, claudecode, cursor, depthapi, flask, google-drive, https, insightface, json, ngrok, openai, python, responseapi, unity, whisper, wit.ai",
    "github": "https://github.com/Wingspear/Swans",
    "youtube": "https://www.youtube.com/watch?v=8wpTuTV3qrM",
    "demo": null,
    "team": "Jaedon Lee",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/700-sf15-jae-z"
  },
  {
    "title": "1508-SF20 ZOOMR",
    "summary": "1508-SF20 ZOOMR is an innovative project designed to leverage hand tracking technology in virtual reality environments. It aims to enhance user interaction by allowing natural hand gestures to control and manipulate virtual objects, thus creating a more immersive experience.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively implementing hand tracking in a seamless manner, which impressed judges and peers alike, earning it the title of Winner and Best Implementation of Hand Tracking by Meta. Its focus on intuitive user experience and real-time interaction distinguished it from other hackathon entries.\n\nNotable technical implementations include the use of C# for programming, integration with Meta's hand tracking API, and the utilization of Unity for creating immersive environments. The project also incorporated advanced shader techniques to enhance visual fidelity and utilized passthrough capabilities on Meta Quest, allowing users to interact with their physical surroundings while engaged in VR.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring accurate hand tracking in various lighting conditions and user environments, optimizing performance to maintain a smooth experience, and designing intuitive interfaces that don't overwhelm users. Additionally, they may have encountered issues with hardware limitations or compatibility across different devices.\n\nFUTURE POTENTIAL\nZOOMR has significant potential for evolution, including the possibility of expanding to support more complex gestures or integrating with other VR applications. Future developments could explore educational or training applications, incorporating AI for personalized interactions, or even creating a platform for collaborative virtual experiences that leverage hand tracking for teamwork and creativity.\n\nWhat it does: Normally, your real world is stuck on one layer. With ZOOMR, you peel that limitation away. A left-hand gesture ‚Äúzooms‚Äù through multiple invisible dimensions around you, revealing ghosts tucked inside different layers. Your right hand does the cleanup ‚Äî form the gesture, shoot, and the spirits vanish.\n\nInspiration: Phones ‚Äúzoom‚Äù by cheating ‚Äî cropping, scaling, stretching pixels. Now that Meta‚Äôs PCA API finally lets us touch raw camera textures, we wanted to see how far we could push that idea inside mixed reality. We explored everything from micro-civilizations living on leaves to biology-scale deep dives into cells. Eventually, we pivoted into something more playful: a zoom mechanic that reveals supernatural layers hiding in your actual room. Think Luigi‚Äôs Mansion, but your hands are the controller and your environment is the level.\n\nHow it was built: We built everything in Unity using the Meta XR SDK and passthrough features, stitching together hand-tracking, custom interactions, and our own zoom-through-reality pipeline. Raw camera textures come straight from the PCA API; our ghost lenses and detector objects were modeled in Maya. The whole system runs on lightweight cropping/scaling logic so we keep full control over digital zoom without relying on AI tricks.\n\nChallenges: Our original plan involved a fully custom hand-gun gesture for raycasting, but designing, training, and tuning that in hackathon time was ambitious. We kept the silhouette but simplified input to a thumb-and-index pinch ‚Äî fast to implement, still readable, and reliable under hand-tracking jitter.\n\nAccomplishments: We‚Äôre proud to be using the PCA API the way it was meant to be used ‚Äî directly manipulating raw camera textures to create a believable zoom mechanic. No enhancement models, just pure control. We also got MR animations, audio cues, and object spawning working cleanly, including automatic ghost respawning to keep gameplay active.\n\nWhat we learned: We dove deep into the PCA API, MRUK, and how far passthrough can be pushed when you stop thinking of it as a background and start treating it as a layer stack you can manipulate.\n\nWhat's next: On the tech side, the zoom mechanic could expand into accessibility tools ‚Äî magnification, selective enhancement, or guided vision overlays. For the game, eye-tracking could move the lens itself, freeing the player‚Äôs hands entirely. And we want to evolve the ghosts ‚Äî color-coded by depth layer, more expressive behavior, and richer feedback to make each resolution feel like its own world.",
    "hackathon": null,
    "prize": "Winner Best Implementation of Hand Tracking by Meta Runner-Up",
    "techStack": "c#, meta, mr, mruk, passthrough, quest, shader, unity",
    "github": "https://github.com/agrikatheprogrammer/ZOOMR",
    "youtube": "https://www.youtube.com/watch?v=YBz7fGK2gQ4",
    "demo": null,
    "team": "Hemang Mehra, Agrika Gupta",
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/placeholder-7rw1qv"
  },
  {
    "title": "Staxel",
    "summary": "Staxel Hackathon Project Summary\n\nStaxel is a project that leverages innovative technology to enhance user experiences in a digital environment, likely focusing on interactive or immersive elements. Although specific details are not provided, the project aims to build upon existing frameworks to create a unique offering that resonates with users.\n\nKEY ACHIEVEMENTS: Staxel stood out by winning the \"Best Upgrade to an Existing Project\" prize, showcasing its ability to significantly improve upon pre-existing technologies or concepts. This recognition indicates a successful integration of user feedback and innovative enhancements that set it apart from competitors.\n\nThe project utilized iwsdk, OpenAI, Python, and WebXR, indicating a focus on web-based immersive experiences, AI integration for enhanced interactivity, and a robust programming foundation. The combination of these technologies likely allowed for rich, engaging user interfaces and experiences.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating diverse technologies, ensuring seamless user experiences across platforms, and managing performance issues associated with immersive web applications. Additionally, refining the user interface and addressing user feedback for improvement could have posed hurdles.\n\nFUTURE POTENTIAL: Staxel has the potential to evolve into a comprehensive platform by expanding its features, such as integrating more AI-driven functionalities, enhancing user customization options, or incorporating community-driven content. There is also an opportunity to explore partnerships with other developers or platforms to further enhance its reach and capabilities.\n\nChallenges: Without prior XR development experience, the main challenge was selecting the right development stack. After consulting with hackathon mentors and other participants, I decided to go the WebXR way and use the IWSDK framework.\n\nWhat we learned: I learned how to develop a WebXR application for Meta Quest using Immersive Web SDK, became a pro in debugging XR applications, and tried a new vibe-coding IDE (Antigravity).",
    "hackathon": null,
    "prize": "Winner Best Upgrade to an Existing Project by Meta Runner-Up",
    "techStack": "iwsdk, openai, python, webxr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=sPFpocr_aMk",
    "demo": "https://sensaihack.staxel.ai/",
    "team": null,
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/staxel"
  },
  {
    "title": "PathfinderXR",
    "summary": "PathfinderXR Project Summary\n\nPathfinderXR is an innovative mixed and virtual reality application designed to enhance user experiences through immersive environments. By integrating various technologies, it aims to provide users with interactive and engaging pathways, enabling them to explore virtual spaces in novel ways.\n\nKEY ACHIEVEMENTS:  \nPathfinderXR distinguished itself by winning the Mixed & Virtual Reality Category at the hackathon, showcasing its unique approach to user interaction in XR environments. Its combination of creativity and technical execution garnered recognition from judges, leading to its success as a runner-up and winner.\n\nThe project utilizes FastAPI for backend services, ensuring high performance and scalability. It integrates Moondream for immersive experiences, and Unity for real-time 3D development, which enhances the interactive aspects of the application. Additionally, it leverages Supabase for database management, providing a robust infrastructure for user data handling.\n\nPOTENTIAL CHALLENGES:  \nThe team likely faced challenges related to the integration of diverse technologies, ensuring seamless user experiences across different devices. Additionally, optimizing performance to accommodate the demanding nature of mixed and virtual reality environments while maintaining high-quality graphics and responsive interactions would have posed significant hurdles.\n\nFUTURE POTENTIAL:  \nPathfinderXR has substantial potential for evolution by expanding its user base and applications. Future developments could include incorporating AI-driven features for personalized experiences, enhancing collaboration tools for multiplayer scenarios, and exploring educational applications that leverage its immersive capabilities for training and learning purposes.\n\nWhat it does: PathfinderXR is a mixed-reality guidance tool for social workers supporting re-entry. It helps practitioners and clients stabilize first, then orient to what matters most next. Using breath-led regulation, perspective-shifting awe, and reflective triage, PathfinderXR helps social workers: See client orientation in real time. Reduce overwhelm before decisions are made. Guide clients toward calmer, clearer next steps. Rather than automating care, PathfinderXR augments the human guide‚Äîmaking orientation visible so judgment and empathy can do their work.\n\nInspiration: PathfinderXR was born from years of frontline work where Mike and Elgin kept seeing the same painful pattern: people exiting homelessness, addiction, or incarceration weren‚Äôt failing because they lacked motivation‚Äîthey were failing because they lacked orientation. No clear map.\nNo way to see the next right step while overwhelmed and stressed. Both of us know what it feels like to rebuild from chaos without guidance. PathfinderXR is the tool we wish we‚Äôd had‚Äîone designed to turn overwhelm into clarity and help people move back toward stability, dignity, and purpose.\n\nHow it was built: For this hackathon, we focused on designing the core interaction model and system logic: Prompt-engineered reflective questioning models. Breathwork and awe-based regulation flows. Spatial triage logic mapped to real-world re-entry needs. Mixed-reality interaction concepts optimized for social workers in active casework. This prototype demonstrates how MR can support guidance without replacing the practitioner.\n\nChallenges: Translating complex frontline realities into a simple, testable experience. Designing AI support that augments human judgment rather than overriding it. Working under tight time constraints while validating a new care paradigm\n\nAccomplishments: Grounding the project in real frontline experience. Clearly articulating a new use case for AI with camera access in social services. Designing a human-first MR guidance model that prioritizes regulation before action\n\nWhat we learned: Re-entry breaks down when decisions are made from dysregulation. Mixed reality is uniquely suited to support orientation and presence. Asking better questions often matters more than providing answers\n\nWhat's next: Expanding breathwork and physiological regulation tools. Adding resource layers for housing, healthcare, detox, food, legal, and spiritual support. Partnering with outreach teams, shelters, and re-entry programs. Developing a ‚ÄúPath to Purpose‚Äù mode for life beyond crisis. Preparing for pilot deployments in Oakland, San Francisco, and similar cities",
    "hackathon": null,
    "prize": "Winner Mixed & Virtual Reality Category by Meta Runner-Up",
    "techStack": "fastapi, moondream, render.com, supabase, tbd, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=hsMKDkCcyEw",
    "demo": null,
    "team": "Elgin Rose, AarnavSan Sangekar",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/pathfinderxr"
  },
  {
    "title": "1201-SF17 ThatsMyJam",
    "summary": "\"ThatsMyJam\" is an innovative project that leverages AI technology and camera access to create a unique music discovery experience. By utilizing advanced algorithms, it identifies songs based on user interactions and context, allowing users to seamlessly connect with and explore music that resonates with their personal tastes.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively integrating multiple AI components and camera functionalities to deliver a user-friendly music discovery platform. Its recognition as the winner of the AI with Camera Access by Meta prize underscores its originality and technical execution, showcasing the potential for engaging user experiences in the music domain.\n\nNotable technical implementations include the use of GPT-5.1 chat for natural language processing, enabling intuitive interactions, and Unity for creating immersive visual experiences. The integration of Meta SDK and MetaPCA enhances the platform's AI capabilities, while Node.js and Express.js provide a robust backend for handling real-time data processing and user requests.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring accurate song recognition in diverse environments, optimizing the performance of AI algorithms in real-time, and managing user privacy concerns with camera access. Additionally, achieving seamless integration of various technologies while maintaining a smooth user experience could have posed significant hurdles.\n\nFUTURE POTENTIAL\n\"ThatsMyJam\" could evolve by expanding its capabilities to include personalized playlists, social sharing features, and collaboration with streaming services. Future enhancements might also involve machine learning to further refine song recommendations and the addition of gamification elements to increase user engagement. With a focus on evolving user needs, this project has the potential to transform how people discover and enjoy music.\n\nWhat it does: Pick up any object ‚Äî a banana, a cup, a plastic bottle ‚Äî and it instantly becomes a musical instrument.\nOur system:\n    1.  Identifies the object using an AI model\n    2.  Generates a sound personality that matches its tone, feel, and vibe\n    3.  Lets you play it using gesture-based controls and a full musical scale\nShake a banana to get slap bass. Tap a cup for warm lofi Rhodes. Pluck a clothespin for pizzicato strings. Every object becomes a new musical experience.\n\nInspiration: We love music, and we love AR. We wanted to combine both using contextual intelligence from a headset ‚Äî letting AI understand the world around you and turn everyday objects into expressive musical tools. Out of that came a simple question:\nWhat if anything you pick up could become an instrument?\nThat idea became That‚Äôs My Jam: a playful, immersive way to explore your environment through sound.\n\nHow it was built: We built the front end as an APC app on Quest using Unity. We built a node backend that provided an endpoint to run an image through an LLM for identification and detailed description. Then we connected the two parts, enabling the user on the Quest to identify an object, which sends the image to the endpoint. Then the app receives details on the object and its music. When the user moves their hand holding the object, the app plays different sounds that fit that object.\n\nChallenges: At midnight, we finally connected the front end and backend‚Ä¶ right before the computer we were using stopped sending POST requests for no explainable reason. After two hours of debugging, switching machines magically fixed everything.\nWe also struggled with reliable hand tracking while holding arbitrary objects, and balancing gesture detection with usability.\nAnd writing these descriptions with a sleep-deprived brain is always a challenge.\n\nAccomplishments: It‚Äôs genuinely fun. We found ourselves wandering IKEA for an hour turning everything into music long after we got our 45-second demo footage.\nWe built something that feels magical, playful, and genuinely shippable with a bit more polish. We‚Äôre proud of how complete the experience feels despite the time pressure..\n\nWhat we learned: ‚Ä¢ Pun-based names can actually turn into real product names\n‚Ä¢ When we lock in, we can build fast\n‚Ä¢ Apparently this building has spontaneous parties at all hours\n‚Ä¢ And yes ‚Äî AI can make anything more fun\n\nWhat's next: We want to polish tracking, improve reliability, add a music sequencing mode, and open this up for creators.\nIn short: let‚Äôs ship it.",
    "hackathon": null,
    "prize": "Winner AI with Camera Access by Meta 1st Place",
    "techStack": "digitalocean, express.js, gpt5.1chat, metapca, metasdk, node.js, openrouter, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=daBf_21oq30",
    "demo": null,
    "team": "Sam Woodard, Zaina Qasim, Daniel Geisler",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/that-s-my-jam"
  },
  {
    "title": "700-SF13 Edgelord Samurai",
    "summary": "The \"700-SF13 Edgelord Samurai\" project combines elements of augmented reality and gaming to create an immersive experience that allows users to engage in samurai-themed challenges. Utilizing advanced computer vision and game development frameworks, the project enables players to interact with the environment and other players in a unique, engaging way.\n\nKEY ACHIEVEMENTS\nThe project distinguished itself by winning the award for the Best Upgrade to an Existing Project by Meta, showcasing its innovative approach to enhancing pre-existing gaming frameworks. Its successful integration of immersive technology and gameplay mechanics likely resonated with judges, highlighting creativity and technical execution.\n\nKey technical implementations include the use of OpenCV for real-time computer vision capabilities, allowing for dynamic interaction with the environment, and Unity for building an engaging user interface and gameplay experience. The integration of Photon enables smooth multiplayer capabilities, facilitating real-time interactions among players.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in synchronizing multiplayer interactions seamlessly, especially in a fast-paced gaming context. Additionally, balancing the complexity of visual effects with performance optimization in real-time applications would have posed significant hurdles.\n\nFUTURE POTENTIAL\nThis project has strong potential for evolution into a more comprehensive gaming platform or an educational tool that teaches users about samurai culture and history through interactive gameplay. Future enhancements could include the expansion of character customization, narrative elements, and integration with more advanced AR technologies for an even more immersive experience.\n\nWhat it does: Edgelord Samurai turns any room into a shared mixed reality dojo on Meta Quest. The player wears the headset and holds a katana style controller. A friend throws a giant orange foam ball through the play space. Our system uses OpenCV color tracking on the passthrough video to lock onto the ball and follow its movement in real time. In the headset, the ball becomes the anchor point for virtual fruit, targets, and impact effects that align with the real world motion. When the player swings the katana through that space, they see satisfying slices, particles, and scoring feedback that feel tied directly to the physical throw. Multiple Meta Quest headsets can join the same session so other players and spectators see the same dojo, the same ball path, and the same hits from different viewpoints\n\nInspiration: Edgelord Samurai began with the original Katana Samurai demo created by Richard Borys, a simple single player bamboo slicing experience in virtual reality with an immersive storyline. It follows a young son, grieving the loss of his father who was killed in the war, as he is recruited for Samurai training by the kingdom to honor his father‚Äôs legacy and prepare for the growing conflict. In the game experience, it felt great to land a clean cut, but it was a lonely with no shared energy or spectators. For the SensAI hackathon, our team asked how we could take that feeling of studying the blade and turn it into a mixed reality dojo party in a living room. We looked to the discipline of Japanese sword training where posture, breath, and precise cuts matter, and combined it with the instant fun\n\nHow it was built: We built Edgelord Samurai in Unity on Meta Quest using passthrough mixed reality and OpenCV. The headset shows the real room, and we layer a stylized dojo environment over it. We use a bright orange foam ball as a physical training orb and feed the passthrough frames into OpenCV. By defining minimum and maximum values for the orange color, OpenCV isolates the ball from the rest of the scene and calculates its position in each frame. Unity receives that position and attaches virtual targets, trails, and hit effects to it so the physical ball and the digital effects feel like one object. The katana controller is treated as a training sword. The game checks when its swing path intersects the tracked ball and triggers slices, sound, and when the timing and placement are correct. On top of this\n\nChallenges: We discovered that tracking a real object in mixed reality is very sensitive to the environment. Different lighting, wall colors, and clothing made it harder for OpenCV to cleanly isolate the orange ball. We spent time tuning the color ranges, smoothing detection, and reducing jitter, so the ball would stay stable on screen. We also had to balance responsiveness and reliability. If tracking reacted too fast, it created noisy movement. If it was too strict, the ball would occasionally pop in and out of view. Finally, we had practical challenges around safety and hardware, such as designing a katana prop that felt good to swing while keeping enough distance between the thrower, the player, and the surrounding furniture.\n\nAccomplishments: We are proud that this is a true upgrade to an existing project. The original Katana Samurai was a single player bamboo slicer. Edgelord Samurai turns that foundation into a social mixed reality dojo with physical props, shared presence, and multiple modes. It moves from a solo toy toward a party game where people take turns and cheer for each other. We are also proud of how much we achieved with relatively simple technology. Careful OpenCV color tracking and good game feel made a foam ball feel like a magical training orb that bridges the real and digital worlds. Seeing multiple people in Meta Quest headsets react to the same throw, shout when someone lands a clean cut, and immediately want another turn is the best proof that the concept works.\n\nWhat we learned: We learned that mixed reality sits at the intersection of computer vision, game design, and physical space. Simple tools like color tracking can create powerful illusions when combined with thoughtful feedback and real objects. At the same time, MR forced us to pay close attention to safety, comfort, and clarity because players are moving their bodies in real rooms, not just using thumb sticks. We also learned that social presence multiplies engagement. The moment we added spectators, and turn taking, the project felt more alive. Finally, we learned how important it is to design within real constraints like noisy environments and limited hackathon time, and to prioritize features that work reliably in front of judges and players.\n\nWhat's next: Next, we plan to fast follow into the Meta Horizon Start Developer Competition next week with an improved build of Edgelord Samurai. Our focus will be on strengthening tracking across more environments, polishing the visual presentation of the dojo, tightening the onboarding so new players can start quickly, and refining the core loop so every throw and every cut feels intentional, readable, and fun in a mixed reality setting.",
    "hackathon": null,
    "prize": "Winner Best Upgrade to an Existing Project by Meta 1st Place",
    "techStack": "opencv, photon, unity",
    "github": "https://github.com/InnerBushido/EdgelordSamurai",
    "youtube": "https://www.youtube.com/watch?v=iuwDH2rIl3c",
    "demo": null,
    "team": "Harris Warren, Olasile Abolade, Caitlyn Croft, Noor Elbanna",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/edgelord-samurai"
  },
  {
    "title": "1609-SF21-Hygge",
    "summary": "The project \"1609-SF21-Hygge\" is an immersive entertainment experience designed to create a cozy and comforting environment for users, leveraging advanced technologies to enhance user interaction and engagement. It combines elements of augmented reality and virtual experiences to foster a sense of well-being and relaxation, embodying the Danish concept of 'hygge'.\n\nKEY ACHIEVEMENTS\nThe project stood out by winning the \"Best Immersive Entertainment Experience\" award by Meta, indicating its exceptional ability to engage users in a unique and comforting way. Its innovative approach to blending technology and emotional experience likely resonated with judges and audiences alike, showcasing a deep understanding of user needs in immersive environments.\n\nKey technical implementations include the use of Unity for immersive environment creation, integration of AI through OpenAI for dynamic user interaction, and real-time multiplayer capabilities via Normcore. The project also utilized Hyper3D for enhanced graphics and effects, with After Effects contributing to high-quality visual storytelling.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in achieving seamless integration between various technologies, particularly in ensuring a smooth user experience across different devices. Additionally, balancing performance and visual fidelity while maintaining an immersive atmosphere can be complex, especially in a live environment with multiple users.\n\nFUTURE POTENTIAL\nThis project has the potential to evolve into a comprehensive platform for wellness and relaxation experiences, possibly incorporating more personalized features driven by AI. Future iterations could expand to include social elements, allowing users to share experiences, or even develop partnerships with wellness brands to create curated content tailored to specific user needs.",
    "hackathon": null,
    "prize": "Winner Best Immersive Entertainment Experience by Meta 1st Place",
    "techStack": "aftereffects, c#, figma, hyper3d, normcore, openai, sora, unity, veo, wit.ai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=4HEdtWKyQk8",
    "demo": "https://drive.google.com/drive/folders/1ppoAQF6n1jznqq-6EBBD_52RsiTrtQLb",
    "team": "Dolores Joya, Remi Gao, Morgan Blair",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/16thfloor-views"
  },
  {
    "title": "GeoPortal",
    "summary": "GeoPortal is an innovative application designed to enhance user engagement through location-based services, integrating aviation data and immersive experiences. Leveraging unique features of Meta Quest, the app provides users with real-time aviation insights and interactive visualizations, making it a valuable tool for aviation enthusiasts and professionals alike.\n\nKEY ACHIEVEMENTS\nGeoPortal stood out by effectively combining cutting-edge technologies and user-centric design, which led to its recognition as the winner of the hackathon and the award for Best Android App Leveraging Features Unique to Meta Quest. Its ability to deliver real-time data through an engaging interface demonstrated a high level of creativity and technical expertise.\n\nThe project utilized a variety of advanced technologies, including:\n- aviationstackapi for real-time aviation data retrieval.\n- Bluetooth integration for seamless connectivity with devices.\n- canvasapi for creating interactive visualizations.\n- ExoPlayer for smooth media playback.\n- Jetpack Compose and Kotlin for building a modern, responsive Android UI.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to data accuracy and latency when integrating real-time aviation data. Additionally, ensuring a seamless user experience while managing complex features unique to Meta Quest may have required significant optimization and testing.\n\nFUTURE POTENTIAL\nGeoPortal has the potential to evolve into a comprehensive platform that offers expanded functionalities such as predictive analytics for flight patterns, integration with social networking features, and enhanced augmented reality experiences. With further development, it could also cater to a wider audience, including travelers and logistics companies, by providing tailored insights and features.\n\nWhat it does: The app transforms in-flight travel into an interactive exploration. At its core, it is a real-time flight monitor that visualizes the journey on a dynamic map. Using a database of over 14,000 cities, it intelligently detects landmarks to trigger \"City Explorations\"‚Äîimmersive 360¬∞ VR video tours of the locations below. To keep passengers engaged, the app features a cyberpunk-inspired interface with educational mini-games like \"Flag Challenge\" and \"Airport Rush.\" These can be played against an AI or nearby passengers via offline Bluetooth multiplayer, creating a complete in-flight entertainment ecosystem.\n\nInspiration: I conceived this idea on a flight to Rome. Frustrated by the lack of entertainment systems and the inability to know our location, I saw an opportunity to solve the boredom of air travel. I wanted to create a solution that turns the \"stuck in a seat\" experience into a journey of discovery.\n\nHow it was built: I built the app using native Android technologies (Kotlin and Jetpack Compose). I engineered a custom data pipeline to cross-reference AviationStack API telemetry with local geospatial data. Visually, I developed a custom graphics engine within Jetpack Compose‚Äôs Canvas API. Instead of heavy 3D libraries, I used custom projection algorithms to render a holographic globe with scientifically accurate day/night cycles. For social features, I implemented a custom Bluetooth networking layer for direct socket connections, while ExoPlayer handles the 360¬∞ video playback.\n\nChallenges: My biggest problem was time. I only saw the competition 4 days before the deadline and wanted to do something even if it is not completed. Although I was still able to add every feature I wanted with a huge success. Also optimizing the 3D rendering engine was quite the process. Implementing custom projection algorithms initially caused stuttering and battery drain; I had to rigorously profile the app and refactor drawing logic to ensure smooth performance. Additionally, managing offline Bluetooth multiplayer was difficult due to the variability of Android‚Äôs Bluetooth APIs. Establishing a stable handshake required building a custom logging system and a robust socket-management solution to handle interruptions and reconnect users seamlessly.\n\nAccomplishments: I am proud of being able to build such useful application in just 4 day. Furthermore, establishing a stable peer-to-peer Bluetooth handshake in an offline environment was a major win. This robust local communication layer not only enables gaming but lays the groundwork for future synchronized in-flight media.\n\nWhat we learned: I learned that with the right mathematical optimization, Jetpack Compose‚Äôs Canvas API is capable of high-performance complex visualizations without needing OpenGL. I also gained deep experience in offline-first architecture, realizing that building resilience against network failures‚Äîwhether for API calls or Bluetooth packets‚Äîis essential for a polished user experience.\n\nWhat's next: I plan to evolve the app into a comprehensive social platform. With our Bluetooth infrastructure in place, our next step is introducing \"Shared Cinema\" and \"Music Lounge\" features, allowing passengers to sync media offline. I also aim to expand the multiplayer ecosystem, using the holographic globe as a shared game board for deeper cooperative experiences.",
    "hackathon": null,
    "prize": "Winner Best Android App Leveraging Features Unique to Meta Quest",
    "techStack": "aviationstackapi, bluetooth, canvasapi, exoplayer, jetpackcompose, kotlin",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=diRcGpDhMl8",
    "demo": null,
    "team": "Merve Yuvaci",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/geoportal"
  },
  {
    "title": "EcoSage",
    "summary": "EcoSage Project Summary\n\nEcoSage is an innovative platform designed to promote environmental awareness and sustainability. By utilizing geospatial data and interactive mapping tools, it empowers users to visualize and engage with local ecological initiatives, fostering community participation in conservation efforts.\n\nKEY ACHIEVEMENTS: EcoSage stood out by seamlessly integrating real-time data from various open-source mapping APIs, allowing users to access critical environmental information in an engaging format. Its user-friendly interface and community-driven features contributed to its recognition as the Grand Prize winner at the hackathon.\n\nThe project leverages a robust tech stack, including Express.js for server-side development, React for a dynamic user interface, and TailwindCSS for responsive design. The use of react-leaflet enhances the mapping experience, while the PWA capabilities ensure accessibility across devices, providing an offline mode for users.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in data integration from multiple sources, ensuring accuracy and relevance of the geospatial information. Additionally, optimizing the application for performance and responsiveness across various devices may have posed technical hurdles.\n\nFUTURE POTENTIAL: EcoSage could evolve by incorporating machine learning algorithms to provide personalized recommendations for users based on their location and interests. Expanding partnerships with local environmental organizations could enhance content and engagement, while gamification elements could incentivize user participation in sustainability initiatives.\n\nWhat it does: EcoSage is your AI-powered sustainability co-pilot that turns complex environmental data into simple, actionable insights. Here's how we revolutionize eco-conscious living:\n\nInspiration: \"The greatest threat to our planet is the belief that someone else will save it.\" - Robert Swan We watched as well-intentioned consumers struggled to make sustainable choices amidst greenwashing, confusing labels, and overwhelming information. The disconnect between environmental awareness and actionable change became our driving force. The Stark Reality: ‚ôªÔ∏è 91% of plastic isn't recycled despite good intentions\nüì± 68% of consumers want to shop sustainably but don't know how\n‚è±Ô∏è Average person spends just 5 seconds evaluating product sustainability We asked: What if every shopping decision could be an informed climate action? What if technology could bridge the gap between intention and impact? EcoSage was born from this vision - to democratize sustainability intelligence and transform every",
    "hackathon": null,
    "prize": "Winner Grand Prize",
    "techStack": "express.js, gemini-api, node.js, overpass-openstreetmap, pwa, react, react-leaflet, tailwindcss, typescript, vite",
    "github": "https://github.com/FrostByte-49/EcoSage",
    "youtube": "https://www.youtube.com/watch?v=tc8ahEA8SDo",
    "demo": "https://ecosage.netlify.app/",
    "team": "Private user",
    "date": "2025-12-05",
    "projectUrl": "https://devpost.com/software/ecosage-pfdcrg"
  },
  {
    "title": "D-Day Simulator",
    "summary": "D-Day Simulator Summary\n\nThe D-Day Simulator is an immersive project designed to recreate the historic events of D-Day, allowing users to engage with the complexities and strategies involved in this pivotal moment of World War II. The simulator aims to provide an educational and interactive experience, blending historical accuracy with engaging gameplay.\n\nKEY ACHIEVEMENTS: This project stood out by winning awards for both overall excellence and for effectively leveraging Noesis, indicating its innovative use of technology to enhance user experience. Its ability to engage users while educating them about a significant historical event contributed to its recognition as a winner in the hackathon.\n\nBuilt with Horizon, the simulator likely incorporates advanced graphics and simulation algorithms to create realistic environments and scenarios. Notable implementations may include dynamic weather systems, AI-driven enemy tactics, and a user-friendly interface that enhances player immersion.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to accurately depicting historical events while maintaining gameplay balance. Technical hurdles could have included optimizing performance for various devices and ensuring that the user experience remains engaging without sacrificing historical fidelity.\n\nFUTURE POTENTIAL: The D-Day Simulator could evolve into a broader historical simulator, encompassing various battles and scenarios from different eras. Future developments might include multiplayer modes, educational partnerships with schools, and the integration of virtual reality to create an even more immersive learning experience.\n\nWhat it does: The player arrives at the beaches of Normandy via landing craft and must clear 1000 meters of sand guarded by machine guns, mines, barbed wire, an airforce and much more. Players work together to identify high risk areas and avoid them, while moving strategically to avoid the threats that sweep across the beach in systematic patterns.\n\nInspiration: We were inspired by the success of Titanic Simulator on HorizonWorlds, and wanted to create a culturally significant, action-packed survival simulator that rewarded multiplayer co-operation and used unique mobile-native design for a peak experience. Our goal was to take the immersion associated with VR and bring it to mobile through smart camera usage and Noesis overlays.\n\nHow it was built: We designed the game as a multiplayer experience where every player progresses independently using their own skill and strategy, yet cooperation is always optionally available. Players can see where others are on the beach, how far they‚Äôve advanced, and where/how they died. This lets them learn from others mistakes, coordinate movements if they choose to, or simply compete for the furthest run. So the experience can feel fully solo or highly cooperative depending on the player‚Äôs preference. Instead of relying on VFX gizmo, we supplemented and enhanced all effects (rain, explosions, smoke, blood splatter, bullet impacts, etc.) with image-sequence overlays in Noesis Studio, delivering far richer visual effects. We used gen AI NPCs for an onboarding experience that was memorable and unique ea\n\nChallenges: One major issue came from our animated overlay system: many intense effects (explosions, rain, etc.) were built as short 4‚Äì5 second sequences of high-resolution images. The combined texture size was too large for Horizon Worlds‚Äô strict memory limits, causing long loading times or failed loads entirely. We solved it by aggressively compressing the images, reducing color depth where the eye wouldn‚Äôt notice, cutting unnecessary frames, and implementing a system that only keeps the currently needed sequences in memory. Also creating Noesis GUI overlays that perfectly cover the entire screen on both mobile and web was tricky. Some devices left visible borders, breaking immersion for effects like heavy rain or flashing explosions. We fixed this by adding a full-screen viewbox and carefully adjus\n\nAccomplishments: We're very proud of using Noesis almost like a VFX tool in addition to as a UI tool. We really wanted to push the limits of what was possible with it. We're also proud of how effectively our onboarding sequence sets the mood, through the animations, cutscene-like camera movements, and NPC usage.\n\nWhat we learned: Noesis was the big one! We learned how to use it from scratch and how to use it as a VFX tool in addition to a UI tool.\n\nWhat's next: We want to focus on developing an uprade system that allows the player to level up in rank and use their playtime to purchase improvements, such as bolt cutters that allow them to cut through barbed wire, gas masks that help them evade toxic threats, etc.",
    "hackathon": null,
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "horizon",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=w_rFqcZSI_k",
    "demo": "https://horizon.meta.com/world/843018298903112",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/d-day-simulator"
  },
  {
    "title": "Awesome Hand",
    "summary": "Awesome Hand is an innovative project developed using Unity that focuses on enhancing user interactions through advanced hand gesture recognition. The project likely aims to create a more immersive experience by allowing users to control and manipulate virtual environments with their hands intuitively.\n\nKEY ACHIEVEMENTS\nThe project stood out by winning two prestigious awards: the overall winner and the Best Implementation of Hand Interactions. This recognition indicates a high level of creativity, technical execution, and user engagement, showcasing its effectiveness in implementing realistic and responsive hand interactions.\n\nNotable technical implementations may include advanced hand tracking algorithms, real-time gesture recognition, and seamless integration with Unity's physics engine and graphics capabilities. These features would enhance the realism and interactivity, making user experiences more intuitive and engaging.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring accurate hand tracking in diverse environments, handling latency issues, and creating a user-friendly interface that accommodates various user skill levels. Additionally, optimizing performance for different devices could have posed significant hurdles.\n\nFUTURE POTENTIAL\nAwesome Hand has substantial potential for evolution, including the integration of machine learning to improve gesture recognition accuracy and adaptability. Future development could also explore applications in virtual reality (VR), augmented reality (AR), and gaming, as well as potential collaborations with educational or therapeutic technologies to promote learning and rehabilitation through interactive hand gestures.",
    "hackathon": null,
    "prize": "Winner Best Implementation of Hand Interactions",
    "techStack": "unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=LB8Yeluiems",
    "demo": null,
    "team": "Ryota Hayashi",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/awesome-hand"
  },
  {
    "title": "YapFormer",
    "summary": "YapFormer is an innovative project that leverages advanced machine learning techniques to enhance natural language processing capabilities. Although specific details are not provided, the project likely focuses on improving text understanding or generation by utilizing models from the Hugging Face library alongside PyTorch for deep learning implementations.\n\nKEY ACHIEVEMENTS\nYapFormer distinguished itself by integrating cutting-edge AI technologies effectively, demonstrating superior performance in tasks such as language comprehension or generation. Its recognition as the \"1st Place ‚Äî Build From Scratch Champion\" indicates exceptional originality and execution, showcasing the team's ability to create a robust solution from the ground up.\n\nThe project utilizes Hugging Face's pre-trained models and PyTorch's flexible deep learning framework, which likely allowed for rapid prototyping and fine-tuning of models. Notable implementations may include custom model architectures or innovative training techniques, enabling YapFormer to achieve high accuracy or efficiency in processing language tasks.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as optimizing model performance while balancing computational efficiency, managing large datasets for training, and ensuring the model's generalizability across different contexts. Additionally, debugging and integrating components from Hugging Face and PyTorch could have posed technical hurdles.\n\nFUTURE POTENTIAL\nYapFormer has significant potential for evolution, including expanding its capabilities to support multilingual processing, real-time applications, or various industry-specific use cases. Future iterations could incorporate user feedback to refine its functionality and extend its application in areas such as chatbots, content creation, or educational tools.",
    "hackathon": null,
    "prize": "Winner 1st Place ‚Äî Build From Scratch Champion",
    "techStack": "huggingface, pytorch",
    "github": "https://github.com/Aravind-808/yapformer",
    "youtube": "https://www.youtube.com/watch?v=sndu7zd9WTg",
    "demo": null,
    "team": "Shri Sai Aravind",
    "date": "2025-12-25",
    "projectUrl": "https://devpost.com/software/yapformer"
  },
  {
    "title": "OnTheGoVR",
    "summary": "OnTheGoVR is an innovative virtual reality application designed for travel enthusiasts, enabling users to explore various destinations in a multiplayer environment. It allows users to experience travel-related activities virtually, providing an immersive way to plan trips and discover new locations while being offline.\n\nKEY ACHIEVEMENTS\nThis project stood out by winning awards for both \"Best Android App\" and the overall hackathon, showcasing its exceptional user experience and functionality. Its emphasis on offline access and multiplayer features enhances social interaction and accessibility, setting it apart from conventional travel applications.\n\nOnTheGoVR leverages Kotlin for development, which ensures a smooth and efficient coding process on Android platforms. The use of Meta for immersive experiences and Spatial SDK for realistic spatial interactions highlights advanced technical implementations, allowing seamless navigation and interaction within the VR environment.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance for mobile devices, particularly in rendering high-quality VR graphics while maintaining offline functionality. Additionally, ensuring a smooth multiplayer experience in varying network conditions could pose significant technical hurdles.\n\nFUTURE POTENTIAL\nOnTheGoVR has the potential to evolve by integrating additional features such as personalized travel recommendations, user-generated content, and partnerships with travel agencies for real-time booking. Expanding to other platforms like iOS and incorporating augmented reality elements could further enhance user engagement and broaden its audience.",
    "hackathon": null,
    "prize": "Winner Best Android App for Travel Mode",
    "techStack": "kotlin, meta, multiplayer, offline, spatialsdk",
    "github": "https://github.com/InventiveOtters/quest-travel-app",
    "youtube": "https://www.youtube.com/watch?v=snMBNi9eN0o",
    "demo": null,
    "team": "Flavius Holerga, Gabi Tiplea",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/syncedvr"
  },
  {
    "title": "Gunship: Quackdown",
    "summary": "Gunship: Quackdown is an innovative project that likely centers around a unique gameplay or interactive experience, possibly involving a blend of flight simulation and whimsical elements involving ducks. By integrating creative design with engaging mechanics, the project aims to captivate users and provide a distinctive gaming experience.\n\nKEY ACHIEVEMENTS\nThe project stood out by winning multiple top prizes, indicating exceptional creativity and execution. Its unique concept, combined with polished visuals and gameplay, likely resonated with judges and participants, showcasing a fresh take on gaming that is both entertaining and memorable.\n\nGunship: Quackdown was developed using Blender for 3D modeling, Noesis for UI and interaction, and TypeScript for robust application development. This combination allowed for high-quality graphics and smooth performance, demonstrating the team's ability to leverage advanced tools effectively to enhance the user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in balancing game mechanics, ensuring smooth gameplay, and optimizing performance across various devices. Additionally, integrating artistic elements with technical functionality can often lead to complexities in design and implementation.\n\nFUTURE POTENTIAL\nGunship: Quackdown has significant potential for evolution, including expansion into different gameplay modes, additional levels or environments, and possible multiplayer features. There is also an opportunity to develop the project into a broader franchise, including merchandise or animated content, which could further increase its reach and audience engagement.\n\nWhat it does: Players work through 6 missions (with more on the way) from the gunner seat of a gunship, eliminating enemies, vehicles, and buildings. When targets are hard to see, you can switch between White Hot and Black Hot thermal modes to spot them. You start with the 25mm cannon and can unlock the 40mm and 105mm cannons to tear through objectives.\n\nInspiration: Gunship: Quackdown was inspired by my favorite Call of Duty campaign missions that put players in the gunner seat of an AC-130.\n\nHow it was built: Gunship: Quackdown was built using GenAI, Copilot, Blender, and a frankly irresponsible amount of caffeine.\n\nChallenges: The game relies on a lot of interconnected systems to function smoothly. Designing, building, and debugging those systems took serious effort. Scope creep was also a major factor, and several enemy variants and upgrade paths had to be cut to hit the deadline.\n\nAccomplishments: I‚Äôm really proud of the per-player audio settings that let players fine-tune sound effects, music volume, and haptics to their liking, making the experience more immersive. The UI was a huge undertaking, but it feels responsive and provides constant feedback through sound and haptics. The menu uses custom camera angles for each section, visually reinforcing the currently selected option. The thermal camera system feels great to use and is unlike anything else I‚Äôve seen on Horizon Worlds. Overall, I‚Äôm extremely proud of the project as a whole‚Äîespecially since it‚Äôs the first time I‚Äôve built something of this scope as a solo dev.\n\nWhat we learned: Dialing in UI is hard, balancing upgrades is its own challenge, and pushing into territory that hasn‚Äôt been done on the platform before adds another layer of complexity.\n\nWhat's next: Next up: an arcade mode for longer sandbox sessions, more enemy variety, weapon mods, special weapons, and additional maps. With some major performance tweaks, the game will be able to support much more content to keep players coming back.",
    "hackathon": null,
    "prize": "Winner Top Prize",
    "techStack": "blender, noesis, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=g8uivZ4bInM",
    "demo": "https://horizon.meta.com/world/695607793117939",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/gunship-quackdown"
  },
  {
    "title": "Ship It!",
    "summary": "Project Summary: Ship It!\n\n\"Ship It!\" is an innovative project developed in a hackathon setting that leverages immersive technologies to enhance user experience in virtual environments. Built using Horizon Worlds, Noesis Studio, and TypeScript, it aims to facilitate seamless interactions and transactions in a digital marketplace, allowing users to explore, shop, and engage in a shared virtual space.\n\nKEY ACHIEVEMENTS: \n   The project stood out due to its unique integration of immersive technology and user-friendly design, effectively bridging the gap between virtual interaction and real-world commerce. Winning both the overall prize and the top prize highlights its exceptional execution, creativity, and potential impact within its target market.\n\nLeveraging Horizon Worlds for immersive environments and Noesis Studio for advanced user interface design, the project showcases sophisticated graphics and responsive interactions. The use of TypeScript ensures strong typing and better maintainability of the codebase, which enhances the overall stability and performance of the application.\n\nPOTENTIAL CHALLENGES: \n   The team likely faced challenges related to optimizing performance in a virtual environment, ensuring cross-platform compatibility, and managing user engagement in a rapidly evolving digital landscape. Additionally, addressing security concerns around virtual transactions would have been a critical focus.\n\nFUTURE POTENTIAL: \n   \"Ship It!\" has significant potential for evolution, including the introduction of AI-driven personalization features, expansion into new virtual marketplaces, and integration with blockchain for secure transactions. As virtual reality continues to grow, the project could evolve into a leading platform for digital commerce, offering enhanced user experiences and broader accessibility.\n\nWhat it does: Players receive orders, pack items into boxes, stamp them, and ship them out as fast as possible! It is frantic and chaotic in the best way. The game relies heavily on Noesis UI to deliver a polished mobile  experience in Horizon Worlds which includes menus, results screens, progression, and lots of visual feedback to keep things exciting!\n\nInspiration: Ship It! was spontaneously born with just a little over a week left in the MHCP competition! The idea came from pondering the ongoing chaos of real-world package loading during the busy holiday season and thinking, \"Hey, there's an idea!\" I wanted to create something energetic, silly, and satisfying to play!\n\nHow it was built: The entire project was developed to highlight the awesome power of the recently supported Noesis Studio. Nearly everything is UI-driven which made the experience incredibly fun and challenging to design. Gameplay systems were written in TypeScript and shaped through rapid iteration. Many features changed several times as the game almost showed me what it was meant to be.\n\nChallenges: Learning Noesis while actively building a full mobile-only Horizon world for the first time was definitely a challenge! Rapid iteration meant refactoring often and discovering what works well and what maybe doesn't work so well. I learned a lot about UI structuring, 2D Animation, performance, and even had a few funny ‚Äúnever do this again‚Äù moments with Noesis! Additionally, considering the challenges of designing for portrait orientation proved difficult, but it was a worthwhile challenge.\n\nAccomplishments: Late night Discord build sessions with MHCP creators were invaluable! We bounced ideas, solved problems, redesigned systems, and kept each other motivated. The game evolved through community collaboration and that feels really special! Beyond that experience, just being able to get something to this level out in this timeframe is huge for me! This is my first world released in over a year!\n\nWhat we learned: I learned how powerful Noesis UI can be and also where the sharp corners are. I improved my TypeScript workflow and discovered new ways to bring mobile-style gameplay into Horizon. Most importantly, I experienced how fast a project can grow through community support and shared creativity!\n\nWhat's next: More levels, item expansions, a smoother progression path, and ongoing iteration based on player feedback! The plan is to keep building with the community and make Ship It! better and more fun with every update!",
    "hackathon": null,
    "prize": "Winner Top Prize",
    "techStack": "horizonworlds, noesisstudio, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=aKAIWndG680",
    "demo": "https://horizon.meta.com/world/4254659188152052/?target=release&hwsh=e6Cwi2B0ot",
    "team": "David James",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/ship-it-0n3erp"
  },
  {
    "title": "IMMERROCK",
    "summary": "IMMERROCK Project Summary\n\nIMMERROCK is an immersive experience project crafted using Unreal Engine and C++. It aims to provide users with a richly interactive environment that blends lifestyle experiences with gaming elements, allowing for unique engagement and exploration.\n\nKEY ACHIEVEMENTS: IMMERROCK stood out by winning the overall hackathon competition and receiving an Honorable Mention for Best Lifestyle Experience. Its success can be attributed to its innovative approach to merging lifestyle elements with interactive gaming, creating a compelling user experience that resonated with judges.\n\nThe project utilizes advanced features of Unreal Engine, showcasing high-fidelity graphics and real-time interaction. Key implementations likely include sophisticated rendering techniques, physics simulations, and user interface design that enhance the immersive quality of the experience.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to optimizing performance within Unreal Engine, ensuring seamless user interactions, and integrating complex lifestyle elements into a cohesive gameplay experience. Additionally, balancing aesthetic quality with technical efficiency could have posed difficulties.\n\nFUTURE POTENTIAL: IMMERROCK has significant potential to evolve into a full-fledged platform that combines lifestyle experiences with social interaction, possibly incorporating augmented reality features. Future iterations could include customizable environments, community-driven content, and partnerships with lifestyle brands to enhance engagement and monetization opportunities.\n\nWhat it does: This update completely transforms how learning works inside IMMERROCK. Courses now react to your playing through real-time Note Detection, exercises evolve as you improve, and new modes give you more ways to learn at your own pace.\nWe also added new celebrations, clearer goals, and more rewarding feedback, so every practice session feels like progress you can see and feel.\n\nInspiration: From the beginning, our goal with IMMERROCK has been simple: help people learn guitar and bass in a way that feels easy, fun, and encouraging. After launching, we listened closely to our community. They wanted progression that felt clearer, feedback that felt more personal, and a practice flow that truly responded to how they played. That feedback became the heart of this update.\n\nHow it was built: We rebuilt big parts of the learning system from the ground up. Some of the biggest changes include: Adding Note Detection to supported course exercises\nCreating 3 exercise types: Wait Mode, Song Tempo, and Song Mastery\nRedesigning the star system so rewards actually reflect your playing\nBuilding new celebration moments: Cheering, Particles, Level-ups, and Result Sequences\nAdding Auto Speed Up and Auto Restart to help players grow more naturally\nImproving UI/UX so goals and progress are easier to understand This update is the biggest learning-system improvement we‚Äôve ever shipped.\n\nChallenges: Improving a live app always comes with challenges. Integrating Note Detection into older course logic meant touching systems that had been stable for a long time. Balancing difficulty, refining reward pacing, and adding new automation without overwhelming players took a lot of iteration.\nBut every challenge pushed the experience closer to what we and our players wanted.\n\nAccomplishments: This update makes IMMERROCK feel more fun and supportive than ever: Exercises that evolve with your skills in real time\nStars and goals feel meaningful\nCelebrations make progress satisfying (because learning an instrument should feel exciting)\nAutomation tools help you stay in flow\nThe whole experience is smoother, clearer, and more polished For many players, this feels like a brand-new version of IMMERROCK.\n\nWhat we learned: We learned that people practice more, and feel better when the app listens, responds, and cheers them on. Clear goals, adaptive difficulty, and positive feedback make a huge difference, especially for beginners.\n\nWhat's next: This update is only the start. Next, we‚Äôre pushing Mixed Reality interactions even further. Guided by constant community feedback, our goal is simple: every update should make players feel more confident, more capable, and more excited to pick up their instrument.",
    "hackathon": null,
    "prize": "Winner Best Lifestyle Experience Honorable Mention",
    "techStack": "cpp, unreal-engine",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=98HlGIi8KMs",
    "demo": "https://www.meta.com/en-gb/experiences/immerrock/7334845636643834/",
    "team": "Yanira Moguel, Evanna Roman",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/immerrock"
  },
  {
    "title": "Lumos",
    "summary": "Lumos is an innovative project that utilizes advanced technologies to enhance online safety and user experience. By integrating a variety of APIs and machine learning models, it likely focuses on detecting and mitigating online threats, such as phishing or malware, ensuring that users can navigate the web securely.\n\nKEY ACHIEVEMENTS\nLumos stood out in the hackathon for its exceptional technical implementation, earning both the overall winner and the Best Technical Implementation Award. Its success can be attributed to the seamless integration of diverse technologies, which not only improved functionality but also showcased the team's ability to address complex problems effectively.\n\nKey technical features of Lumos include the use of the Google Safe Browsing API for real-time threat detection, Tesseract OCR for text extraction from images, and machine learning with XGBoost for predictive analysis. The project also leverages Twilio's lookup API for enhanced user verification and communication capabilities.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the integration of multiple APIs and ensuring data accuracy and security in real-time. Managing the performance of machine learning models and addressing potential privacy concerns while handling user data could also have posed significant hurdles.\n\nFUTURE POTENTIAL\nLumos has the potential to evolve into a comprehensive cybersecurity tool that could be expanded to include features like user behavior analytics and customizable security settings. Future iterations could also explore partnerships with cybersecurity firms for enhanced threat intelligence and develop user-friendly interfaces for broader accessibility.\n\nWhat it does: Lumos is an AI Co-pilot for Digital Trust. It is a centralized platform that provides instant, AI-powered analysis for suspicious messages and images, powered by machine learning models such as XGBoost.\nMulti-Modal Analysis: Users can paste text or upload a screenshot of a suspicious message.\nInstant Verdict: The system analyzes the input and delivers a clear Risk Score (0-100) in seconds.\nActionable Intelligence: Unlike simple blockers, Lumos explains why a message is risky through \"Analysis Evidence\" (e.g., unknown sender, urgency keywords, flagged URLs) and provides concrete \"Recommendations\" (e.g., Block number, Do not click links).\n\nInspiration: We live in a state of constant digital alert. Every unexpected text or email forces a difficult choice: Is this a critical notification or a sophisticated trap? The fear of being scammed is matched only by the fear of missing something important, like a delivery update or an appointment reminder.\nWe realized that without an integrated, instant verification tool, users are left guessing. This uncertainty leads to two costly outcomes: missed opportunities (ignoring real messages) or financial/data loss (falling for malicious links). We wanted to eliminate this guesswork by building a tool that provides clarity in a world of deception.\n\nHow it was built: We engineered a Multi-Layered Intelligence Engine that deconstructs threats into parts for analysis.\nThe Stack: We built a scalable backend using Node.js and Express.js, serving a responsive Web UI.\nData Processing (OCR): To handle image-based scams that evade text filters, we integrated Tesseract.js. This allows Lumos to extract and analyze text, URLs, and phone numbers directly from screenshots.\nExternal Intelligence: We chained multiple powerful APIs to gather context:\n\nTwilio API: For sender reputation and phone number validation.\nGoogle Safe Browsing API: For checking URL reputation and domain age.\nOpenAI API: For semantic analysis to detect urgency, threats, and linguistic patterns.\n\nThe Brain (Machine Learning): All these signals feed into our custom-trained XGBoost Model. This mod\n\nChallenges: OCR Accuracy on Varied Backgrounds: Extracting text reliably from screenshots with different lighting, resolutions, and compression artifacts using Tesseract.js required significant tuning to ensure we didn't miss critical URLs or phone numbers.\nFeature Engineering for XGBoost: Determining which features were most predictive of a scam was difficult. We had to balance technical signals (such as URL age) with linguistic signals (such as \"urgency\") to ensure legitimate high-priority messages (such as 2FA codes) weren't flagged as false positives.\nReal-Time Latency: Chaining multiple APIs (Twilio, Google, OpenAI) creates a risk of slow responses. We had to optimize our asynchronous requests to ensure the user gets a \"verdict in seconds\" rather than waiting for a long analysis.\n\nAccomplishments: The \"Glass Box\" Approach: We didn't just build a black-box AI that says \"Bad.\" We are proud of our UI that breaks down the Analysis Evidence. Showing the user exactly why a message was flagged (e.g., \"URL not flagged, but AI analysis detected urgency tactics\") builds genuine user trust.\n45-Feature Threat Analysis: Successfully implementing a model that considers 45 different variables gives our detection engine a depth that simple keyword matching cannot achieve.\nSeamless Image Handling: Getting the \"Parser & OCR\" layer to work smoothly allows us to catch scams that hide in images, which is a massive gap in many current security tools.\n\nWhat we learned: Scams are Structural: We learned that while the content of scams changes, the structure (urgency + obscure link + unknown sender) remains remarkably consistent. This validated our choice to use XGBoost to detect these patterns.\nContext is King: A URL might be safe, but the context (e.g., asking for a toll payment via text) is what makes it a scam. Relying on a single data point (like Safe Browsing) isn't enough; you need the multi-layered approach we built.\n\nWhat's next: Lumos is just our first step toward a safer digital ecosystem. Our roadmap includes:\n\nBrowser Extensions & Email Plug-ins: Moving Lumos from a destination site to a tool that lives where the users are, automatically scanning content as it arrives.\nReal-Time APIs: Opening our detection engine to other developers to bring trust to their platforms.\nEnhanced Community Feedback: Allowing users to report new scam templates to retrain our XGBoost model in real-time.",
    "hackathon": null,
    "prize": "Winner Best Technical Implementation Award",
    "techStack": "axios, express.js, github, google-safe-browsing, javascript, node.js, ocr, openai, tesseract, twilio-lookup-api, xgboost",
    "github": "https://github.com/AndersonTsaiTW/HackTheSource_lumos",
    "youtube": "https://www.youtube.com/watch?v=HYaMgOeQ3ko",
    "demo": null,
    "team": "Ming Hung Fan, Anderson Yu-Hong Cai",
    "date": "2025-12-01",
    "projectUrl": "https://devpost.com/software/corematrix"
  },
  {
    "title": "K1",
    "summary": "K1 is an innovative project that leverages cutting-edge technologies to create an engaging user experience, likely in the realms of web applications or interactive visualizations. While specific details are not provided, it appears to focus on delivering a seamless and immersive experience, possibly utilizing 3D graphics and real-time data interactions.\n\nKEY ACHIEVEMENTS\nK1 stands out for its exceptional performance, having won multiple awards at the hackathon, including the Best Innovation Award, Best User Experience (UX) Award, and Best Overall Hack Award. These accolades suggest that the project not only introduced a novel concept but also excelled in usability and overall execution, resonating well with judges and users alike.\n\nThe project was built using a robust tech stack that includes Azure for cloud services, Next.js 14 for server-side rendering and static site generation, and Three.js for creating 3D graphics. The use of TypeScript enhances the code's reliability and maintainability. These technologies combined likely contributed to a high-performance application that supports dynamic content and interactive features.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in integrating various technologies, especially ensuring smooth interactions between 3D graphics and web functionality. Performance optimization and cross-browser compatibility could have posed additional hurdles, particularly with complex visual elements. Additionally, creating an intuitive user interface that balances functionality with aesthetics in a dynamic environment is often a significant challenge.\n\nFUTURE POTENTIAL\nK1 has the potential to evolve into a more comprehensive platform by expanding its features, such as incorporating real-time collaboration tools, enhancing its 3D capabilities, or integrating AI elements for personalized user experiences. The project could also explore scalability options to serve larger audiences or adapt to various use cases across industries, such as education, gaming, or virtual events.\n\nWhat it does: K1 routes AI workloads across Earth + orbital data centers using live APIs. Input: GPU-hours, dataset size, priority. K1 calls UK National Grid, Electricity Maps (200+ zones), and Azure for real-time carbon and cost data. Output: Optimal routing with 41% less carbon, 53% less water, 47% lower cost. For flexible jobs, recommends time-shifting: \"Wait 4 hours for solar peak ‚Üí Save 156kg more CO‚ÇÇ.\"\n\nInspiration: AI training evaporates 1.8 liters of water per kWh. Grid carbon varies 24x globally: Sweden's hydro at 13 gCO‚ÇÇ/kWh whereas Germany's coal at 320 gCO‚ÇÇ/kWh. Same GPU-hour costs 10x more in the wrong location. For eg,  Starcloud-1 launched, on Nov 3, 2025, first NVIDIA H100 in orbit, confirming energy is 22x cheaper in space. Zero water. K1 answers: Where and when should AI run?\n\nHow it was built: Stack: Next.js 14, TypeScript, react-globe.gl (3D Earth), Tailwind APIs: UK National Grid (free), Electricity Maps (500/day free tier), Azure Retail Prices Algorithm: Weighted scoring (40% carbon, 30% cost, 30% latency) ‚Üí 70% to best region, 30% to rest. Data centers: Quebec (2 gCO‚ÇÇ/kWh), UK (LIVE), California (LIVE), Germany (LIVE), Sweden (LIVE), Orbital LEO-1 (8 gCO‚ÇÇ/kWh), Orbital LEO-2 (10 gCO‚ÇÇ/kWh).\n\nChallenges: Making it real: Most \"green AI\" projects are mockups. \nSolution: Starcloud-1 launched Nov 2025 (operational hardware). \nLive data reliability: Graceful fallbacks to cached data (30 min old) with clear indicators.\n\nAccomplishments: First orbital routing demo using real Starcloud-1 specs (launched 3 weeks ago, in orbit now)\nLive APIs proving 24x variance on screen in real-time\nClean UI with 3D globe showing exactly where jobs run and why Impact at scale: If K1 routed 1% of global AI training ‚Üí 150,000 tons CO‚ÇÇ saved, 6.5 billion liters water conserved, $1.8B cost reduction annually\n\nWhat we learned: Location is everything: Same GPU-hour produces 24x different emissions depending on grid. Most ML frameworks don't even ask \"where should this run?\" Water is the hidden crisis: AI labs obsess over carbon offsets. Meanwhile California data centers drain aquifers in drought zones. Orbital radiative cooling uses zero freshwater, that should be the headline. Space is cheaper: Solar in orbit generates 5x more power (no atmosphere, no night). $0.002/kWh vs $0.045/kWh on Earth grid.\n\nWhat's next: Expand to 10+ regions (ERCOT, CAISO, NYISO), historical carbon charts, export to Terraform/Kubernetes for real deployments. Starcloud-2 telemetry integration when it launches, batch job scheduling, ML carbon forecasting (48-hour ahead predictions).",
    "hackathon": null,
    "prize": "Winner Best Innovation Award; Winner Best User Experience (UX) Award; Winner Best Overall Hack Award",
    "techStack": "azure, next.js-14, three.js, typescript",
    "github": "https://github.com/samartho4/K1",
    "youtube": "https://www.youtube.com/watch?v=EAqc6N6vVQE",
    "demo": "https://k1-phi.vercel.app/",
    "team": null,
    "date": "2025-12-04",
    "projectUrl": "https://devpost.com/software/orbitops"
  },
  {
    "title": "Timeline Chaos",
    "summary": "Timeline Chaos is an interactive game that likely centers around managing events or actions within a chaotic timeline, utilizing a tap-to-move mechanic for user engagement. Players navigate through various challenges that arise in a non-linear narrative, aiming to restore order or achieve specific objectives within the timeline.\n\nKEY ACHIEVEMENTS\nThe project stood out and won awards for its innovative use of the tap-to-move mechanic, which enhances player interaction and engagement. The combination of a compelling narrative with dynamic gameplay likely captivated judges, showcasing creativity and effective gameplay design.\n\nDeveloped using horizontesdesktopeditor and TypeScript, the project likely features a robust architecture that supports complex event management and interactive gameplay. The implementation of the tap-to-move mechanic suggests a focus on intuitive controls, possibly incorporating advanced pathfinding algorithms to enhance user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to balancing gameplay mechanics, ensuring that the timeline chaos remained engaging without becoming overwhelming. Other hurdles may have included optimizing performance for smooth interactions and debugging complex event transitions within the game.\n\nFUTURE POTENTIAL\nTimeline Chaos has the potential to evolve into a more expansive game, introducing additional levels, characters, or storylines. Future development could include multiplayer functionalities, user-generated content, or integration with other platforms, expanding its reach and enhancing community engagement.\n\nInspiration: We're all major sci-fi fans in our team and time travel or the multiverse is a frequent conversation topic among us. We love movies and shows that address this subject like Dr. Who, Dark Matter, They Cloned Tyrone, 12 Monkeys, Edge of Tomorrow etc.  And although it's a pretty hard subject to tackle in game design, we took on the challenge!\n\nChallenges: Time travel and time paradoxes are hard to explain in real life, so you can imagine that creating a tutorial and gameplay that makes sense and can be easily picked up was quite a challenge. There's still room for improvement here but we did our best.\n\nAccomplishments: In 3 weeks time we created a time travel game that makes sense and isn't so convoluted that it breaks your brain.\n\nWhat's next: new maps\nobjectives\nupgrades to the clones",
    "hackathon": null,
    "prize": "Winner Best use of tap-to-move",
    "techStack": "horizondesktopeditor, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=wXLEjbjVnck",
    "demo": "https://horizon.meta.com/world/10231158779333279",
    "team": "David Pripas, Martin Kelly, AndreiSnyte18, Andrei Vaduva, Liviu Focsa",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/timeline-chaos"
  },
  {
    "title": "The Merchant",
    "summary": "Project Summary: The Merchant\n\nThe Merchant is a platform designed to facilitate seamless interactions between merchants and consumers in a virtual environment. By leveraging advanced technologies, it aims to enhance the shopping experience within digital ecosystems, potentially allowing for personalized recommendations and immersive product showcases.\n\nKEY ACHIEVEMENTS: This project stood out by effectively leveraging Noesis, demonstrating creativity in its implementation and user engagement strategies. Winning the \"Best World that Leveraged Noesis\" and overall hackathon winner showcases its innovation and execution quality, indicating a well-thought-out user experience and functionality.\n\nThe project utilized a combination of generative AI (genai) and robust development frameworks such as Horizon and TypeScript. Notable implementations likely include interactive 3D environments and AI-driven functionalities that enhance user engagement and streamline the shopping process.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring a smooth integration of various technologies, maintaining user data privacy, and creating an intuitive user interface. Additionally, they may have encountered difficulties in scaling the platform for diverse merchants and consumers while ensuring consistent performance.\n\nFUTURE POTENTIAL: The Merchant has significant potential for evolution, including expansion into new markets and integration with emerging technologies like augmented reality (AR) and virtual reality (VR). Future developments could focus on enhancing personalization through machine learning algorithms, expanding the ecosystem to include a wider range of merchants, and creating partnerships to enhance the platform's reach and utility.\n\nWhat it does: The Merchant is a mobile-first and portrait-oriented cozy fantasy shopkeeping and crafting game built primarily around Noesis where players are guided by Julia, the games AI Mentor, as they craft items using simple taps, fulfill customer requests, hire workers, customize their space, and steadily grow their shop over time. With World Broadcast, aspiring shopkeepers can peek into the big leagues and see how seasoned owners run their shops\n\nInspiration: With The Merchant, I wanted to revisit the foundation of an older game of mine, Hantverk Village, and take it in a new direction. It keeps a simple, satisfying crafting loop at its center and builds it into a full shopkeeping world where you run your own shop, fulfill customer requests, customize your space, hire workers with personality, and grow through progression that feels genuinely earned\n\nHow it was built: I built The Merchant by starting small and expanding the world one system at a time. The crafting loop came first, anchoring everything, then customer requests, then workers as the shop took shape. Progression grew alongside each new feature. Once the core was solid, the day-night cycle brought the world together. Each major feature was built as its own clean, independent component, which let the shop slowly wake up without breaking anything\n\nChallenges: Learning Noesis and working through the depth inside each core system, from customers to crafting to workers, took a lot of iteration to make everything feel responsive, cohesive, and fully integrated\n\nAccomplishments: I'm proud of how much the project grew. The Merchant became the largest game I have ever built, with over sixty handcrafted scripts working together to create a world that's fun and looks good. It pushed me into deeper system design, more thoughtful structuring, and a level of polish I had not reached before. Seeing all of my work come into the playable experience that it is feels like a genuine milestone, both in scope and in what I learned while building it\n\nWhat we learned: Building The Merchant showed me how essential structure and iteration are on a large project. Modular systems made complex behavior manageable, and constant tuning shaped the feel of the world. Designing for long-term progression pushed me to think more carefully about pacing. The whole project became a crash course in building for scale and stability without getting overwhelmed\n\nWhat's next: I am committed to bringing the following to The Merchant as time permits: IWP (In-world purchases)\n\nIncluding clothing with traits\n\nMore tiers for each category of item recipes\nMore furnishings and decorations\nShop expansions with larger layouts and rooms\nExpand on customer interaction & behaviors\nExpand on worker personality and depth\n\nIncluding specializations (traits of sorts) and randomized voice lines \n\nExpand on rewards to include more than just time played\n\nIncluding achievements/milestones for taps, items crafted, requests completed, etc.\n\nEnable multiplayer support (once it can be properly tested) with multiple areas/players\n\nEvery system was built to support multiplayer, but I have not been able to properly test it before the deadline\nAlternatively, keep the game 1P to be abl",
    "hackathon": null,
    "prize": "Winner Best World that Leveraged Noesis",
    "techStack": "genai, horizon, horizonworlds, meta, ts, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5xCAhIs8gNU",
    "demo": "https://horizon.meta.com/world/799885093215507/",
    "team": "I did everything",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/shop-simulator"
  },
  {
    "title": "Compile Crops",
    "summary": "Compile Crops Summary\n\nCompile Crops is an innovative project that leverages immersive technology to enhance agricultural experiences. By utilizing spatial computing, it aims to provide users with an engaging platform for crop management, education, and community interaction.\n\nKEY ACHIEVEMENTS: The project stood out by winning the Best Immersive Experience Built with Spatial SDK, showcasing its excellence in integrating immersive technologies with practical applications in agriculture. Its unique approach to combining educational and management tools within a virtual space captured the attention of judges, leading to its recognition as a winner.\n\nCompile Crops was built using a combination of Android Studio for app development, Blender for 3D modeling, and Figma for UI/UX design, alongside Meta's Spatial SDK for creating interactive experiences. This blend of tools allowed for a rich immersive environment that enhances user interaction and learning.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to technical integration between different platforms and tools, optimizing performance for mobile devices, and ensuring a seamless user experience in a potentially complex spatial environment. Additionally, gathering user feedback and iterating on the design may have posed logistical hurdles.\n\nFUTURE POTENTIAL: Compile Crops has the potential to evolve into a comprehensive agricultural platform that includes features such as real-time crop monitoring, community-driven data sharing, and gamification elements to engage users further. Expansion into augmented reality (AR) applications and partnerships with agricultural organizations could enhance its reach and impact.\n\nInspiration: New learners often struggle with how abstract coding feels. Tutorials and browser-based edutainment apps are helpful, but they rely on flat screens and offer little sense of physical cause and effect. We wanted to explore how meta-learning and gamification could feel alive inside a spatial environment.\n\nHow it was built: Compile Crops was created by Gulzar, a full-stack developer with experience in mobile apps, and Grace, a product designer and XR prototyper. Gulzar built the core farming systems, drone logic, code-block interpreter, and UI interactions using kotlin and the Spatial SDK‚Äôs Entity Component System. Special efforts went into building a custom AST to visualise code execution in realtime. We made use of custom shaders provided by the spatial sdk to enhance visuals Grace designed the experience flow, created all 3D assets, and implemented object placement, farm layout, and the styling within the Spatial Editor. Our workflow combined Kotlin programming knowledge (for logic, ECS, and UI) with Meta‚Äôs Spatial Editor (for placement, surfaces, and MR composition). More on Development (Gulzar) 2D stuff:\n\nChallenges: On the Developer Side (Gulzar): Custom shaders took time to work with since they use GLSL, which I‚Äôm not very familiar with. The dual-grid tile map also took time to implement. Positioning entities relative to each other was tricky at first‚ÄîI had to figure out the TransformParent component. After that, it became much simpler. On the Designer Side (Grace): Working with the Spatial Editor was tough because importing custom assets wasn‚Äôt predictable. Editing, testing, and rebuilding assets through Android Studio was also a big shift from Unity or Blender-first workflows. But, I did appreciate the fast build times compared to Unity!\n\nWhat's next: We‚Äôre just getting started with Compile Crops. Our next steps include: Compete in multiplayer with your friends by building the most optimized drone for your farm.\nAdd better rewards and a more robust progression system.\nAdd improved levels that incorporate interesting algorithms such as nearest-neighbour and greedy approaches. With additional support or funding, we believe we can launch a publicly playable version in summer of 2026, bringing spatial coding education closer to users everywhere.",
    "hackathon": null,
    "prize": "Winner Best Immersive Experience Built with Spatial SDK",
    "techStack": "android-studio, blender, figma, meta-spatial-sdk, spatial-sdk",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ZNeVG0tUZiQ",
    "demo": null,
    "team": null,
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/compile-crops"
  },
  {
    "title": "Safe2Go",
    "summary": "Safe2Go Hackathon Project Summary\n\nSafe2Go is a safety-oriented project designed to enhance personal security during travel. Utilizing GPS technology and connectivity features, it aims to provide real-time location tracking and emergency alerts to ensure users can travel safely and efficiently.\n\nKEY ACHIEVEMENTS: The project stood out by effectively combining hardware and software components to create a seamless user experience. Its innovative use of an ESP32 for connectivity and GPS integration, along with a user-friendly interface developed in React, contributed to its recognition as a hackathon winner.\n\nNotable implementations include the integration of Arduino and ESP32 for real-time data transmission, GPS functionality for accurate location tracking, and a robust backend developed in Node.js to handle data processing and user interactions. The use of C++ for low-level hardware programming ensured efficient performance and responsiveness.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to hardware-software integration, ensuring reliable GPS accuracy in various environments, and managing power consumption for the ESP32 to ensure long-lasting performance. Additionally, creating a secure user authentication system to protect personal data may have posed difficulties.\n\nFUTURE POTENTIAL: Safe2Go could evolve into a comprehensive personal safety application by incorporating features such as geofencing, enhanced emergency response capabilities, and integration with other wearable devices. Collaborations with local law enforcement and community services could also enhance its functionality, making it a vital tool for personal safety in urban environments.\n\nWhat it does: Safe2Go is a compact IoT-based module that fits into any helmet and monitors motion, impact, and rider activity. When a crash is detected, it instantly sends alerts to emergency contacts via WhatsApp or email along with live GPS location. The system works offline for accident detection and uses network connectivity for alerts. A dashboard visualizes real-time sensor data and activity logs.\n\nInspiration: The idea grew from observing how many road accidents go unnoticed or receive delayed help. I wanted to design something small yet impactful‚Äîsomething that could sit inside any helmet and call for assistance when the rider cannot. The thought that a few seconds could save a life became the core motivation behind Safe2Go.\n\nHow it was built: We started by researching accident dynamics and exploring sensors like:\n-Accelerometer and Gyroscope\n-Sound and Vibration sensors\n-GPS and Emergency Communication APIs\nUsing an ESP32 microcontroller, we captured multi-sensor readings and set impact thresholds\nWe created a Node.js backend to process incoming data, integrated Twilio for WhatsApp alerts, SMTP for email notifications, and built a lightweight React dashboard for visualization.\n\nChallenges: Setting correct impact thresholds without false triggers\nAchieving stable communication during high-speed movements\nPower optimization to keep the device small and battery-efficient\nDesigning the module to fit inside any helmet without discomfort\nDebugging real-time data flow between ESP32 ‚Üí Server ‚Üí WhatsApp/Email\n\nAccomplishments: Built a working prototype capable of detecting accidents reliably\nSuccessfully sent automated WhatsApp and email alerts with location\nCreated a live monitoring dashboard with continuous sensor streaming\nEnsured the system remains compact, low-cost, and helmet-agnostic\n\nWhat we learned: We learned about IoT workflows, real-time sensor calibration, backend-to-device communication, and emergency automation. This project strengthened our skills in embedded systems, cloud integration, and data visualization.\n\nWhat's next: We plan to add fall-detection AI, fatigue monitoring, and voice-based SOS triggers. Future upgrades include integrating eye-state detection, navigation instructions, improved power efficiency, and launching Safe2Go as a universal plug-and-use safety device for everyday riders.",
    "hackathon": null,
    "prize": null,
    "techStack": "arduino, c++, esp32, gps, node.js, react",
    "github": "https://github.com/prav10140/Safe2Go",
    "youtube": "https://www.youtube.com/watch?v=aYt9UUsKFaM",
    "demo": "https://navigation-nine-umber.vercel.app/",
    "team": "Praveen Kumar Patel",
    "date": "2025-12-01",
    "projectUrl": "https://devpost.com/software/safe2go"
  },
  {
    "title": "Food Fighter",
    "summary": "Food Fighter Project Summary\n\nFood Fighter is an innovative project designed to enhance social experiences around food, potentially through gamification or interactive elements. It aims to foster community engagement and promote healthy eating habits by leveraging technology to create enjoyable food-related challenges or activities.\n\nKEY ACHIEVEMENTS: The project stood out by winning both the overall competition and the prize for Best Social Experience. Its unique approach to blending social interaction with food-related activities likely resonated with judges and participants, showcasing the project's ability to connect people through shared culinary experiences.\n\nFood Fighter was built using a combination of advanced technologies, including editor, horizon, meta, typescript, and worlds. These tools likely facilitated the development of a seamless user interface and interactive features, allowing users to engage with the platform effectively.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user engagement and scaling the platform. Ensuring a diverse range of food experiences that appeal to various demographics, while also maintaining an engaging user experience, could have posed significant hurdles.\n\nFUTURE POTENTIAL: Food Fighter has the potential to evolve into a comprehensive platform that incorporates various aspects of food culture, such as recipe sharing, community cooking events, and nutrition education. Expanding its functionality to include partnerships with local restaurants or food producers could further enhance its social impact and user base.\n\nWhat it does: Food Fighter is a competitive game where players grow larger by eating food. Eat food that drops onto the field to increase your size\nThe bigger you get, the slower you move ‚Äî a clear risk & reward system\nUse weapons to attack and hinder other players\nWhen you think you're big enough, head to the finish zone in the center to record your score on the leaderboard\nIntuitive HUD and ‚ÄúHow to Play‚Äù UI make it easy for newcomers to jump right in The goal is simple:\n‚ÄúWho can eat the most?‚Äù\n\nInspiration: I believed that a game where you grow bigger the more you eat is a fun concept anyone can enjoy.\nBut rather than making it just a simple growth game, I wanted to add a competitive PvP element ‚Äî\ncombining gluttony and combat to create a humorous yet intense experience.\nMy goal was to deliver an immersive gameplay experience where players move around, eat for themselves, and fight for themselves.\n\nHow it was built: Food Fighter is developed using the Meta Horizon Worlds Editor and TypeScript. Implemented interactions based on META Horizon Script\nCreated custom HUD UI (food count display) and How To Play popup UI\nBuilt a grabbable weapon system (force hold, collision detection, swing validation)\nSaved individual player scores using PersistentStorage\nImplemented accurate hit detection using collision events + swing timing\nLocal UI updates through bindings for player-specific HUD refresh\nProduced custom image assets and integrated them into the UI While it may seem feature-rich, the overall architecture is optimized with one clear goal ‚Äî\n‚ÄúFast and intuitive combat.‚Äù\n\nChallenges: While developing Food Fighter, I encountered and solved the following issues: Understanding the Horizon UI System\n-Challenges with distinguishing between Local Script and Default Script\n-Fixed issues where onClick wouldn‚Äôt trigger (layer, input mode, hidden state etc.)\n-Resolved UI updates affecting all players instead of individual ones\nDuplicate Weapon Collision Detection\n-OnPlayerCollision was being triggered multiple times per frame\n-Introduced a Set-based system to ensure only one hit is counted per swing\nAttach & Grabbable Conflicts\n-Replaced attachToPlayer with a force hold approach\n-Applied per-player grip offsets to correct inconsistent weapon grip positions\nImage Asset Loading\n-Solved Image source undefined issues\n-Fixed custom UI failing to render properly Each issue helped deep\n\nAccomplishments: The four aspects of Food Fighter that I‚Äôm most proud of: A fully custom UI system\n-HUD, How To Play popup, icons, and close buttons\n-Easy for new players to understand the game quickly\nAccurate weapon hit detection\n-Swing timing, collision detection, and duplicate-hit prevention provide satisfying combat feedback\nGrowth mechanic based on eating\n-Simple yet highly addictive core gameplay rule\nFull multiplayer support\n-Player-specific HUD\n-Player-specific storage\n-Competitive elements are cleanly handled\n\nWhat we learned: I learned a lot about how to structure games in Horizon for better maintainability and scalability, especially through developing Food Fighter.\nKey learnings include: How Horizon Worlds‚Äô UI system works\nHandling clicks through Pressable objects and Input Modes\nUnderstanding collision event physics\nManaging player data using PersistentStorage\nUpdating UI uniquely per player (bindings + player-specific updates)\n\nWhat's next: Moving forward, I plan to add the following features: Additional weapon types (hammer, stick, frying pan, etc.)\nUltimate skill system: unleash a powerful attack after eating a certain amount\nNew maps with strategic elements (jump pads, narrow paths, etc.)\nEnhanced hit feedback with sound effects and impact particles\nLobby waiting room and tutorial area\nCosmetic customization (skins, colors, different forks)",
    "hackathon": null,
    "prize": "Winner Best Social Experience",
    "techStack": "editor, horizon, meta, typescript, worlds",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=kDbQK6JTjaY",
    "demo": "https://horizon.meta.com/world/873769362490133/?hwsh=ijnVoN2iJF",
    "team": "finallee, Shin JaeYoung, DaeWan Kwon",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/food-fighter"
  },
  {
    "title": "ModStorm Block Party",
    "summary": "ModStorm Block Party Project Summary\n\nModStorm Block Party is an interactive virtual gathering platform that leverages web technologies to create immersive social experiences. By integrating real-time communication and 3D environments, users can engage in activities and connect with others in a dynamic, shared online space.\n\nKEY ACHIEVEMENTS: The project distinguished itself by seamlessly combining various technologies to enhance user engagement in virtual events. Winning both the overall and Judge‚Äôs Choice awards indicates exceptional execution, innovative design, and a strong user experience, likely coupled with unique features that resonated with judges.\n\nKey technical implementations include the use of WebRTC for real-time audio and video communication, WebSockets for efficient data transfer, and Three.js for rendering 3D graphics. The integration of WebXR allows for immersive experiences across different devices, enhancing the overall interactivity of the platform.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to ensuring scalability and performance during high user traffic, optimizing the network for low latency in real-time interactions, and creating an intuitive user interface that works seamlessly across various devices and browsers.\n\nFUTURE POTENTIAL: ModStorm Block Party could evolve into a comprehensive platform for virtual events, including features for event hosting, customizable environments, and integration with social media. Expanding the user experience through gamification or augmented reality elements could further enhance engagement and attract a broader audience.",
    "hackathon": null,
    "prize": "Winner Judge‚Äôs Choice",
    "techStack": "amazon-web-services, javascript, node.js, three.js, webrtc, websockets, webxr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=qsvlQcJF09A",
    "demo": null,
    "team": "Kory Fluckiger",
    "date": "2025-12-09",
    "projectUrl": "https://devpost.com/software/party-blast"
  },
  {
    "title": "Sprint Focus",
    "summary": "Sprint Focus is a web application designed to enhance productivity and focus during work sprints. By providing tools for time management and task prioritization, it aims to help users maintain concentration and effectively track their progress throughout their work sessions.\n\nKEY ACHIEVEMENTS\nThe project stood out for its intuitive user interface and seamless integration of productivity features, which resonated well with judges during the hackathon. Winning the 3rd Overall prize demonstrates its strong execution and innovative approach in addressing common productivity challenges faced by users.\n\nBuilt using a modern tech stack including CSS3, HTML5, JavaScript, React, TypeScript, and Vite, Sprint Focus leverages React's component-based architecture for dynamic user experiences. The use of TypeScript enhances code robustness, allowing for better maintainability and fewer runtime errors.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring cross-browser compatibility and optimizing performance for various devices. Additionally, integrating real-time collaboration features could have posed difficulties in managing state and handling user interactions smoothly.\n\nFUTURE POTENTIAL\nSprint Focus has the potential to evolve into a comprehensive productivity platform by incorporating features such as analytics for tracking user performance, integration with popular project management tools, and even AI-driven recommendations for task prioritization based on user habits and preferences.\n\nWhat it does: SprintFocus is a comprehensive productivity Chrome extension that combines three essential tools into one seamless experience. At its core is a customizable Pomodoro timer with a beautiful visual progress ring that helps users maintain focus through timed work sessions. The integrated Kanban style task board allows users to organize their work with drag and drop functionality, custom tags, and the ability to link tasks directly to focus sessions. The analytics dashboard provides deep insights into productivity patterns with pie charts showing time distribution across different subjects, a GitHub style activity heatmap displaying daily focus patterns, and streak tracking to maintain motivation. Users can customize everything from timer durations to color themes, making it truly their own pr\n\nInspiration: As students and developers, we constantly struggle with maintaining focus in an age of endless distractions. We noticed that while many Pomodoro timer apps exist, they often lack integration with task management and meaningful analytics. We wanted to create a solution that not only helps users focus but also provides insights into their productivity patterns and helps them organize their work effectively. The goal was to build an all in one productivity tool that lives right in your browser, always accessible when you need it most.\n\nHow it was built: We built SprintFocus using modern web technologies to ensure a smooth and responsive user experience. The frontend is powered by React 19 with TypeScript for type safety and maintainable code. We used Tailwind CSS for styling, creating a beautiful and consistent design system with support for multiple themes and dark mode. For data visualization, we integrated Recharts to create interactive pie charts and custom activity heatmaps. The extension uses Chrome Extension Manifest V3, leveraging the side panel API for a native browser experience and the local storage API for data persistence. We used Vite as our build tool for fast development and optimized production builds. The drag and drop functionality was implemented using native HTML5 APIs, and we created custom React hooks for managing t\n\nChallenges: One of our biggest challenges was managing state synchronization between the React components and Chrome's storage API. We needed to ensure that timer state, tasks, and analytics data remained consistent across browser sessions and component updates. Another significant hurdle was optimizing the animations and re renders. Initially, our pie charts would re render on every timer tick, causing performance issues. We solved this by implementing React.memo and carefully managing component dependencies. Creating a responsive and beautiful UI that works seamlessly in the Chrome side panel's constrained space required multiple iterations. We also faced challenges with the drag and drop implementation, particularly ensuring smooth animations and proper state updates when moving tasks between colum\n\nAccomplishments: We're incredibly proud of creating a polished, production ready extension that genuinely solves a real problem. The visual design exceeded our expectations, with smooth animations, beautiful color themes, and a professional interface that rivals commercial productivity apps. Successfully implementing complex features like the activity heatmap and real time analytics while maintaining excellent performance is something we're particularly proud of. The seamless integration of three major features (timer, tasks, analytics) into one cohesive experience demonstrates strong architectural planning. We're also proud of the attention to detail, from custom scrollbars to thoughtful loading states and error handling. The extension is fully functional with no external dependencies or API keys required\n\nWhat we learned: This project taught us valuable lessons about building browser extensions with modern web technologies. We gained deep experience with Chrome Extension Manifest V3 and its APIs, particularly the side panel and storage APIs. We learned how to optimize React applications for performance, especially when dealing with frequent updates like timer ticks. Managing complex state across multiple components and persistent storage taught us important patterns for state management. We improved our TypeScript skills by creating comprehensive type definitions for our data structures. Working with data visualization libraries like Recharts gave us insights into creating meaningful and beautiful charts. We also learned the importance of user experience design, spending significant time on animations, tran\n\nWhat's next: We have exciting plans to expand SprintFocus into an even more powerful productivity platform. First, we want to add cloud sync functionality so users can access their data across multiple devices while maintaining privacy through end to end encryption. We plan to implement more advanced analytics, including weekly and monthly reports, productivity trends over time, and AI powered insights that suggest optimal work patterns based on user data. Integration with popular task management tools like Todoist, Trello, and Notion would make SprintFocus a central hub for productivity. We're considering adding team features, allowing groups to share focus sessions and track collective productivity. Customizable notification sounds, more theme options, and the ability to create custom color schemes a",
    "hackathon": null,
    "prize": "Winner 3rd Overall",
    "techStack": "css3, html5, javascript, react, typescript, vite",
    "github": "https://github.com/sauruvibes/SprintFocus",
    "youtube": "https://www.youtube.com/watch?v=VX_ybg3MQ-A",
    "demo": "https://chromewebstore.google.com/detail/oljpknajnmpcbmcalcfjpgbjaifolikb?utm_source=item-share-cb",
    "team": null,
    "date": "2025-12-14",
    "projectUrl": "https://devpost.com/software/sprint-focus"
  },
  {
    "title": "Style Swipe",
    "summary": "Style Swipe is an innovative digital platform designed to enhance the shopping experience for the next generation of fashion consumers. By leveraging advanced technologies, it connects users with personalized fashion recommendations and interactive experiences, ultimately facilitating a more engaging and tailored retail journey.\n\nKEY ACHIEVEMENTS\nThe project stood out by winning multiple awards, including the MLH Best Use of Gemini API and recognition from Frasers Group for its compelling digital experience aimed at attracting and retaining younger customers in the fashion retail market. Its success is indicative of its effective integration of technology with user-centric design.\n\nStyle Swipe utilizes the Gemini API for machine learning-driven fashion recommendations, enhancing user engagement through tailored suggestions. The project is built on a robust PostgreSQL database, ensuring efficient data management and scalability, which is crucial for handling user interactions and preferences.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating diverse technologies, ensuring seamless user experiences, and managing data privacy and security, especially given the personalized nature of fashion recommendations. Additionally, maintaining user interest and adapting to fast-changing fashion trends could pose ongoing challenges.\n\nFUTURE POTENTIAL\nStyle Swipe has significant potential for growth by expanding its features, such as incorporating augmented reality for virtual try-ons or social sharing functionalities. Furthermore, partnerships with fashion brands could enhance its offerings, while data-driven insights could continuously refine user experiences, solidifying its position in the competitive fashion retail landscape.",
    "hackathon": null,
    "prize": "Winner [MLH] Best Use of Gemini API; Winner [Frasers Group] Build a digital experience that attracts and retains the next generation of customers to the fashion retail market.",
    "techStack": "frasers, gemini, postgresql",
    "github": "https://github.com/GitFarhanS/HackSheffield25",
    "youtube": "https://www.youtube.com/watch?v=xhO6g5oC-e8",
    "demo": null,
    "team": "Fun to use sometimes",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/style-swipe-7h2zak"
  },
  {
    "title": "SupplyTrace",
    "summary": "SupplyTrace Project Summary\n\nSupplyTrace is a project designed to enhance supply chain transparency and traceability through the use of technology. By leveraging data visualization and AI, it aims to provide real-time insights into the movement and status of products within the supply chain, helping businesses optimize their operations and improve accountability.\n\nKEY ACHIEVEMENTS: The project garnered attention for its innovative approach to solving supply chain issues, securing 3rd place at HackSheffield10 and earning the title of winner. Its emphasis on user-friendly design and practical application of AI in supply chain management set it apart from other entries.\n\nSupplyTrace was built using CSS, HTML, and JavaScript, showcasing a strong front-end development. The integration of OpenAI technologies likely enabled advanced data processing and analytics, allowing for dynamic insights and user interactions, which enhanced the overall user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data integration from various sources within the supply chain, ensuring real-time updates, and maintaining data security and privacy. Additionally, creating a user interface that is both intuitive and informative could have posed design challenges.\n\nFUTURE POTENTIAL: SupplyTrace has the potential to evolve into a comprehensive supply chain management platform, integrating more advanced AI capabilities and machine learning algorithms for predictive analytics. Future iterations could also explore partnerships with logistics companies to enhance data sources and improve real-world applicability, as well as expanding its user base to include various industries beyond its initial focus.\n\nWhat it does: SupplyTrace is a web application that generates interactive supply chain visualisations. Users input a product, and the application deconstructs it into its raw materials, refinement processes, and components, geographically linking each to its most likely industrial hub. The results are presented on an interactive global map, highlighting the intricate network that connects resources, factories, and consumers. An accompanying flow diagram provides an alternative view of the supply chain, further enhancing understanding.\n\nInspiration: The grand adventures of mythology might seem distant, but a far more impressive journey unfolds every day ‚Äì the complex supply chains that bring the products we use to our hands. From raw materials to finished goods, a single smartphone or pair of shoes traverses continents, undergoing incredible transformations. This project explores that journey.\n\nHow it was built: The core of SupplyTrace lies in leveraging the power of OpenAI's GPT models. We developed specific prompts to dynamically generate data related to raw materials, locations, and components. This information is then visualised using the HTML Canvas API. To accurately represent the curvature of the Earth, we implemented custom plotting logic, drawing points and lines to create a geographically accurate representation of the product's journey.\n\nChallenges: Taming the AI Output: Ensuring the raw data from GPT could be reliably formatted for visualisation required careful prompt engineering and data parsing techniques.\nCanvas Interaction on a Curved Surface: Detecting mouse hover events accurately over curved lines on the Canvas presented a unique challenge.\nGeographic Accuracy: Correctly plotting locations on the map required careful consideration of map projections and coordinate transformations.\n\nAccomplishments: The transformation of linear AI output into a linked, interactive graph.\n\nWhat we learned: This project reinforced the importance of precise prompt engineering for generating consistent and reliable AI outputs.\n\nWhat's next: Future iterations of SupplyTrace will integrate a dashboard to provide comprehensive data analysis. This will include metrics such as total travel distance, carbon footprint calculations, and total time taken for completion, enabling users to gain deeper insights into the environmental and economic impact of supply chains.",
    "hackathon": null,
    "prize": "Winner HackSheffield10 3rd Place",
    "techStack": "css, html, javascript, openai",
    "github": "https://github.com/Ro11ast/product_traceability_visualisation",
    "youtube": "https://www.youtube.com/watch?v=L-CRFjq0HzY",
    "demo": null,
    "team": "Private user",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/supply-trace"
  },
  {
    "title": "Clash Royale Deck Analyzer",
    "summary": "Clash Royale Deck Analyzer\n\nThe Clash Royale Deck Analyzer is a tool designed to evaluate and optimize player decks in the popular mobile game Clash Royale. By analyzing card synergies, strengths, and weaknesses, it provides users with insights and recommendations to improve their gameplay strategy.\n\nKEY ACHIEVEMENTS: This project distinguished itself by winning the overall competition and being recognized as the People's Choice winner, showcasing its appeal and effectiveness among participants and judges. Its comprehensive analysis and user-friendly interface likely contributed to its success.\n\nThe project utilizes Python for backend processing, including the PyTorch library for implementing machine learning algorithms that assess deck performance. Additionally, it employs Matplotlib for visualizations, allowing users to easily interpret data and insights, while Tkinter provides a clean graphical user interface for user interaction.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to accurately modeling the game's mechanics and balancing the complexity of analysis with user accessibility. Additionally, gathering sufficient data for training the machine learning models and ensuring real-time performance without lag could have posed difficulties.\n\nFUTURE POTENTIAL: The project could evolve into a more advanced tool by incorporating real-time data from live games, expanding its analytical capabilities to include opponent decks and adapting strategies accordingly. Future developments could also introduce community features, such as sharing deck setups and strategies among players or integrating with Clash Royale's API for automated updates.\n\nWhat it does: Our application allows users to easily input their Clash Royale deck either through a graphical interface, by pasting a deck link, or by manually typing in the cards. The app then provides a detailed analysis of the deck, displaying key stats such as the 4-card cycle, average elixir cost, and deck archetype, all backed by data from the top 100 players in the game. The analysis is presented visually through engaging graphs and pie charts, helping users understand their deck's strengths and weaknesses while offering strategic insights on the best ways to play and maximize their chances of winning.\n\nInspiration: Inspired by the Grafana dashboard, we sought to create a unique version tailored to Clash Royale, a popular mobile game. Noticing a gap in deck analysis tools, we decided to build our own solution that not only analyzes a deck but also provides a wealth of information to help players optimize their strategy and improve their chances of winning games.\n\nHow it was built: we found the 100 most commonly used deck from an API and manually classified them as the various archetypes this was then used as training data to create our classifier that provided a confidence rating for each archetype. then we created the GUI based from the in-game version to create a deck to perform the analysis on.\n\nChallenges: lack of existing training data which we solved by creating it our own. issues with bugs created by gen-AI that had to be solved by manual debugging, we also began to run into a point in the project where the AI could no longer make successful modification to the code and development had to proceed manually.\n\nAccomplishments: We are incredibly proud of our achievement in training a highly accurate model using just 100 decks. We‚Äôre also thrilled with the features we‚Äôve implemented, including a user-friendly graphical interface and text-based input methods, which together provide a seamless and versatile experience for deck analysis and strategy optimization.\n\nWhat we learned: how to integrate AI into our workflow, how to gather and create training data for a neural network, how to train a neural network, how to make a image based GUI with search function. How to make and post a YouTube video.\n\nWhat's next: In the future, we plan to integrate an AI assistant to further assist players in mastering their deck, offering personalized tips and in-depth guides for each archetype. Additionally, we aim to enhance the user interface, making it more modern and visually appealing. We also plan to introduce features such as EVO variants for cards and other small improvements to make the app more comprehensive and user-friendly. Our goal is for the app to be web-based, allowing users to access it seamlessly from any device.",
    "hackathon": null,
    "prize": "Winner [Comp Soc] People's Choice",
    "techStack": "matplotlib, python, tkinter, torch",
    "github": "https://github.com/Shadowblades746/hackSheffield10.git",
    "youtube": "https://www.youtube.com/watch?v=4VCsr4iWfVc",
    "demo": null,
    "team": "Emad Riyaz, Benjamin Whittaker",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/clash-royale-deck-simulator"
  },
  {
    "title": "Photon",
    "summary": "Photon is an innovative entertainment experience that leverages mobile technology to deliver immersive interactions, likely incorporating elements of gaming or augmented reality. The project aims to engage users in a captivating way, enhancing their entertainment options through its unique features.\n\nKEY ACHIEVEMENTS\nPhoton stood out in the hackathon by winning the \"Best Entertainment Experience\" prize, showcasing its ability to effectively engage users and offer a compelling entertainment solution. Its recognition as a winner indicates a strong presentation, user experience, and originality, making it a favorite among judges and participants.\n\nThe project utilizes Android for mobile compatibility, Cloudflare for secure and efficient data management, and Unity for creating rich, interactive environments. This combination allows for high-quality graphics, smooth performance, and a scalable backend, which are critical for delivering an engaging entertainment experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring seamless integration between the various technologies (Android, Cloudflare, Unity), optimizing performance for different devices, and addressing user experience design to make the application intuitive. Additionally, competition in the entertainment space may pose challenges in differentiating Photon from existing solutions.\n\nFUTURE POTENTIAL\nPhoton has the potential to evolve into a larger platform that could incorporate social features, multiplayer capabilities, or additional content, such as user-generated experiences. By expanding its reach, it could tap into various entertainment markets, including gaming, virtual reality, and educational experiences, further enhancing its user base and engagement.\n\nWhat it does: Photon is a spatial video sharing platform, where people create, share and explore 3D videos. We also developed the technology to convert any 2D video to 3D, so anyone can convert their favorite 2D video to 3D and easily share on Photon.\n\nInspiration: In 2024 we developed Spatial TV, where we found real-time lighting could create a truly amazing video watching experience. We believe the next step would be bringing this amazing experience to 3D videos Back in early 2025, there was no platform for sharing spatial video on Meta Quest yet. So we thought it might be a good idea to be the first the build it out and integrate the spatial lighitng effect.\n\nHow it was built: We started off building the core framework for a content platform: a website for uploading videos and the Photon app for discovering and watching 3D videos. Later, we worked on critical improvements that we think matters the most for our users, including the spatial lighting, personalized recommendation and UI refinement.\n\nChallenges: Implementing spatial lighting for 3D video was the most challenging part, as it involves close integration of a hybrid tech stack: getting the video texture from Android and then sending them to Unity through low level graphic APIs. It also involves a lot of craftsmanship to make the lighting effect look truly beautiful.\n\nAccomplishments: We're proud to build the first and largest spatial video sharing community on Meta Quest!\n\nWhat's next: We see three areas where we'll make improvements in the near future More customization in discovery page, like tag filters\nBetter personalized recommendation and search experience\nCreator incentivization: we plan to launch creator incentive program to support our creator community",
    "hackathon": null,
    "prize": "Winner Best Entertainment Experience Runner-up",
    "techStack": "android, cloudflare, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=5WpxaLfLWOo",
    "demo": null,
    "team": "Shared Account, Noy Wang",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/photon-bm9p31"
  },
  {
    "title": "E-Translator",
    "summary": "E-Translator Project Summary\n\nE-Translator is a web-based application designed to facilitate real-time translation between multiple languages. It aims to enhance communication by allowing users to input text and receive accurate translations instantly, making it a valuable tool for travelers, students, and professionals engaging in cross-language interactions.\n\nKEY ACHIEVEMENTS: The project stood out for its innovative app design, which effectively combines simplicity and functionality. Its user-friendly interface and seamless translation experience earned it recognition as a winner in the hackathon, highlighting its potential to address a common pain point in multilingual communication.\n\nE-Translator leverages HTML and JavaScript to create an interactive front-end, enabling dynamic user input and output without requiring page reloads. Its implementation of APIs for language processing likely enhances translation accuracy and speed, showcasing a well-integrated technical solution.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring the accuracy and contextual relevance of translations, as language nuances can vary significantly. Additionally, optimizing the app for performance and handling multiple simultaneous users could have posed technical hurdles.\n\nFUTURE POTENTIAL: E-Translator has the potential to evolve by integrating more advanced machine learning algorithms for improved translation quality. It could also expand its capabilities to include voice translation, offline functionality, and support for a wider variety of languages, ultimately broadening its user base and applications.",
    "hackathon": null,
    "prize": "Winner Most Innovative App Design",
    "techStack": "html, javascript",
    "github": "https://github.com/Pronoob5066/E-Translator/tree/main",
    "youtube": "https://www.youtube.com/watch?v=7zE5Aml9cTc",
    "demo": null,
    "team": "Vishak Saravana Kumar",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/e-translator"
  },
  {
    "title": "SCX_MUS: Mostly Unfair Scheduler",
    "summary": "SCX_MUS, or Mostly Unfair Scheduler, is a scheduling solution designed to optimize resource allocation in Kubernetes environments. By leveraging eBPF (extended Berkeley Packet Filter) technology, it provides a mechanism for fine-tuning the scheduling of tasks, potentially improving system performance and resource utilization in cloud-native applications.\n\nKEY ACHIEVEMENTS\nThis project stood out by successfully combining advanced scheduling algorithms with eBPF's capabilities, resulting in significant improvements in task prioritization and resource management. Winning the eBPF Starter Track indicates its accessibility and effectiveness, particularly for beginners in the eBPF ecosystem, while also demonstrating innovation in a competitive hackathon environment.\n\nNotable technical implementations include the use of eBPF for dynamic tracing and performance monitoring, integration with Kubernetes for seamless deployment, and the application of C and Golang for efficient processing. The project also utilizes client-go for Kubernetes API interactions and libbpf for managing eBPF programs, showcasing a robust multi-language approach.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the complexity of eBPF programming and debugging in a Kubernetes context, ensuring compatibility with existing scheduling frameworks, and managing the performance overhead that could arise from dynamic scheduling adjustments. Additionally, integrating multiple languages and tools may have posed coordination and maintenance difficulties.\n\nFUTURE POTENTIAL\nSCX_MUS could evolve into a more comprehensive scheduling framework that incorporates machine learning for predictive resource allocation and further enhances Kubernetes orchestration capabilities. Additionally, expanding its features to support hybrid cloud environments and integrating user-defined policies could make it a versatile tool for diverse cloud-native infrastructures.\n\nWhat it does: SCX_MUS: Mostly Unfair Scheduler is a custom Linux scheduler designed to prioritize containers within Kubernetes, by dynamically adjusting the CPU resources allocated to containers based on their priority. It is implemented using the sched ext framework, to give priority to certain containers over others, improving the performance of specific workloads, even in resource-heavy or \"noisy neighbor\" environments. The userspace component, written in Go, provides a simple CLI that allows users to select which container to prioritize. The tool lists all currently running containers and prompts the user to choose one for prioritization. Once a container is selected, the scheduler retrieves the associated cgroup ID and saves it to the BPF map.\n\nInspiration: The idea for this project started casually at a bar table. Our group, patos‚Äîall computer science students‚Äîwas gathered when a friend showed us the new Linux scheduling technology, sched_ext. We were fascinated, but it felt unrealistic for a group of students to build a scheduler from scratch. Months later, during a discussion about Kubernetes CPU management‚Äîspecifically why cpu.limits is often discouraged and why cpu.shares is generally preferred‚Äîwe began exploring how computational resources are allocated to containers. This led to a question: Is it possible to dynamically give more compute resources to a container, beyond the usual horizontal scaling or static allocation in manifests? The idea was born: to attempt giving a container higher priority via a custom scheduler.\n\nHow it was built: We spent the hackathon diving deep into the internals of the Linux scheduler ecosystem. We studied the kernel source code, from discovering that Linux actually runs multiple schedulers (not just CFS. FIFO and RR are also present) to examining functions like __schedule() inside kernel/sched/core.c. We combined this understanding with Kubernetes API communication and, most importantly, hands-on eBPF development.\nUsing sched_ext and struct_ops, we implemented a custom scheduling policy in eBPF that applies dynamic, cgroup-based priority to containers.\n\nChallenges: One of the hardest parts was figuring out how to implement and run a scheduler using sched_ext_ops. There is very little documentation or guides online on how to load and execute your own custom scheduler, only examples of people running pre-made schedulers included with SCX. Another challenge was our initial workflow: we wrote most of the code without compiling or testing it (a terrible practice, we know). Only after reaching a reasonable implementation did we start the work of compiling the kernel with sched_ext support, setting up Kubernetes clusters and designing the benchmark. The debugging phase involved days of solving compilation mysteries, and unexpected behaviors before everything finally worked.\n\nAccomplishments: The goal of our custom scheduler, wasn't to critique the Completely Fair Scheduler (CFS), our scheduler is, in fact, a highly simplified version of CFS. Instead, it was a two-fold endeavor: To see if students could successfully implement a scheduler from scratch.\nTo explore different approaches to container scalability by dynamically adjusting a container's priority/resource share via a custom scheduler. We consider the project a success in demonstrating both of these concepts.\n\nWhat we learned: We learned about: The Linux scheduler architecture and its multiple scheduling classes\nKernel internals and low-level scheduling paths\nKubernetes resource management and API communication\neBPF development and the sched_ext subsystem\nPerformance evaluation, benchmarking, and debugging complex systems\n\nWhat's next: Building a more sophisticated control mechanism that uses the Kubernetes API to gather metrics and automatically adjust the container's priority share based on workload (e.g. implementing a hook that prioritize a container when its netns be with a X quantity of packets)\nRefining SCX_MUS by:\n\nAdding multi DSQs for multi-cores enviroments\nDeveloping more sophisticated migration heuristics and improving L2/L3 cache locality",
    "hackathon": null,
    "prize": "Winner eBPF Starter Track (Beginner-Friendly)",
    "techStack": "c, cillium/ebpf, client-go, ebpf, golang, kubernetes, libbpf, magalu-cloud, python, sched-ext",
    "github": "http://github.com/patos-ufscar/Hackathon-eBPF-2025",
    "youtube": "https://www.youtube.com/watch?v=3Z7UYZ7sJTU",
    "demo": null,
    "team": "Rodrigo Coffani, Lucas de Ara√∫jo Cardoso, Oliver M. U., Luiz Ot√°vio Teixeira Mello",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/scx_mus-mostly-unfair-scheduler"
  },
  {
    "title": "erm.ai",
    "summary": "erm.ai Project Summary\n\nErm.ai is a multi-agent artificial intelligence platform designed to facilitate interactions and tasks among various AI agents. The project aims to enhance collaborative problem-solving capabilities through the integration of multiple AI models, allowing them to work together efficiently in real-time.\n\nKEY ACHIEVEMENTS: \n   The project stood out in the Multi-Agent AI Challenge due to its innovative approach to agent collaboration, demonstrating effective communication and task delegation among agents. Its success was recognized with a winning prize, highlighting its potential to address complex tasks in a streamlined manner.\n\nBuilt using JavaScript, Next.js, Python, React, and TypeScript, erm.ai leverages a robust tech stack to ensure a seamless user experience and efficient performance. Notable implementations include real-time collaboration features and dynamic agent interaction protocols, which enhance the system's adaptability and responsiveness.\n\nPOTENTIAL CHALLENGES: \n   The team likely faced challenges related to ensuring smooth interoperability among various AI agents, managing the complexity of agent communication, and optimizing performance for real-time interactions. Additionally, debugging multi-agent systems can be intricate, requiring meticulous attention to detail.\n\nFUTURE POTENTIAL: \n   Erm.ai has significant growth potential, with opportunities to expand its capabilities into various domains such as robotics, automated customer service, and smart home technologies. Future iterations could incorporate advanced machine learning algorithms and user customization options, further enhancing agent collaboration and efficiency.\n\nWhat it does: erm.ai is an AI Slack agent that monitors conversations, adds helpful insights, and automatically generates n8n-powered LinkedIn-ready updates. It can also be expanded with domain-specific agents.\n\nInspiration: We wanted to make teamwork smoother by giving Slack groups an AI teammate - one that listens, supports, and turns group progress into something actionable and shareable. In addition to this, we wanted to be able to provide multi-agent support with fine-tuned models so that we can create a suite supporting a variety of purposes.\n\nHow it was built: We integrated Slack‚Äôs MCP with our custom AI agent, added message filtering and context handling, and connected everything to an n8n workflow for automated LinkedIn content generation, which gets outputted to the Slack account. We also experimented with fine-tuning for specialised agents.\n\nChallenges: Connecting the n8n workflow with the web application due to frequent collisions with the agent running in the background. Managing noisy data, ensuring that it doesn't interfere with our valid data.\n\nAccomplishments: Working well as a team despite not being familiar with people\n\nWhat we learned: How to manage conversational context at scale, orchestrate multi-service integrations, design better agent behaviors, and build AI that supports teams without overwhelming them.\n\nWhat's next: Expanding our library of domain-specialized agents, improving context detection, adding voice and meeting integrations, and creating a polished dashboard for workflow management.",
    "hackathon": null,
    "prize": "Winner [Reply] Multi-Agent AI Challenge",
    "techStack": "javascript, next.js, python, react, typescript",
    "github": "https://github.com/Vrun1506/Hack-Sheff-10",
    "youtube": "https://www.youtube.com/watch?v=owogrzfarmU",
    "demo": null,
    "team": "Ollie Fishwick, Alric Marvel, Chris Williams",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/erm-io"
  },
  {
    "title": "FAKE_IT_TILL_YOU_MAKE_IT-02",
    "summary": "FAKE_IT_TILL_YOU_MAKE_IT-02 is an innovative project designed to address future societal challenges by leveraging advanced technologies. Although specific details are not provided, the project's focus on solving future problems suggests a proactive approach to anticipating and mitigating potential issues through creative solutions.\n\nKEY ACHIEVEMENTS\nThe project stood out by winning multiple awards at the hackathon, including the overall winner and the \"Best Megatrends - Build Something That Solves a Future Problem - Today Challenge.\" These accolades indicate that the project not only showcased originality and relevance but also effectively demonstrated a tangible solution to pressing future challenges.\n\nThe project incorporates a diverse tech stack, utilizing tools such as Gmail API for communication, JavaScript and Python for programming, and PostgreSQL for database management. Additionally, integrations with n8n for workflow automation, Redis for caching, Supabase for backend services, and Twilio for communication services highlight a sophisticated infrastructure that enhances functionality and user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced several challenges, including coordinating various technologies and ensuring seamless integration between them. Additionally, anticipating future problems accurately and developing effective solutions within a limited timeframe could pose significant hurdles. Balancing complexity with usability for end-users may also have been a challenge.\n\nFUTURE POTENTIAL\nFAKE_IT_TILL_YOU_MAKE_IT-02 has the potential to evolve into a scalable platform that continuously adapts to emerging trends and societal needs. By expanding its features and refining its technology, it could serve as a valuable tool for businesses and individuals alike, helping them navigate and prepare for future challenges. Further collaboration with industry experts could enhance its effectiveness and reach.\n\nWhat it does: Our software serves as a layer of active protection between scammer and non-tech-savy people. It will actively take calls, interrupt emails and text messages for the sake of wasting scammers' time. With such software integrated into every phone and email provider we will make scamming impossible. We will waste so much of the scammers time that the scamming itself will stop making money and will seize to exist. We will make sure than scammers's time is wasted and they are reported to the authorities.\n\nInspiration: Scams are getting more and more sophisticated and it's not enough to just protect people. We need to fight the roots of this evil. AI keeps getting more and more realistic, cheaper and easier to use. Therefore if we don't do anything about it, scams will keep getting more and more widespread.\n\nHow it was built: Wired Gmail API OAuth (credentials.json + token.json) to fetch unread messages.\nSent message content to an n8n workflow that classifies senders via GPT.\nPersisted sender status (whitelist/blacklist/none) in Supabase PostgreSQL.\nAutomated replies for blacklisted senders with time-wasting, humorous templates.\nAdded optional Redis for lightweight caching and throughput.\n\nAccomplishments: End-to-end pipeline from Gmail ‚Üí n8n/GPT ‚Üí Supabase ‚Üí auto-replies.\nRobust scam engagement that convincingly wastes scammers‚Äô time without risking users.\nClear sender lifecycle: unknown ‚Üí blacklist/whitelist with learning over time.\n\nWhat we learned: Strong, explicit system prompts improve consistency in classification and replies.\nConservative defaults and reproducible workflows reduce noisy or ambiguous outcomes.\nSimple states (whitelist/blacklist/none) make automation auditable and robust.\nHumor and confusion patterns are effective for long scammer engagement.",
    "hackathon": null,
    "prize": "Winner The Hackathon WINNER; Winner Best Megatrends - Build Something That Solves a Future Problem - Today Challenge project",
    "techStack": "gmailapi, javascript, n8n, openaisdk, postgresql, python, redis, supabase, twilio",
    "github": "https://github.com/robyseelps/FAKE_IT_TILL_YOU_MAKE_IT-02",
    "youtube": "https://www.youtube.com/watch?v=kS1R03MzwVg",
    "demo": null,
    "team": "Vasyl Paliuha, Oleksandr Kryvolapov, Sabeel Wani, Yurii Levchenko",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/fake-it-till-you-make-it-02"
  },
  {
    "title": "LES-01",
    "summary": "LES-01 is an innovative application designed to enhance daily life through artificial intelligence. While specific functionalities are not detailed, its focus on making users' days better suggests it likely integrates personalized AI-driven features to improve productivity or well-being.\n\nKEY ACHIEVEMENTS\nLES-01 distinguished itself by winning the \"Best AI that Makes Your Day Challenge,\" indicating exceptional performance in delivering user value and innovation. Its ability to stand out among competitors likely stems from a unique approach to user engagement and effective problem-solving through AI.\n\nThe project leverages a robust technology stack including ai-sdk for AI functionalities, Next.js for server-side rendering, and Tailwind CSS for responsive design. The use of TypeScript enhances code reliability, while Supabase provides scalable backend services, and Zod ensures type safety and validation for data handling.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in integrating various technologies seamlessly, ensuring performance and user experience, and addressing potential scalability issues as user engagement grows. Additionally, aligning the AI functionalities with user needs and expectations would have required continuous iteration and testing.\n\nFUTURE POTENTIAL\nLES-01 has significant potential for evolution by expanding its feature set to include more advanced AI capabilities, such as predictive analytics or personalized recommendations. Future iterations could also explore integration with other platforms or devices to enhance user interaction and functionality, positioning it as a comprehensive daily assistant.\n\nWhat it does: What2Eat is an AI-powered personal nutrition assistant designed to be your go-to kitchen companion. Here‚Äôs what it does: Personalized Nutrition Tracking: It calculates your daily targets for calories, protein, carbs, and fat based on your profile. The dashboard provides a beautiful, at-a-glance view of your progress with animated circular progress bars.\n  Smart Meal Suggestions: Don't know what to cook? Tell the app what ingredients you have on hand, and our AI wizard will generate delicious meal ideas for you.\n  Instant Recipe Finder: Already know what you want to make? Instantly get a detailed recipe with ingredients and step-by-step instructions.\n  Automatic Shopping Lists: Every recipe you choose can automatically generate a shopping list, so you never forget an ingredient at the store\n\nInspiration: The inspiration for What2Eat came from a universal, daily struggle: standing in front of a full fridge and having no idea what to cook. This often leads to decision fatigue, food waste, and unhealthy choices like ordering takeout. We wanted to create a smart, simple solution that removes the stress from meal planning and empowers people to eat better using the food they already have.\n\nHow it was built: What2Eat is a modern web application built with a focus on a clean user experience and a robust technical foundation. Frontend: We used Next.js with React and TypeScript to build a fast, server-rendered, and type-safe application.\n  UI/UX: The user interface was crafted using shadcn/ui for its excellent, accessible component library, styled with Tailwind CSS. We designed custom components, like the CircularProgress SVG element, to create a unique and intuitive data visualization experience.\n  State Management: Client-side state and interactivity are managed using React Hooks (useState, useEffect). We implemented a custom useWizard hook to handle the modal flows for our core features, ensuring a smooth user journey.\n  Backend: A Next.js API route (/api) serves as the endpoints to fetch dyna\n\nChallenges: One of the biggest challenges was figuring out how AI should work inside the app. There were so many potential directions - meal planning, fridge detection, grocery optimization, macro analysis - and we had to clearly define what problems our assistant should actually solve. This required a lot of brainstorming, prototyping, and refining before we even started coding. Another major challenge was designing the user interface and overall user experience. Our goal was to make the app feel simple, intuitive, and ‚Äúgrandma-proof,‚Äù even though it performs complex AI-driven tasks. Creating a smooth onboarding flow, meal suggestion interactions, and recipe generation UI pushed us to rethink layout, user choices, and how to present AI results in a clear and friendly way.\n\nAccomplishments: We are especially proud of our ingredient detection from fridge photos. Being able to take a picture, run it through AI, and automatically identify what's inside the fridge felt like a huge breakthrough for the user experience. Seeing those detected items appear in the pantry list made the app feel instantly useful and surprisingly smart. We're also proud of our detailed and thoughtful onboarding process. By collecting the user‚Äôs goals, preferences, dietary restrictions, and cooking habits, we were able to tailor AI prompts with high precision. This led to highly personalized and accurate recommendations throughout the app. Another accomplishment is the quality of the AI-generated meal suggestions and full recipes. With careful prompt engineering and refinement, the AI consistently deliver\n\nWhat we learned: We gained a lot of experience working with modern AI tooling and full-stack technologies. From using the OpenAI SDK for streaming recipe generation, to designing database schemas and exporting typed definitions from Supabase, we learned how to integrate powerful backend services with a clean, responsive frontend. This project also gave us hands-on practice with structured prompt design, AI-driven UX flows, and building a seamless user experience around intelligent features.\n\nWhat's next: The future for What2Eat is bright! Our roadmap includes: Grocery Delivery Integration: Allowing users to send their shopping list directly to services like Instacart.\n  Automatic Weekly Meal Planning: Generating a full week's worth of meals based on user preferences and goals.\n  Dietary Preferences: Adding filters for vegetarian, vegan, gluten-free, and other dietary needs.\n  Pantry Tracking: Allowing users to maintain a virtual pantry of their ingredients.",
    "hackathon": null,
    "prize": "Winner Best AI that makes your day Challenge project",
    "techStack": "ai-sdk, nextjs, react-hook-form, shadcn/ui, supabase, tailwindcss, typescript, vercel, zod",
    "github": "https://github.com/TeamLES/hackathon-telekom-2025-What2Eat",
    "youtube": "https://www.youtube.com/watch?v=hS8PuuQIQUo",
    "demo": null,
    "team": "Matej Bend√≠k, Bc. Miroslav Hanisko, Bc. Luk√°≈° ƒåeƒç, kupkoXD Janok, Bc. Oliver Fecko",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/what2eat-etv6d0"
  },
  {
    "title": "CropGuard",
    "summary": "CropGuard Project Summary\n\nCropGuard is a technology-driven solution aimed at enhancing agricultural practices by utilizing IoT sensors, weather forecasting, and AI. The project likely focuses on monitoring crop health, optimizing irrigation, and providing actionable insights to farmers to increase yield and sustainability.\n\nKEY ACHIEVEMENTS: CropGuard stood out by integrating multiple advanced technologies, including real-time weather data and AI-driven analytics, to deliver a comprehensive tool for farmers. Its innovative use of IoT devices and the ability to provide timely insights likely contributed to its recognition as the hackathon winner.\n\nThe project is built with a robust tech stack that includes FastAPI for backend development, React Native for mobile app interface, and AI models from OpenAI and YOLO (You Only Look Once) for image recognition and analysis. The integration of Google Maps for location tracking and weather APIs for forecasting enhances user experience and functionality.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data integration from various sources, ensuring the accuracy of AI predictions, and the scalability of IoT devices in diverse agricultural settings. Additionally, achieving user adoption among farmers who may be reluctant to embrace new technologies could have posed a significant hurdle.\n\nFUTURE POTENTIAL: CropGuard has the potential to evolve into a comprehensive agricultural management platform that incorporates more advanced AI analytics and machine learning models. Future developments could include partnerships with agricultural institutions for research, expansion to cover more crops and regions, and the addition of features like pest detection and market price forecasting to further assist farmers.",
    "hackathon": null,
    "prize": null,
    "techStack": "fastapi, github, google-colab, google-maps, google-web-speech-api, iot, open-meteo-forecast, openai-api, plpgsql, react-native, roboflow, supabase, typescript, vercel, visual-studio, weatherapi, wokwi, yolo11s",
    "github": "https://github.com/Caleb-Tech001/CropGuard.git",
    "youtube": "https://www.youtube.com/watch?v=78Rddq4DgEk",
    "demo": "https://drive.google.com/file/d/1cpXxLF14d4JEKKi1MXjD6t9u7B16OLRX/view?usp=drive_link",
    "team": null,
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/cropguard"
  },
  {
    "title": "AURA ‚Äî Autonomous Unified Review Agent",
    "summary": "AURA ‚Äî Autonomous Unified Review Agent\n\nAURA is an intelligent platform designed to streamline and automate the review process across various domains, such as code reviews or product feedback. By leveraging advanced algorithms, it consolidates and analyzes user inputs, offering actionable insights to enhance productivity and decision-making.\n\nKEY ACHIEVEMENTS: AURA distinguished itself by winning the overall hackathon and receiving the Best Productivity Award. Its ability to effectively automate and unify review processes likely contributed to its recognition, showcasing significant improvements in efficiency and user experience.\n\nThe project utilizes a robust tech stack, including JavaScript and React for the frontend, Node.js for backend services, and PostgreSQL for data management. Additionally, it integrates OpenAI capabilities, enhancing its analytical and conversational features, and Python for data processing tasks, ensuring versatility and performance.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating various technologies smoothly, ensuring data accuracy in reviews, and managing user input variability. Additionally, maintaining the system's responsiveness and scalability as user demand grows would have been critical considerations.\n\nFUTURE POTENTIAL: AURA has the potential to evolve into a comprehensive review platform that incorporates machine learning to further personalize and improve its feedback mechanisms. Future enhancements could include integrations with other productivity tools, real-time collaboration features, and an expanded range of applications beyond reviews, such as project management and team performance analysis.\n\nWhat it does: AURA is a fully autonomous AI engineering assistant that transforms how developers approach code quality. Here's what it does:\n\nInspiration: The inspiration for AURA came from a fundamental problem we've all experienced as developers: the endless cycle of manual code reviews, test writing, and bug fixing. We noticed that developers spend 30-40% of their time on repetitive QA tasks, and critical bugs often slip through to production, costing companies billions annually. Traditional code analysis tools are reactive‚Äîthey find problems after they exist. We asked ourselves: What if we could predict and prevent issues before they happen? AURA was born from the vision of creating a truly autonomous AI agent that doesn't just analyze code‚Äîit actively improves it, generates tests intelligently, predicts regressions, and takes automated actions. We wanted to build something that works 24/7, learns from your codebase, and becomes smarter\n\nHow it was built: We built AURA as a modern, scalable full-stack application with a focus on modularity and extensibility.\n\nAccomplishments: We're incredibly proud of what we've built in this hackathon:\n\nWhat we learned: This hackathon was an incredible learning experience:\n\nWhat's next: IDE Integration ‚Äî VS Code and IntelliJ plugins for real-time inline suggestions\nMulti language support expansion\nTeam collaboration feature",
    "hackathon": null,
    "prize": "Winner Best Productivity Award",
    "techStack": "javascript, node.js, openi, postgress, python, react",
    "github": "https://github.com/palsure/AURA?tab=readme-ov-file",
    "youtube": "https://www.youtube.com/watch?v=JJivljuk9T0",
    "demo": null,
    "team": null,
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/aura-autonomous-unified-review-agent"
  },
  {
    "title": "Spell Sword",
    "summary": "Spell Sword Hackathon Project Summary\n\nSpell Sword is an innovative project that merges interactive storytelling with immersive gameplay, allowing users to engage in a fantasy world where they can cast spells using intuitive controls. The project leverages augmented reality or virtual environments to enhance user interaction and experience.\n\nKEY ACHIEVEMENTS:  \n   The project stood out by winning two prizes: the overall winner and the award for Best Portrait Mode Implementation, showcasing its exceptional execution and unique approach to user engagement. Its ability to seamlessly integrate gameplay mechanics with artistic presentation likely contributed to its recognition.\n\nSpell Sword utilizes a range of advanced tools and technologies, including Adobe Illustrator and Photoshop for high-quality graphics, and TypeScript for robust coding. The implementation of portrait mode enhances usability on mobile devices, making the experience more accessible and visually appealing.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges related to optimizing performance across different devices, ensuring a smooth user experience in portrait mode, and managing the intricate graphics and animations required for immersive storytelling. Balancing technical complexity with user-friendliness may have also posed difficulties.\n\nFUTURE POTENTIAL:  \n   Spell Sword could evolve by expanding its narrative scope and integrating multiplayer features, allowing users to collaborate or compete in real-time. Additionally, incorporating machine learning could personalize user experiences and refine spell-casting mechanics based on player behavior, enhancing engagement and replayability.\n\nWhat it does: Spell Sword is a fantasy turn based RPG that blends strategic wordplay with classic combat progression. Players form attacks by crafting words from dynamic letter pools, using vocabulary skill as a core combat mechanic. As they battle waves of monsters they earn loot, upgrade equipment, unlock consumables and progress through daily quests that encourage long term engagement. Designed for both casual players and RPG fans, Spell Sword aims to deliver an accessible but demanding experience that stands as the first chapter of a larger saga.\n\nInspiration: Wield your words as weapons against waves of monsters. Defeat numerous foes, gather loot, upgrade your gear and complete daily quests as you push deeper into battle in this turn based RPG experience.\n\nHow it was built: The project began as a full exploration RPG with multiple classes, co-op play and story driven quests. To align with the Mobile Innovation theme I intentionally narrowed the scope to a focused combat scope that is optimized for a comfortable one handed portrait orientation mobile game. The result is a one player experience where the challenge comes from choosing the right word at the right moment rather than learning complex input patterns.\n\nMost of the early work went into building the word engine. I developed dictionary utilities, custom rarity tiers and a scoring model for each letter, then curated a 170,000+ word list tuned for modern English, fantasy terms and a few seasonal extras. I merged and cleaned multiple sources, filtered out low quality entries, and created a separate block l\n\nChallenges: On top of this system I designed a combat loop across 99 floors of increasing difficulty. Each floor considers enemy archetypes, player stats, equipment tiers, consumable drop rates and three player mindsets: casual, dedicated and pay-to-win. Balancing those factors produced more than 4,700 data points that shape a curve which starts generous and ramps into a meaningful challenge. The first 10 floors are intentionally forgiving so new players can jump straight into the fun. Deeper in, stronger weapons, armor, and afflictions become available and a simple word choice turns into a tactical tradeoff.\n\nAfflictions are a key layer. Bleeding accelerates as the player is hit, poison becomes more dangerous the longer a fight lasts, and curse behaves like poison while making the victim more vulnera\n\nAccomplishments: To make combat feel like a modern RPG instead of a static puzzle I rebuilt the presentation around the player avatar. An avatar and NPC controller drives animations, visual effects, sound cues and the health bar, while a spline based camera system glides through cinematic angles instead of cutting between fixed shots. The result feels closer to a steady cam following action sequence than a simple 2D panel of letters.\n\nLater in development I partnered with Takatado, whose strengths in environment and 2D art complemented my focus on systems and UX. Together we replaced the placeholder arena and wireframe UI with a cohesive visual identity: a damaged stronghold that hints at past battles and a clean, legible interface built for phones. The chibi style monsters, knight and spell tiles carry th\n\nWhat we learned: Under the hood, world persistent variables and JSON configuration files drive almost everything, including word lists, shop inventory, drop tables, starting gear, and equipped avatar item traits. This keeps the experience data driven and easy to tune after launch. Custom leaderboards highlight not only highest levels and damage dealt, but also the best words used in battle giving players recognition for both power and creativity while respecting the filtered word lists.\n\nWhat's next: In an age dominated by AI generated brain rot and low effort doom scrolling, Spell Sword is my attempt to reward players for skills that rarely get attention on the Meta Horizon platform: vocabulary, planning, and critical thinking. My hope is that this focused combat slice can grow into a broader universe with raid style bosses, additional character classes, co-op play, PvP tournaments and exploration worlds with cognitive skill challenges that all feed into this same word powered combat system.",
    "hackathon": null,
    "prize": "Winner Best Portrait Mode Implementation",
    "techStack": "adobe-illustrator, flaticon-license, horizonworldsdesktopeditor, photoshop, premiere, sunoai-license, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=tcNFya_hTDU",
    "demo": "https://horizon.meta.com/world/10172905149910634/",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/spell-sword"
  },
  {
    "title": "GoodVibes Connect",
    "summary": "GoodVibes Connect Project Summary\n\nGoodVibes Connect is a platform designed to foster positive interactions among users, likely focusing on community engagement and support. It aims to create a space where individuals can connect, share uplifting content, and form meaningful relationships.\n\nKEY ACHIEVEMENTS: This project distinguished itself by effectively combining user-friendly design with robust functionality, allowing seamless communication and interaction. Winning both the top prize and the second place suggests that the judges found its concept innovative and impactful, potentially addressing a significant need for mental wellness and community support.\n\nGoodVibes Connect was built using a modern tech stack, including Docker for containerization, MongoDB for scalable database management, Node.js for server-side operations, and React for a dynamic user interface. This combination enabled efficient development and deployment, as well as the ability to handle real-time user interactions.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user engagement, ensuring the platform remains a positive space without toxic interactions. Technical hurdles could have included managing real-time data updates, scaling the application as user numbers grew, and ensuring data privacy and security.\n\nFUTURE POTENTIAL: GoodVibes Connect could evolve into a more comprehensive mental wellness platform by incorporating features such as personalized content feeds, professional support options, and community-driven events. Expanding its reach through mobile applications or partnerships with mental health organizations could further enhance its impact and user base.\n\nWhat it does: GoodVibes Connect provides: Resident-facing features: browse and request shared resources, join or create events, view neighbors.\nAdmin features: approve resource requests, manage apartment members, create apartment-wide announcements and events.\nRole-based access: resident, apartment_admin, super_admin (platform).\nReal-time-friendly foundation for notifications and event participation.\nSecure auth with JWT and per-apartment scoping. Core flows: Auth: register / login ‚Üí JWT with role.\nResources: list ‚Üí request ‚Üí approve ‚Üí return.\nEvents: create ‚Üí RSVP/participate ‚Üí list by apartment.\n\nInspiration: GoodVibes Connect was inspired by the need for stronger, safer, and more connected apartment communities. The project started from observing fragmented neighborhood communication ‚Äî lost notices, unused shared resources, and low event turnout ‚Äî and imagining a single app that makes coordination simple and inclusive.\n\nHow it was built: Frontend: React SPA (context-based auth, modular API clients).\nBackend: Express + Mongoose (ES modules), RESTful controllers and service layer.\nData: MongoDB for flexible, apartment-scoped schemas (User, Apartment, Resource, Event).\nDev tooling: nodemon, Jest, Supertest for backend tests; React Testing Library for frontend.\nRBAC: role field on User model, middleware that injects req.user from JWT and allows route-level checks. Architectural notes: Controllers are thin; business logic lives in services for testability.\nSchemas enforce relations via ObjectId refs (events -> users/apartment).\nSeed script creates sample apartment, admin, and resident users for dev/testing.\n\nChallenges: Designing RBAC so permissions remain simple but expressive across apartments.\nEnsuring event/resource operations are scoped to an apartment (multi-tenant safety).\nHandling concurrent resource requests and avoiding race conditions ‚Äî required cautious service-level checks.\nBalancing simplicity for residents with enough admin controls for apartment_admins.\nTest flakiness early on due to DB state; solved with clear seed/reset steps in tests.\n\nAccomplishments: Clear role-based flow allowing apartment_admins to manage resources without full platform access.\nA modular backend service layer that is easy to unit test and extend.\nSeed and env tooling that enable quick dev setup (single command to create sample data).\nWell-documented API surfaces and a working frontend that exercises key flows.\n\nWhat we learned: Practical RBAC design: store a small enum (resident | apartment_admin | super_admin) and enforce via middleware ‚Äî simple and effective.\nImportance of service-level atomic checks when mutating shared objects (resources/events).\nTests that reset DB state or use dedicated test DB drastically reduce flakiness.\nUX matters: small details (event reminders, resource status labels) greatly improve adoption. A concise math example used in planning: Expected concurrent attendees for events was estimated with a simple growth model:\n\nIf each resident has probability p of attending an event, expected attendees for apartment with N residents: E[A] = N * p.\nFor capacity planning choose p based on historical turnout (e.g., p ‚âà 0.15 ‚Üí E[A] = 0.15N). (LaTeX example) Expected attendees: E[A] = N * p\n\nWhat's next: Real-time features: WebSocket notifications for resource approvals and event reminders.\nBetter analytics: attendance trends, popular resources.\nImproved conflict handling: optimistic locking or job queue for resource approvals to avoid races at scale.\nMobile-first UI improvements and push notifications.\nExpand RBAC to support finer-grained permissions and guest roles for visitors.",
    "hackathon": null,
    "prize": "Winner 2nd Place",
    "techStack": "docker, mongodb, node.js, react",
    "github": "https://github.com/nvssai11/Vibe",
    "youtube": "https://www.youtube.com/watch?v=LKRQ3Pkeckw",
    "demo": "https://drive.google.com/file/d/1XxGE2nRBf7PrIhqT5gEPUsRe_WjeOl_P/view?usp=sharing",
    "team": "VARSHITHA SRI SAI NALLAPUDI",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/goodvibes-connect"
  },
  {
    "title": "Cargigo - AI for Cars",
    "summary": "Cargigo - AI for Cars: Project Analysis\n\nCargigo leverages artificial intelligence to enhance automotive experiences, potentially offering features like predictive maintenance, driving assistance, or personalized vehicle settings. The project aims to integrate AI seamlessly into cars, optimizing performance and user interaction.\n\nKEY ACHIEVEMENTS: Cargigo stood out due to its innovative application of AI in the automotive sector, combining modern technologies to deliver a user-friendly interface and intelligent features. Winning the hackathon reflects the project's potential impact and the team's ability to execute a compelling solution under time constraints.\n\nThe project utilized a robust tech stack, including Firebase for backend services, Gemini for AI functionalities, and React for a responsive frontend. Tailwind CSS facilitated efficient styling, while Vite provided a fast development environment, allowing for rapid iteration and deployment.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating AI algorithms with real-time data from vehicles, ensuring system reliability, and addressing security concerns regarding user data and vehicle control. Additionally, creating a seamless user experience while managing complex AI processes would have been a significant hurdle.\n\nFUTURE POTENTIAL: Cargigo could evolve by expanding its AI capabilities to include more advanced features such as autonomous driving support, integration with smart city infrastructure, or enhancements in user customization based on driving habits. Partnerships with automotive manufacturers could also facilitate real-world applications and broader market reach.",
    "hackathon": null,
    "prize": null,
    "techStack": "firebase, gemini, lucide, react, tailwind, vite",
    "github": "https://github.com/RubaiyatAraaf/cargigo",
    "youtube": "https://www.youtube.com/watch?v=Ke6vHgD1bKc",
    "demo": "https://cargigo.com/",
    "team": null,
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/cargigo-ai-for-cars"
  },
  {
    "title": "Infinity Quest",
    "summary": "Infinity Quest Project Summary\n\nInfinity Quest is an interactive web application designed to engage users in a gamified experience that combines storytelling with exploration. By leveraging a dynamic user interface and real-time data management, it allows users to embark on quests, solve puzzles, and uncover narratives in a visually appealing environment.\n\nKEY ACHIEVEMENTS: \n   The project stood out due to its innovative blend of gamification and user experience design, creating an immersive platform that captivates users. Its success in the hackathon was further cemented by its polished presentation and seamless functionality, which demonstrated both creativity and technical skill.\n\nInfinity Quest incorporates a range of modern web technologies, including React for building user interfaces and Tailwind CSS for responsive design. It utilizes Supabase for backend database management, ensuring real-time data handling, while Vite provides a fast development environment. The use of motion and animation libraries enhances user engagement, making interactions more dynamic.\n\nPOTENTIAL CHALLENGES: \n   The team likely faced challenges related to integrating various technologies smoothly, ensuring cross-browser compatibility, and optimizing performance. Additionally, maintaining data consistency and managing user sessions in a real-time environment may have posed significant hurdles.\n\nFUTURE POTENTIAL: \n   Infinity Quest has the potential to evolve into a robust platform with expanded features such as multiplayer capabilities, user-generated content, and mobile accessibility. Future iterations could incorporate artificial intelligence for personalized experiences, as well as integration with other platforms to broaden its reach and user engagement.\n\nWhat it does: Infinity Quest is a browser-based arcade game where players control a gauntlet to catch falling Infinity Stones. Collect all six stones (Space, Mind, Reality, Power, Time, Soul) to unlock the ability to perform the snap. Complete 3 snaps to achieve victory. Features a global leaderboard to compete with other players.\n\nInspiration: Inspired by the iconic Infinity Gauntlet from the Marvel Cinematic Universe - the idea of collecting powerful stones and wielding ultimate power through the legendary \"snap.\"\n\nHow it was built: Frontend: React + TypeScript with Vite for fast development\nStyling: Tailwind CSS with custom cosmic-themed design system\nUI Components: shadcn/ui for polished interactions\nBackend: Lovable Cloud for database and real-time leaderboard\nGame Engine: Canvas-based rendering with React hooks for game state management\n\nChallenges: Balancing game difficulty progression to keep players engaged\nCreating smooth canvas animations while maintaining React state\nDesigning an intuitive touch/mouse control system that works across devices\n\nAccomplishments: Beautiful cosmic visual design with glowing effects and animations\nSeamless leaderboard integration with instant score submission\nProgressive difficulty system that scales with player score\n\nWhat we learned: Canvas rendering techniques within React components\nReal-time database integration for competitive gaming\nCreating engaging visual feedback through CSS animations and effects\n\nWhat's next: Sound effects and background music\nPower-ups and special abilities\nMobile-optimized controls\nMultiplayer competitive mode\nAchievement system",
    "hackathon": null,
    "prize": "Winner Certificate",
    "techStack": "canvas, cloud, css, framer, html5, lovable, motion, react, shadcn/ui, supabase), tailwind, typescript, vite",
    "github": null,
    "youtube": null,
    "demo": "https://balkrishan99.github.io/movie-verse/",
    "team": null,
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/infinity-quest-oz529k"
  },
  {
    "title": "FocusAI",
    "summary": "FocusAI Project Summary\n\nFocusAI is an innovative project designed to enhance productivity and focus through the use of artificial intelligence. The platform leverages AI algorithms to help users prioritize tasks, manage time effectively, and reduce distractions, ultimately fostering a more efficient work environment.\n\nKEY ACHIEVEMENTS: FocusAI stood out by seamlessly integrating multiple technologies to deliver a user-friendly interface and robust functionality. Its ability to provide personalized task management and focus-enhancing features likely contributed to its recognition as a winner at the hackathon.\n\nThe project utilizes a diverse tech stack, including Next.js for its powerful front-end capabilities, Python for backend logic, and Supabase for database management. The integration of n8n allows for automated workflows, enhancing user experience and efficiency. The use of TypeScript and CSS ensures that the application is both maintainable and visually appealing.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the AI algorithms could accurately assess user behavior and preferences. Additionally, integrating various technologies while maintaining performance and security standards might have posed significant hurdles during development.\n\nFUTURE POTENTIAL: FocusAI has significant potential for evolution, including the incorporation of machine learning to further personalize user experiences and predictive analytics to anticipate user needs. Expanding the platform to include collaborative features could also attract teams and organizations, broadening its user base and enhancing its utility.",
    "hackathon": null,
    "prize": null,
    "techStack": "camuda, css, gemini, html, n8n, next.js, python, supabase, typescript, vercel",
    "github": "https://github.com/4artiseth/focus-ai-wemakedevs",
    "youtube": "https://www.youtube.com/watch?v=07tCpDj20Po",
    "demo": "https://focus-ai-wemakedevs.vercel.app/",
    "team": "Taran M.R",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/focusai-bf18dx"
  },
  {
    "title": "FowazzAI",
    "summary": "FowazzAI Project Summary\n\nFowazzAI is an innovative project that leverages artificial intelligence to enhance user interactions and streamline processes within a specific domain. While specific details of its functionality are not provided, its integration of advanced technologies suggests a focus on providing intelligent solutions tailored to user needs.\n\nKEY ACHIEVEMENTS: FowazzAI distinguished itself by winning both the overall hackathon and securing third place, indicating its strong appeal to judges and participants alike. Its ability to combine multiple technologies effectively and deliver a compelling user experience likely contributed to its success.\n\nThe project utilizes a robust tech stack, including frameworks and services such as Flask for web development, Redis for data caching, and Supabase for backend services. The integration of AI through platforms like Anthropic and Claude suggests advanced machine learning capabilities, while the use of Tailwind indicates a focus on responsive and aesthetically pleasing UI design.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating various technologies seamlessly, ensuring scalability, and optimizing performance. Additionally, managing AI model training and deployment may have presented complexities, especially within a limited timeframe typical of hackathons.\n\nFUTURE POTENTIAL: FowazzAI has significant potential for evolution, including the expansion of its feature set based on user feedback, enhancement of its AI capabilities, and scaling to accommodate a larger user base. Future iterations could also explore partnerships or integrations with other platforms to broaden its reach and functionality.\n\nWhat it does: FowazzAI lets anyone create a complete, functional website just by talking. A user speaks or types: ‚ÄúI run a bakery called SweetCrumbs. Make me a modern website with my menu and WhatsApp button.‚Äù FowazzAI then: \n-Designs the website automatically \n-Generates all text + images \n-Builds the layout with modern UI -Searches and stores the Domain the user types in (via Fayez backend) -Deploys the site to production \n-Sets up hosting, SSL, and DNS automatically In less than 2 minutes, a business gets a fully deployed website, no coding, no designers, no $300 fees. For the hackathon, FowazzAI is free, because public good is the whole point. However, this could be expanded upon for commercial use in the future and an extremely small business-friendly pricing could be implemented ($5/month). Domain\n\nInspiration: Small businesses deserve a fighting chance online, but most simply can‚Äôt afford it. In many places, especially in developing regions, a basic website can cost $300‚Äì$400 upfront or expensive monthly fees. That‚Äôs a huge barrier for local shops, home-based sellers, students, and freelancers who rely on being discoverable online. I kept seeing businesses around me struggle with this: great products, horrible digital presence. They lose customers not because they‚Äôre bad, but because they‚Äôre invisible. So I built FowazzAI, an AI website builder that removes the financial and technical barriers of getting online. All you have to do is explain how you want your site, and its live in the next couple of hours on the web.\n\nHow it was built: -AI Layer: Anthropic Claude + structured prompting to interpret user intent and generate site blueprints -Backend (Fowazz): Python Flask API that converts the blueprint into actual HTML/CSS/JS sections -Provisioning System (Fayez): -Node.js (Express + TypeScript) -BullMQ queue for stable provisioning -Redis for job management -Supabase for user auth & storage -DNSimple API for domain search availability -Cloudflare Pages for hosting + automatic SSL The whole pipeline turns natural language ‚Üí finished website ‚Üí live on a custom domain.\n\nChallenges: Getting AI to generate consistent, structured website data We had to create a multi-stage planning prompt system so the model plans first, then generates clean components. Automatic domain registration Domain APIs are expensive and strict, and running them through a queue system took a lot of debugging. For now, and for the sake of this hackathon, Domain registration is done manually (cheaper and more viable option), however i do have plans of automating this work in the near future. Deployment reliability Making the system reliable enough that ANY user command produces a working website required heavy error handling. Avoiding hallucinations AI tends to invent features or code. We had to enforce templates and validation checks.\n\nAccomplishments: that we're proud of -Turning a spoken description into a real published website automatically \n-Successfully integrating AI ‚Üí design ‚Üí code ‚Üí deployment with no human developer in the loop -Making website creation accessible for $5 (currently FREE) instead of $300+ -Creating something that genuinely helps small businesses become visible online -Testing dozens of flows to make sure non-technical users can use it with zero friction\n\nWhat we learned: -AI agents are powerful, but they need guardrails -Small businesses don‚Äôt need fancy ‚Äî they need functional and fast -Deployment automation requires more engineering than the AI part -Voice interaction dramatically increases accessibility -People are excited by tools that remove ‚Äútech intimidation‚Äù\n\nWhat's next: Launch a permanent lite-tier for micro-businesses (for example $5/month) Add multi-language support (Urdu, Somali, Tagalog, French, Bahasa, etc.) Allow editing after creation using voice commands Add built-in analytics Partner with local incubators to give free websites to student entrepreneurs Create a ‚Äúmarketplace‚Äù of templates that can be mixed with AI-generated content Create an automatic domain registration API so this can be implemented with FowazzAI Offer ultra-cheap hosting so nobody ever has to pay $300 for a simple website again.",
    "hackathon": null,
    "prize": "Winner 3rd Place",
    "techStack": "anthropic, bullmq, claude, cloudflare, dnsimple, flask, ionos, node.js, railway, redis, supabase, tailwind, upstash",
    "github": "https://github.com/fmwz/FowazzAI-WebLauncherAgent",
    "youtube": "https://www.youtube.com/watch?v=vzxyUOuNyGc",
    "demo": "https://fowazz.fawzsites.com/",
    "team": null,
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/fowazzai-s7hvao"
  },
  {
    "title": "Medi Care Connect",
    "summary": "Medi Care Connect Project Summary\n\nMedi Care Connect is a healthcare-focused platform designed to facilitate communication and connection between patients and medical providers. By leveraging technology, it aims to streamline appointment scheduling, enhance patient-provider interactions, and improve overall healthcare accessibility and efficiency.\n\nKEY ACHIEVEMENTS: This project stood out by effectively addressing a critical need in the healthcare industry: the simplification of patient access to medical services. Its innovative approach to connecting patients with providers likely contributed to its recognition as a hackathon winner, demonstrating a strong understanding of user needs and market gaps.\n\nBuilt with base44, Medi Care Connect likely incorporates advanced features such as a user-friendly interface, secure data handling for patient information, and possibly integration with existing healthcare systems. Its technical implementation may include real-time notifications, scheduling algorithms, and secure messaging functionalities.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring data privacy and compliance with healthcare regulations (e.g., HIPAA in the U.S.), integrating with diverse healthcare systems, and addressing user adoption across various demographics. Additionally, scalability and maintaining a seamless user experience could have been significant hurdles.\n\nFUTURE POTENTIAL: Medi Care Connect has the potential to evolve into a comprehensive healthcare management platform, incorporating features like telehealth services, AI-driven patient analytics, and personalized health recommendations. It could also expand its reach by partnering with healthcare providers and insurance companies, ultimately enhancing patient care and engagement on a larger scale.\n\nWhat it does: A dual-interface platform that connects elderly patients and their caregivers: 1 .Seniors get a simple, large-button dashboard to confirm medications with one tap. Caregivers get a real-time dashboard showing exactly what was taken, missed, or is upcoming, with instant alerts.\n\nInspiration: Watching family members struggle to manage medications for elderly parents‚Äîthe constant worry, the frantic phone calls. We learned that 50% of seniors miss their medications, leading to 125,000 preventable deaths annually. We built Medi Care Connect to give families real-time peace of mind.\n\nHow it was built: We built the entire mobile-first, 5-page web application using the Base44 no-code platform, focusing on a professional healthcare aesthetic, responsive design, and interactive elements.\n\nChallenges: 1 .Designing an interface simple enough for seniors with limited tech literacy while still being powerful for caregivers. 2 . Simulating realistic, dynamic data within Base44 to demonstrate the real-time tracking effectively.\n\nAccomplishments: 1 . Validation: Surveyed 47 caregivers‚Äî92% said they would use our app, and 78% are willing to pay for it. 2  . Execution: Creating a fully functional, intuitive prototype that solves a real, painful problem for millions. 3  . Design: Building a senior-friendly interface that passes the \"grandmother test.\"\n\nWhat we learned: The profound emotional toll and scale of the medication non-adherence crisis. The power of no-code platforms like Base44 to rapidly transform a validated idea into a professional prototype. The critical importance of user testing and feedback, even at the earliest stages.\n\nWhat's next: The profound emotional toll and scale of the medication non-adherence crisis. The power of no-code platforms like Base44 to rapidly transform a validated idea into a professional prototype. The critical importance of user testing and feedback, even at the earliest stages.",
    "hackathon": null,
    "prize": "Winner Gift Card",
    "techStack": "base44",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=AYlIPOfb3og",
    "demo": "https://medi-care-connect-c8e2bb11.base44.app/",
    "team": "VISHAL POKALA, Neethu Priya",
    "date": "2025-12-22",
    "projectUrl": "https://devpost.com/software/medi-care-connect"
  },
  {
    "title": "EcoGuard AI",
    "summary": "EcoGuard AI Project Summary\n\nEcoGuard AI is an innovative project designed to leverage artificial intelligence to monitor and protect the environment. It aims to provide real-time data analysis and insights on ecological health, helping communities and organizations make informed decisions regarding conservation efforts.\n\nKEY ACHIEVEMENTS: The project distinguished itself by demonstrating a comprehensive use of AI in environmental monitoring, showcasing its ability to process large datasets efficiently. It won the hackathon due to its impactful application of technology in a pressing global issue, as well as the clarity and feasibility of its implementation plan.\n\nEcoGuard AI was built using the base44 framework, which likely enabled rapid development and integration of various AI components. Notable implementations may include machine learning algorithms for data analysis, real-time monitoring capabilities, and user-friendly dashboards for data visualization.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data collection and accuracy, particularly in sourcing reliable environmental data. Additionally, ensuring the AI algorithms are robust and can adapt to diverse ecological scenarios would have been a significant hurdle.\n\nFUTURE POTENTIAL: EcoGuard AI has the potential to evolve into a comprehensive platform that can integrate various data sources, such as satellite imagery and IoT sensors. Future developments could include expanding its functionality to support predictive analytics and community engagement tools, further enhancing its impact on environmental conservation efforts.\n\nWhat it does: EcoGuard AI turns everyday people into environmental protectors. Users can report issues by uploading photos with GPS, sending voice messages in local languages, or typing descriptions with a severity level.\nThe system uses AI to instantly analyze the problem, detect the type of threat, rate its danger, and give safety instructions.\nReports are automatically routed to the right authorities and local community groups.\nThe platform tracks progress and shows users the status of their complaints.\nIt also generates awareness content like safety guides and posters.\n\nInspiration: Urban India faces serious environmental problems like air pollution, water contamination, and illegal waste dumping. People witness these issues daily, but there‚Äôs no fast, simple way to report them or track action. Traditional monitoring systems are slow and limited, which created the need for a real-time, citizen-powered platform. With the rise of AI tools like voice recognition and image analysis, it became possible to build a smart and inclusive solution ‚Äî that‚Äôs what sparked EcoGuard AI.\n\nHow it was built: EcoGuard AI was built using Base44 to rapidly design and deploy a working prototype within hackathon time limits. Frontend and backend were created using Base44‚Äôs no-code/low-code environment, enabling fast, responsive web and mobile-friendly interfaces.\nAI models were integrated via APIs to handle environmental threat analysis and multi-language voice processing.\nImage and voice data were processed through Base44-connected services to generate real-time insights and safety recommendations.\nAutomation workflows were built inside Base44 to route reports, trigger notifications, and manage tracking.\nDevelopment followed an MVP-first approach, focusing on reporting, AI analysis, and alerts.\nFree and scalable tools were used to keep the system lightweight, low-cost, and easy to deploy.\n\nChallenges: Limited real-world data made it hard to fully validate AI predictions inside the Base44 environment.\nConfiguring accurate multi-language voice support through Base44 integrations was challenging.\nLocation-based routing required careful spatial mapping inside Base44.\nBalancing speed vs reliability in real-time workflows was difficult.\nReal authority integrations were simulated inside Base44 since this was a prototype.\nMedia uploads and real-time processing created performance and storage constraints.\n\nAccomplishments: Built a fully working end-to-end prototype using Base44 in a hackathon timeline.\nEnabled multi-input and multi-language reporting for real-world accessibility.\nDesigned a complete citizen ‚Üí authority ‚Üí community workflow using Base44 automations.\nDelivered a real, demo-ready product, not just a concept.\nProved that Base44 can power scalable, real-impact solutions.\n\nWhat we learned: Real-world problem solving requires strong user and environmental domain understanding, not just features.\nDesigning AI workflows inside Base44 taught us how to balance speed and accuracy.\nWe learned that trust and usability are critical for adoption.\nHandling media made us appreciate resource-efficient design.\nAn MVP-first mindset helped us build faster and iterate better.\n\nWhat's next: EcoGuard AI is just the beginning. Launch pilot deployments in small cities with NGOs and municipalities.\nImprove AI accuracy using feedback loops.\nAdd predictive intelligence to detect environmental risks early.\nExpand language, voice, and accessibility features.\nBuild community engagement tools like dashboards and gamified actions.\nCreate real integrations with government and NGO systems.\nScale across regions and develop sustainable partnerships and funding models.",
    "hackathon": null,
    "prize": "Winner Gift Card",
    "techStack": "base44",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=klF-jEgVGbk",
    "demo": "https://eco-guard-ai-e81f2588.base44.app/home",
    "team": "rakesh mahapatro, Sita Ganesh",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/ecoguard-ai-efq32a"
  },
  {
    "title": "Tailysis",
    "summary": "Tailysis Project Summary\n\nTailysis is an innovative health monitoring solution that leverages various sensors and software to provide real-time insights into physiological data, such as pulse oximetry and motion tracking. By integrating multiple technologies, the project aims to enhance personal health tracking and analysis in an user-friendly manner.\n\nKEY ACHIEVEMENTS: Tailysis distinguished itself by securing both the Winner and 2nd Place + Silver Prize Bundle at the hackathon, indicating a strong reception from judges. Its standout features likely included a seamless integration of hardware and software components, offering a practical solution to health monitoring that resonates with current trends towards personal wellness.\n\nThe project utilized a diverse range of technologies, including C++ for core programming, JavaScript for potentially web-based interfaces, and various sensor inputs such as the MAX30100 pulse oximeter and MPU6050 for motion tracking. The integration of these components, alongside creative tools like Canva for design, showcases a comprehensive technical approach.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to sensor calibration and data accuracy, ensuring the reliability of physiological readings. Additionally, integrating multiple programming languages and hardware components can complicate development, requiring effective debugging and cohesive system design.\n\nFUTURE POTENTIAL: Tailysis has significant potential for further development, including expanding its features to provide more detailed health analytics, integrating with mobile applications for better user engagement, and exploring partnerships with health organizations for broader reach. Additionally, it could evolve into a platform for personalized health insights and telehealth applications, tapping into the growing demand for remote health monitoring solutions.\n\nWhat it does: Tailysis is a biometric dog collar that tracks key physiological indicators and converts them into simple emotional and health insights.\nMeasures:\n-heart rate\n-body temperature\n-movement patterns\nIdentifies:\n-stress or anxiety\n-overstimulation\n-excitement\n-calm/rest\n-discomfort\n-early overheating\n-early sickness warning signs that owners normally cannot see\nApp features:\n-real-time emotional & health indicators\n-clear, simple insights instead of complex graphs\n-gives owners a chance to act early and protect their dog‚Äôs wellbeing\n\nInspiration: Dogs show internal stress and health signals long before humans notice them.\nChanges in heart rate, body temperature, and movement patterns often reveal early signs of sickness ‚Äî signals invisible to the human eye.\nMany owners discover something is wrong only when it‚Äôs already serious, and sometimes too late to act.\nTailysis was created to detect these early internal signals and reveal what dogs feel before visible symptoms appear.\n\nHow it was built: Hardware:\n-Custom-built using separate components: MAX30100 (heart rate), DS18B20 (temperature), MPU6050 (motion), Arduino Uno R3.\n-Hand-wired, tested, and assembled into a wearable collar system.\nFirmware:\n-Structure drafted with Anigravity AI.\n-Fully refined in Arduino IDE: optimized sampling, noise filtering, timing, and data formatting.\nApp:\n-Developed in Natively.\n-Receives Bluetooth data and translates it into readable emotional states.\nWebsite:\n-Built using Lovable to present the product and gather early sign-ups.\n\nChallenges: Building Tailysis was anything but smooth. When one sensor worked, another failed. When the hardware behaved, the app broke. If the firmware compiled, Bluetooth stopped responding. Choosing the right app tools took longer than expected, and coding issues followed us everywhere‚Äîconnection drops, formatting errors, UI bugs, and unstable readings.\nThe website‚Äôs admin page also took far more time than planned. Managing hardware, firmware, app, algorithm, and website development at the same time stretched our time and focus. There were moments when nothing aligned‚Ä¶\nBut after countless late-night fixes, rewires, and debugging cycles, everything finally connected and worked as one system. Every setback made Tailysis better‚Äîand made the final success worth it.\n\nAccomplishments: Despite the challenges, we achieved far more than we expected. Our full prototype works end-to-end ‚Äî the sensors read data, the firmware processes it, the app displays it, and everything communicates in real time. We managed to solve our coding issues under pressure and get every part of the system functioning before the deadline. We also launched a clean, professional website to present the project and gather early interest. But our biggest accomplishment wasn‚Äôt the hardware or the software ‚Äî it was the teamwork behind it. We supported each other, divided tasks, stepped in when someone got stuck, and pushed through late nights together. Tailysis exists today because we built it as a team, not as individuals. This combination of technical progress and strong collaboration is what makes us\n\nWhat we learned: This project forced us to learn faster than ever. We used many tools for the first time ‚Äî hardware sensors, app builders, and AI-assisted coding ‚Äî and had to figure everything out on the go. Some teammates started with little coding experience, so writing firmware, fixing errors, and building the app pushed us to develop real skills quickly.\nWe learned to adapt fast when things broke, stay flexible, and find solutions under pressure. Managing hardware, software, design, and testing at the same time taught us to prioritize, communicate, and stay organized.\nAbove all, we learned to work as one team ‚Äî supporting each other, stepping in when someone struggled, and solving problems together. Tailysis taught us not just new tech, but how to learn, adapt, and collaborate at high speed.\n\nWhat's next: Our next step is to finish the full prototype and move from testing hardware modules to integrating the entire system into a real, durable dog collar. Once the final electronics and form factor are complete, we plan to begin sourcing reliable suppliers and manufacturing partners. We will launch Tailysis first in Lithuania, where we‚Äôll gather real user feedback from dog owners, trainers, and veterinarians. After validating the product and refining the design, we aim to expand across the Baltic states. Once Tailysis reaches a polished, stable, market-ready version, our long-term goal is to enter the US market, where demand for pet-tech and wellbeing products is rapidly growing. Tailysis is only at the beginning ‚Äî and we‚Äôre excited for the next stages of building it into a fully developed con",
    "hackathon": null,
    "prize": "Winner 2nd Place + Silver Prize Bundle",
    "techStack": "antigravity, c++, canva, dallastemperature, javascript, lovable, max30100-pulseoximeter, mpu6050, natively, onewire, softwareserial, wire",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EbCYp4ye0FA",
    "demo": "https://expo.dev/preview/update?message=App+submission&updateRuntimeVersion=1.0.0&createdAt=2025-11-27T20%3A48%3A14.887Z&slug=exp&projectId=26846698-0826-40ff-acd0-4a5da589b37a&group=1ab415c4-6f28-41a8-a89d-17c9a2ff409b",
    "team": "Rugilƒó Noreikaitƒó",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/tailysis"
  },
  {
    "title": "Shakti",
    "summary": "Shakti is an innovative project that leverages AI technologies, including OpenAI's capabilities, to streamline communication and enhance productivity within teams. By integrating various collaboration tools like Slack and WhatsApp, the project aims to create a cohesive workflow that simplifies task management and information sharing.\n\nKEY ACHIEVEMENTS\nShakti stood out in the hackathon by effectively combining multiple platforms and AI functionalities to solve real-world collaboration challenges. Its user-centric design and seamless integration led to its recognition as a winner, demonstrating both technical prowess and practical applicability.\n\nThe project utilized a diverse tech stack, including base44 for AI layers, Google services for payment integration, and Figma for design, showcasing its versatility. Noteworthy implementations include AI-driven communication enhancements and task automation features that improve user engagement and efficiency.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating multiple APIs and ensuring data security across various platforms. Additionally, achieving a smooth user experience while managing the complexity of AI interactions may have posed significant hurdles.\n\nFUTURE POTENTIAL\nShakti has the potential to evolve into a comprehensive team collaboration platform that not only enhances communication but also incorporates advanced analytics for workflow optimization. Future iterations could explore additional integrations with other productivity tools and further AI enhancements to personalize user experiences.\n\nWhat it does: Shakti is a women-first LinkedIn + marketplace, built to empower women entrepreneurs (esp. rural). It solves the financial and mentorship gap by giving women a trusted, voice-first, AI-powered platform to pitch ideas, gain feedback, connect with investors/mentors, and grow into enterprises ‚Äî making them independent, financially secure, and socially impactful.\n\nInspiration: I was inspired to build this app after meeting a rural woman who made amazing homemade pickles but had no way to showcase her idea or get financial support. She once told me, ‚ÄúIf someone believed in me, I could change my family‚Äôs future.‚Äù\nThat one sentence stayed with me.\nI realized there are thousands of women like her‚Äîfull of ideas, talent, and courage, but invisible to the world. My app is inspired by them. It‚Äôs a small effort to give their ideas a platform, connect them with the right investors, and help them stand on their own feet with confidence.\nA place where a woman‚Äôs dream doesn‚Äôt end because she lacked support.\nThese examples prove that women just need visibility, mentorship, and financial backing to scale. My app bridges this gap.\n\nHow it was built: We built this app with Base44.",
    "hackathon": null,
    "prize": "Winner Gift Card",
    "techStack": "base44, chat-gpt, figma, gemini, google, google-pay, notion-/-google-docs, open-ai, openai-(through-base44-ai-layer), slack, whatsapp",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=mjx2NnChqqs",
    "demo": "https://www.pi.inc/docs/384344906671680?share_token=UAZX5GY4KTWIQ",
    "team": "Asritha Nallamalli, Amrutha Bingi",
    "date": "2025-11-27",
    "projectUrl": "https://devpost.com/software/shakti-v7lnk6"
  },
  {
    "title": "AquaGuard",
    "summary": "AquaGuard Project Summary\n\nAquaGuard is a project designed to enhance water quality monitoring and management through an intuitive digital platform. It likely leverages user-friendly design tools to present data visually, making it accessible for both consumers and environmental agencies.\n\nKEY ACHIEVEMENTS: AquaGuard distinguished itself by not only winning the \"Best Overall Solution\" award but also by effectively addressing a critical environmental issue‚Äîwater quality. Its focus on user experience and impactful design likely resonated with judges, showcasing its potential for real-world application.\n\nThe project was developed using Adobe Illustrator and Figma, which indicates a strong emphasis on design and user interface. The integration of visual elements likely aids in data interpretation, making complex information easier to understand for various stakeholders.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to data accuracy and integration, ensuring the platform provides real-time updates on water quality. Additionally, balancing aesthetics with functionality in design could have been a significant hurdle during development.\n\nFUTURE POTENTIAL: AquaGuard has the potential to evolve into a comprehensive water management system, incorporating features like real-time alerts, community engagement tools, and partnerships with environmental organizations. Expanding its capabilities could position AquaGuard as a vital resource for municipalities and environmental advocates.\n\nWhat it does: AquaGuard helps users see the current water quality in their area, perform a simple home water check, report problems such as smell, color, rust, pressure issues, and view their previous reports. It also includes a clean profile page and a clear user flow that makes the app easy to understand even for someone who is not familiar with technology.\n\nInspiration: The idea for AquaGuard came from a simple problem: people often do not know whether the water in their home is safe to use. Many face issues like rusty color, strange smell, low pressure or unclear tap water, but they have no quick way to understand what is happening. I wanted to create an app that lets anyone easily check the condition of their water and report a problem in just a few steps.\n\nHow it was built: I built AquaGuard in Figma. I created the main screens, including the home page, the ‚ÄúReport a Problem‚Äù process, the profile card and all the interface elements like buttons, icons, cards, and the bottom navigation bar. Then I connected everything through Figma‚Äôs prototype mode to make the app interactive.\n\nChallenges: One of the challenges I ran into was organizing the data in a way that stayed simple for users but still worked reliably in the system. I also had difficulties with time management, because some features took longer to design and polish than I expected. Another challenge was keeping the visual style consistent, even small design changes sometimes affected the whole interface. Despite these obstacles, I managed to find solutions and move the project forward.\n\nAccomplishments: I managed to create a clean, minimalistic design, build a full interactive prototype, structure a realistic reporting flow and keep the interface simple and understandable. The project feels purposeful and visually consistent.\n\nWhat we learned: I learned how to build a full mobile UI from scratch in Figma, how to work with frames, components, alignment and interactions, how to simplify complex information and make it user-friendly, and how to design screens that work together logically.\n\nWhat's next: Next steps include adding a real-time map of water quality, creating notification alerts for dangerous water conditions, expanding the home testing steps, improving the profile page and eventually building a functional version of the app beyond the prototype.",
    "hackathon": null,
    "prize": "Winner Best Overall Solution",
    "techStack": "adobe-illustrator, figma",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=JzfOoxHhFds",
    "demo": "https://www.figma.com/proto/3OhGNuRapWHuxCTfFTFlh9/Untitled?node-id=0-1&t=rw4dGviPYEogHaQ9-1",
    "team": "–ê–ª–∏–Ω–∞ Knel",
    "date": "2025-11-29",
    "projectUrl": "https://devpost.com/software/aquaguard-aohqtg"
  },
  {
    "title": "Husky Maps",
    "summary": "Husky Maps Project Summary\n\nHusky Maps is a web-based mapping application designed to enhance navigation and location discovery for users, potentially targeting specific user needs such as outdoor adventures or urban exploration. By leveraging interactive maps, it aims to provide personalized routes and points of interest.\n\nKEY ACHIEVEMENTS: The project stood out in the hackathon for its innovative approach to user engagement, possibly integrating unique features like real-time updates or user-generated content. Winning the competition demonstrates its effectiveness in addressing user needs through functionality and design.\n\nBuilt with JavaScript, Husky Maps likely utilizes advanced libraries and frameworks for interactive mapping, such as Leaflet or Mapbox. Its implementation may include features like geolocation, dynamic data retrieval, and responsive design to enhance user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in data integration, ensuring accurate location services, and creating a user-friendly interface. Additionally, optimizing performance for real-time updates could have posed technical hurdles.\n\nFUTURE POTENTIAL: Husky Maps could evolve into a comprehensive platform by incorporating social features, such as community-driven content and user feedback, or expanding its functionality to support various environments (e.g., hiking, city navigation). Future enhancements could also explore partnerships with local businesses for promotions or integration with other apps for a seamless user experience.",
    "hackathon": null,
    "prize": "Winner Claude Credits + $!",
    "techStack": "javascript",
    "github": "https://github.com/ericyuxuanye/Claude-Project",
    "youtube": "https://www.youtube.com/watch?v=0ZGnCae04tU",
    "demo": null,
    "team": "Rohan Pandey, Eric Ye, Katie Hsu, Christina Cocona Kawai",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/husky-maps"
  },
  {
    "title": "KiKi Color!",
    "summary": "KiKi Color! Project Summary\n\nKiKi Color! is an interactive virtual reality experience that utilizes a tap-to-move control scheme to engage users in a colorful and immersive environment. Users can explore and interact with vibrant visuals while navigating through the virtual space, enhancing their sensory experience.\n\nKEY ACHIEVEMENTS: The project stood out for its innovative use of the tap-to-move mechanic, which provided an intuitive navigation option in VR, making it accessible for users unfamiliar with traditional VR controls. Its engaging visuals and immersive gameplay contributed to its recognition as a winner in both overall categories.\n\nKiKi Color! was built using Blender for 3D modeling, TypeScript for development, and VR technology for an immersive experience. The integration of these technologies allowed for smooth performance and visually appealing environments, showcasing the team's skill in developing a cohesive and interactive VR experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to optimizing the performance of the VR experience, ensuring smooth user interactions with the tap-to-move feature, and creating visually compelling art assets within the constraints of the hackathon timeframe. Balancing user engagement with technical limitations in VR could also have posed difficulties.\n\nFUTURE POTENTIAL: KiKi Color! has the potential to evolve into a broader educational tool or game that incorporates art therapy principles, allowing users to express creativity and relieve stress. Future iterations could introduce multiplayer features, different environments, or customizable elements to enhance user engagement and expand its audience.\n\nWhat it does: Super smooth experience with only 1 finger needed, maybe even playable with just one toe. Just touch the screen where-ever! When I asked my relative's 4 year old to try, I found they were even able to get the hang of it instantly. A tap-to-move portrait game: players spawn with a circle of territory color and then keep making loops to expand their territory. A percentage bar at the top of the screen tracks how much of the map is covered. Try to reach 100 percent to win! Collect food items that give auto-boosts to territory creation, plus other special power-ups around the map. Be careful: if another player cuts or touches your trail while you‚Äôre making a loop, you‚Äôre eliminated! You can eliminate others the same way by hitting their trail. It's an infinite game designed like an .io experie\n\nInspiration: Paper.io and other io-style games, with infinite collectibles and gameplay that keeps going as new players join and are eliminated. This is my first Meta world ever, and also my first tap-to-move game! I also noticed Meta Worlds does not have a game like this currently so I decided to fill the gap of \"easy to learn, hard to master\" games which succeed at being infinite and still fun!\n\nHow it was built: Blood, sweat, and TypeScript. Firstly I started by doing a lot of research and planning the style and gameplay with physical plans and Meta's AI to help bounce ideas around. Meta GenAI 3D models was used especially for the map and creating small decorations, plus GenAI was used to organize large parts of the project and generate small collectibles. Some decorations were rendered from Blender and the project mechanisms was all built inside the Meta Horizon editor along with VS Code for the actual scripts of the project.\n\nChallenges: Keeping tap-to-move easy and connected to the core mechanic, plus debugging multiplayer code for up to eight players. Initially it was difficult to create the color-trail mechanism for spawning territory. At first I wanted to spawn full stamped shapes of the territory, but eventually switched to spawning territory blocks to keep it fast and intuitive. Also faced general multiplayer sync issues, timing challenges, and making sure territory capture stayed readable and responsive.\n\nAccomplishments: It's UNIQUENESS is our proudest aspect. So far I haven‚Äôt seen any Meta world that uses tap-to-move in this mobile-style, open-and-play way, so we take pride in being the first of its kind. I tried to make the game mechanisms as smooth as possible for the player while standing out by being completing different, and for tap-to-move to feel natural throughout gameplay. I believe to have somewhat succeeded! Making a whole working game within a month while everyone is chilling for Christmas or Thanksgiving is also no easy feat, haha, lot's of zero sleep nights.\n\nWhat we learned: Handling input for portrait-style gameplay, syncing fast movement across players, refining trail collision rules, and balancing speed versus territory control. Also succeeded at learning how to use AI asset workflows efficiently without losing control of the core mechanic. I learnt that creating a world is a combo of 3d, 2d, code, editing, game design and everything in between so you have to be detailed.\n\nWhat's next: Upgrades to characters, new skins, new maps, and additional infinite modes like team-vs-team or 1v1. Doing event-themed maps, Christmas and Ice Age-themed maps with more creative hazards and powerups. Also I want to make the purchasable shop skins for colorful trails, and adding wearable avatar items could be cool, especially creative hats to fit the top-down tap-to-move aesthetic. I was very close to adding the actual skins and equipable rainbow trail in the shop but was honestly tight for time, though the basic shop is set up. Next I will definitely be honing in on the backend and then having the shop UI fully made. Also new anti-power ups that a player could throw or sabotage others with. Each mode built on the same coloring-territory core but with new obstacles, pacing, and competitive",
    "hackathon": null,
    "prize": "Winner Best use of tap-to-move",
    "techStack": "blender, typescript, vr",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Z-_LfFlmkE4",
    "demo": "https://horizon.meta.com/world/891565677371249/?hwsh=kfNiUbcBBD",
    "team": "Llama Lover",
    "date": "2025-12-07",
    "projectUrl": "https://devpost.com/software/penfight-world-id-is-in-details"
  },
  {
    "title": "AI Transfer Evaluation Tool",
    "summary": "AI Transfer Evaluation Tool\n\nThe AI Transfer Evaluation Tool is designed to streamline the assessment of AI models and their performance in various transfer learning scenarios. By utilizing advanced AI algorithms, the tool helps users easily evaluate and compare the effectiveness of different models in real-time, enhancing decision-making in AI deployment.\n\nKEY ACHIEVEMENTS: This project stood out due to its innovative approach to simplifying complex AI evaluations, making it accessible for users without deep technical expertise. Its success in the hackathon can be attributed to the seamless integration of AI capabilities with a user-friendly interface, impressing judges with its practicality and potential impact on the AI community.\n\nThe tool is built using a robust tech stack, including Next.js for server-side rendering and React for a responsive front-end experience. Notably, the integration of Claude for AI processing allows for sophisticated model evaluations, while jsPDF is utilized for generating reports, enhancing usability and functionality.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to ensuring accuracy in AI model evaluations and managing the complexity of integrating various technologies. Additionally, addressing scalability and performance issues, particularly when handling large datasets, would have been critical considerations.\n\nFUTURE POTENTIAL: The AI Transfer Evaluation Tool has significant potential for evolution, including features like automated benchmarking against industry standards, expanded support for more AI frameworks, and collaboration tools for teams. Further development could also see the incorporation of machine learning techniques to enhance predictive capabilities and user recommendations for model selection.",
    "hackathon": null,
    "prize": "Winner Claude Credits + $!",
    "techStack": "claude, jspdf, next.js, node.js, react, typescript",
    "github": "https://github.com/JosephDavisC/AI-Transfer-Evaluation-Tool",
    "youtube": "https://www.youtube.com/watch?v=F8It4V7N1hg",
    "demo": "https://transfer.joechamdani.cloud/",
    "team": "winson teh",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/ai-transfer-evaluation-tool"
  },
  {
    "title": "Project Œ±lpha",
    "summary": "Project Œ±lpha Summary\n\nProject Œ±lpha is a web-based application that utilizes geographic information systems (GIS) to visualize and analyze spatial data. By integrating JavaScript and OpenLayers for mapping and Python for backend processing, the project aims to provide users with interactive tools for data exploration and decision-making based on geographical insights.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative approach to data visualization and user interaction, leading it to be recognized as a winner in the hackathon. Its success in post-event analysis demonstrates its effectiveness in providing actionable insights from complex datasets, which impressed judges and participants alike.\n\nNotable technical implementations include the seamless integration of JavaScript with OpenLayers for dynamic map rendering, and the use of Vite for efficient front-end development. The backend, powered by Python, enables robust data processing, allowing for real-time updates and user-driven queries, enhancing the overall user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring data accuracy and handling large datasets efficiently. Additionally, optimizing the application for performance while maintaining an intuitive user interface could have posed significant hurdles during the development process.\n\nFUTURE POTENTIAL: Project Œ±lpha has the potential to evolve into a comprehensive platform for various industries, such as urban planning, environmental monitoring, and disaster response. Future enhancements could include machine learning algorithms for predictive analytics, mobile app development for on-the-go access, and broader data integration capabilities to accommodate diverse user needs.\n\nWhat it does: Project Œ±lpha is a live weekend brain that ingests timing, scoring, and telemetry to rank drivers and teams on raw pace, sector performance, optimal laps, and consistency across every session. It reconstructs races as true to life replays with accurate positions, gaps, overtakes, and anomalies, then adds AI generated commentary and audio on top so that teams, series, and fans can relive the event from any driver view. On top of that, it provides race compare tools, talent and anomaly insights for organizers, and a shared, dynamic weekend schedule hub so everyone in the paddock stays aligned.\n\nInspiration: Modern race weekends generate huge amounts of timing, telemetry, and video, but most stakeholders only see static PDFs and basic timing screens that miss the real story. Project Œ±lpha was born from wanting a single live brain for the weekend that can explain who is actually fast, who is improving, and how the race unfolded, in a way that is useful for engineers, drivers, organizers, and storytellers. It treats data not as a by product of the event but as the foundation for competitive insight and engaging narrative.\n\nHow it was built: We built a data pipeline that normalizes timing and scoring feeds, car telemetry, simulator traces, and geospatial inputs into a single lap and sector aware model for each series. Analytics services sit on top of that model to compute rankings, optimal laps, consistency metrics, race event detection, and driver versus driver deltas, which then feed a visualization layer for race replay, race compare, and weekend dashboards. Finally, an AI layer consumes structured race events and performance insights to script commentary, generate voices, and present digestible insights through web, desktop, and trackside displays.\n\nChallenges: Synchronizing different data sources at scale was a major challenge, since timing systems, telemetry loggers, and simulator outputs all use different clocks, formats, and levels of precision. We also had to design interfaces and workflows that work for very different users, such as series directors, engineers, drivers, and media teams, without overwhelming them with graphs or burying critical insights. Building AI commentary that is aligned with the actual data, stays credible to experts, and still feels engaging and human was another key challenge that required careful event modeling and constraints.\n\nAccomplishments: We can now reconstruct a full race timeline with accurate car positions, gaps, passes, and incidents, then filter that experience down to a single driver, a team, or the whole field with one click. The platform surfaces standout drivers, big improvers, unusual pace gains, and consistency outliers across a full weekend, which lets organizers spot talent and potential rule issues while helping teams and drivers understand performance quickly. Turning raw analytics into watchable race replays with AI voiceover created a bridge between deep performance data and broadcast grade storytelling that did not exist before.\n\nWhat we learned: We learned that clarity beats complexity; people want a fast answer to questions like who was actually the fastest, who executed the cleanest race, and where pace was found, not just more charts and exports. We also learned that combining a rigorous data model with a narrative layer is powerful, because series executives and commercial partners engage much more with replay, stories, and simple rankings than with raw telemetry plots. Finally, we saw that something as simple as a live, trusted weekend schedule with countdowns and status can remove a surprising amount of friction across the paddock.\n\nWhat's next: Next we plan to deepen integrations with timing vendors, simulators, and onboard systems so that more series can adopt Project Œ±lpha with minimal setup and higher data fidelity. We will expand anomaly detection for stewards and organizers, add richer talent development views that track improvement over multiple events, and introduce configurable automated reports for teams and drivers after every session. Longer term, we want to ship lightweight mobile and paddock screens so that everyone in the ecosystem, from drivers to partners, can see the same live brain of the weekend in real time.",
    "hackathon": null,
    "prize": "Winner Best of Post-event analysis",
    "techStack": "javascript, openlayers, python, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=0_WUD8kLr2Y",
    "demo": "http://tgr-alpha.dhodgejr.com/",
    "team": "David Hodge Jr",
    "date": "2025-11-24",
    "projectUrl": "https://devpost.com/software/project-lpha"
  },
  {
    "title": "Night City Ride",
    "summary": "Night City Ride Project Summary\n\nNight City Ride is an innovative transportation solution designed to enhance urban mobility within a vibrant digital environment. The project likely focuses on providing users with an immersive ride-sharing experience, leveraging augmented reality and interactive features to navigate through a futuristic cityscape.\n\nKEY ACHIEVEMENTS  \nThe project distinguished itself by winning two prizes: the overall winner and the Best potential World Broadcast Implementation. This recognition highlights its unique approach to integrating real-time data and immersive experiences, making it a standout in the competition.\n\nBuilt with metahorizoneditor and TypeScript, Night City Ride implements advanced features such as dynamic city modeling, user interaction through augmented reality, and potentially real-time traffic and navigation updates. The use of TypeScript ensures robust code quality and maintainability.\n\nPOTENTIAL CHALLENGES  \nThe team likely faced challenges related to creating a seamless user experience in a complex virtual environment, ensuring accurate real-time data integration, and addressing performance optimization for smooth interactions. Additionally, navigating the technical limitations of AR technology may have posed hurdles.\n\nFUTURE POTENTIAL  \nNight City Ride has significant potential for evolution into a comprehensive urban mobility platform, incorporating features like AI-driven route optimization, social features for user connections, and partnerships with local businesses for promotions. Expanding into real-world applications could also enhance its relevance and usability.\n\nWhat it does: Night City Ride is a multiplayer night-driving and drifting experience. Players pick from 10 distinct cars (each with unique handling and custom SFX), cruise through a neon city, and chain drifts to build score. Global leaderboards reward clean lines, long combos, and style. Lobbies let you free-drive, race to checkpoints, or join ‚Äúdrift trains‚Äù where you earn bonus points for synchronized runs with other players.\n\nInspiration: Night City Ride started from a simple wish: capture that late-night ‚Äúone more run‚Äù feeling‚Äîneon streets, wet asphalt, engine echoes‚Äîand turn it into a social, mobile-first playground where anyone can drift with friends in seconds.\n\nHow it was built: We built the world for smooth mobile play: simple touch-friendly controls, readable UI, short-session objectives, and lightweight VFX that still feel cinematic at night. Networking syncs position, scoring, and combo states so drift battles stay fair. Audio is layered (engine, tire, skid, and city ambience) to keep feedback clear even on phone speakers.\n\nChallenges: Balancing believable drift physics with accessible controls, keeping performance stable on mobile, and preventing leaderboard exploits were the toughest problems‚Äîespecially under real-time multiplayer latency.\n\nAccomplishments: A fast onboarding flow, 10 cars that truly feel different, and competitive leaderboards that make every run meaningful.\n\nWhat we learned: Mobile-first readability, instant feedback, and short rewards loops matter more than maximum complexity.\n\nWhat's next: Car customization, new districts, seasonal ranked leaderboards, daily challenges, and a photo/replay ‚Äúhighlight‚Äù system for sharing best drifts.",
    "hackathon": null,
    "prize": "Winner Best potential World Broadcast Implementation",
    "techStack": "metahorizoneditor, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=_IzSp5HYJAk",
    "demo": "https://horizon.meta.com/world/900382536487584",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/night-city-ride"
  },
  {
    "title": "Sourced",
    "summary": "Sourced Project Summary\n\nSourced is a project designed to streamline the sourcing and management of data, likely utilizing advanced data processing techniques to improve user experience and efficiency. Although specific functionalities are not detailed, the integration of various technologies suggests a robust platform aimed at enhancing data accessibility and usability.\n\nKEY ACHIEVEMENTS: Sourced stood out by winning multiple awards, including First Place and Best Solo Hack, indicating exceptional innovation and execution. Its success can be attributed to a unique approach to solving a pressing problem in data sourcing, coupled with a seamless user interface and strong performance metrics.\n\nThe project was built using a combination of Flask for backend development, React for the frontend, and Tailwind for styling, ensuring a modern and responsive design. The utilization of Gemini and SAM3D indicates advanced capabilities in data handling and visualization, making the system both powerful and user-friendly.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating various technologies and ensuring seamless communication between the frontend and backend components. Additionally, data security and scalability might have been significant concerns, especially when handling sensitive or large datasets.\n\nFUTURE POTENTIAL: Sourced has considerable potential for evolution, including expanding its functionalities to incorporate machine learning for predictive analytics or enhancing user collaboration features. Future iterations could also focus on broader data integration capabilities or customization options to cater to specific industries or user needs, making it a versatile tool in data management.\n\nWhat it does: Sourced is an AI-powered supply chain transparency tool that allows users to upload a photo of any electronic device to instantly explore its internal components and trace their global origins. It generates an interactive 3D exploded view of the device's internals and visualizes the manufacturing journey on an interactive global supply chain map. This helps consumers understand the complexity and environmental impact of their devices, directly supporting UN SDG 12 (Responsible Consumption and Production).\n\nHow it was built: I built a modern full-stack application using React 18, Vite, and Tailwind CSS for the frontend, with React Three Fiber for the high-fidelity 3D rendering and react-globe.gl for the supply chain visualization. I powered the backend with Python Flask to orchestrate a sophisticated hybrid AI pipeline: Google Gemini 2.5 Pro Vision analyzes the uploaded photo to identify the specific device model.\nMeta's SAM 3D (Segment Anything in 3D) is utilized for initial 3D reconstruction.\nGemini 2.5 Pro acts as a \"procedural engine,\" using its knowledge of real-world teardowns to generate detailed JSON specifications for internal components (batteries, logic boards, sensors) when 3D scanning is insufficient.\nGemini with Search Grounding conducts real-time research to map these components to their actual\n\nChallenges: 3D Generation from 2D Images: Extracting detailed 3D internal structures from a single 2D photo was my biggest hurdle. SAM 3D often only captured the outer shell. I overcame this by building a fallback system where Gemini procedurally generates the internal components based on teardown knowledge, requiring extensive prompt engineering to enforce physical constraints (no overlapping parts) and realistic geometry.\nComponent Positioning: Teaching an LLM to \"think in 3D coordinates\" was difficult. I had to define strict coordinate systems and \"industrial design rules\" in my prompts to ensure components like batteries and CPUs were placed logically within the device chassis.\nVisual Fidelity vs. Performance: I wanted the app to feel \"premium\" with realistic materials (glass, metal, silicon). Bal\n\nAccomplishments: The \"Magic\" Factor: I am incredibly proud of the seamless \"photo to exploded view\" experience. Taking a picture of a phone and immediately being able to interactively \"explode\" it to see the chips and sensors inside feels genuinely magical.\nHybrid AI Implementation: Successfully combining the spatial capabilities of SAM 3D with the semantic reasoning of Gemini 2.5 Pro. I didn't just use one model; I built a pipeline where they complement each other to create a result neither could achieve alone.\nReal-World Impact: I turned the abstract concept of \"supply chain transparency\" into a tangible, interactive experience that anyone can understand, making a complex global issue accessible and engaging.",
    "hackathon": null,
    "prize": "Winner First Place; Winner Best Solo Hack",
    "techStack": "flask, gemini, python, react, sam3d, tailwind",
    "github": "https://github.com/jacobamobin/Sourced",
    "youtube": "https://www.youtube.com/watch?v=_Ifkfl1hr6g",
    "demo": null,
    "team": "Jacob Mobin",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/sourced"
  },
  {
    "title": "TenantShield",
    "summary": "TenantShield Project Summary\n\nTenantShield is a web-based application designed to enhance the tenant experience by providing essential tools and resources for renters. It likely offers features such as property listings, tenant rights information, and neighborhood insights, promoting informed decision-making in the rental process.\n\nKEY ACHIEVEMENTS: \n   The project stood out due to its comprehensive approach to tenant needs, integrating various features that empower renters. Winning multiple prizes indicates recognition from judges for its innovative concept, user-centered design, and functionality that addresses common pain points in the rental market.\n\nTenantShield was built using a modern tech stack, including React for a responsive user interface, Express.js for server-side logic, and various Google APIs (like Google Places) for enhanced location-based services. The use of Vite for development streamlines the build process, while integration with OpenStreetMap enhances mapping capabilities.\n\nPOTENTIAL CHALLENGES: \n   The team likely faced challenges related to data accuracy and integration, especially when sourcing property listings and neighborhood information. Additionally, ensuring a seamless user experience across different devices and managing real-time data updates could have posed technical hurdles.\n\nFUTURE POTENTIAL: \n   TenantShield could evolve by incorporating machine learning algorithms to provide personalized property recommendations, expanding to include landlord tools for better communication and management, or integrating payment solutions for rent transactions. Partnerships with local housing authorities and real estate platforms could also enhance its offerings and user base.\n\nWhat it does: Its a website designed to help tenants recognize their housing rights and what they can do in a seemingly powerless situation with their landlords. It uses the location of the tenant to determine the housing laws in the region and allows the user to provide an image of the issue and additional context to decide: Detect the unsafe housing conditions.\nDetermine the housing laws that may have been breached.\nHelps you understand your rights as a tenant.\nAssists you in creating a follow able action plan.\nSupports you by generating a letter to provide your landlord.\nUses your location to find the nearest 6 legal clinics.\n\nHow it was built: On the front end, we used react to build the UI and to route the pages on the client side, CSS for the styling, Google Maps API for the map component and the locations of the legal clinic locations and OpenStreetMap API for the address autocomplete in the address box in the signup page. In the back end, we utilized Node.js, Express for the API endpoints, Google Gemini API to analyze the prompt and images given by the user, the Google Places API to find the nearest legal clinic and the Fetch API.\n\nChallenges: The hardest challenge we faced was the implementation of Maps in the website, the first challenge we faced was getting the address to autocomplete as a dropdown in the sign up page. The second and by far the biggest challenge was getting the Google Maps and Google Place API working but after a long debugging session, we had persevered. The third and final challenge was refining the UI/UX to make it more intuitive and easy to use.\n\nWhat we learned: We learned a lot of new technologies especially implementing OpenStreetMap, Google Maps, Google Places and Google Gemini API. Using Gemini to analyze and provide the solution to the tenants issues helped us learn tremendously about implementing the API and proper prompting. Using a completely new API in OpenStreetMap, Google Maps and Google Places to implement the address functionality, Map interface and to show the clinics near the location helped us learn more about implementing the API, understanding how it works and how it can be used in a real world situations. We also learnt how important the user experience is in making or breaking a website, if the code has perfect logic but is unusable, no one will use the website. So, we took away that a well designed website is as important as a\n\nWhat's next: Our plans are to expand from just using visual indicators of hazards, but also implementing ways to analyze documents to identify illegal leases, allowing users to create an evidence vault that stores evidence and creates a compiled document to store and use when needed. Also, expanding to for landlords to use to identify if their leases or practices they want to implement are legal in the location they are in. Links:\nGithub: https://github.com/Jason-Tan1/TenantShield",
    "hackathon": null,
    "prize": "Winner Google Swag; Winner 1st Place",
    "techStack": "css, express.js, fetch, gemini, google-gemini, google-places, html, javascript, node.js, npm, openstreetmap, react, vite",
    "github": "https://github.com/Jason-Tan1/TenantShield",
    "youtube": "https://www.youtube.com/watch?v=NkXW_5iodko",
    "demo": null,
    "team": "Ryan Reddy",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/tenantshield"
  },
  {
    "title": "ResQ.ai",
    "summary": "ResQ.ai Project Summary\n\nResQ.ai is a cutting-edge solution that leverages artificial intelligence and advanced computer vision to enhance emergency response systems. By utilizing real-time data and image processing, it aims to streamline rescue operations, improve situational awareness, and facilitate faster decision-making during emergencies.\n\nKEY ACHIEVEMENTS: The project stood out by winning both the overall hackathon and the People's Choice Award, indicating strong support from both judges and peers. This recognition likely stems from its innovative approach to solving critical issues in emergency management and its practical application of AI and computer vision technologies.\n\nResQ.ai is built using a robust tech stack that includes Dart and Flutter for the front end, FastAPI for the back end, and MongoDB for data storage. Notable implementations include the use of OpenCV for image processing, Twilio for communication functionalities, and Ultralytics for advanced AI model integration, allowing it to quickly analyze and respond to emergency scenarios.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as integrating various technologies seamlessly, ensuring real-time performance under varying network conditions, and addressing privacy and ethical considerations related to data collection and usage in emergency situations.\n\nFUTURE POTENTIAL: ResQ.ai has significant potential for evolution, including the integration of more sophisticated machine learning models, expansion to additional emergency scenarios (like natural disasters), and partnerships with emergency services for practical deployment. Future iterations could also incorporate user feedback to enhance functionality and user experience, making it an essential tool for first responders globally.\n\nWhat it does: ResQ.ai continuously monitors a person using a real-time camera feed and pose estimation. When a fall is detected:\nA 30-second alert is sent to the user‚Äôs phone.\nIf not cancelled, the system notifies emergency contacts.\nIf still unanswered, it escalates to an automated call.\nNo wearables. No buttons. Fully hands-free safety.\n\nInspiration: Many seniors and people living alone experience falls that go unnoticed for long periods. We came across stories of individuals lying on the floor for 30‚Äì60 minutes before help arrived. Wearables often failed because people forgot to wear or charge them, and panic buttons required the person to be conscious and able to reach them.\nWe wanted a solution that never depends on memory or effort-a system that is automatic, reliable, and always watching out for loved ones.\n\nHow it was built: Computer Vision:\nYOLO-Pose is used for real-time posture detection. We analyze centroid drop, velocity, and angle shifts to detect falls.\nBackend (FastAPI):\nHandles fall events, user profiles, and escalation flow.\nMobile App (Flutter):\nDisplays alerts, provides the cancel button, and handles notification logic.\nDatabase (MongoDB Atlas):\nStores users, incidents, contacts, and history.\nPipeline Flow:\nCamera ‚Üí YOLO Pose ‚Üí Fall Logic ‚Üí FastAPI ‚Üí Flutter App ‚Üí Emergency Contacts.\n\nChallenges: Integrating Python backend with Flutter¬†frontend\nEnsuring YOLO runs smoothly in real time.\nRewriting backend schemas when switching from SQL to MongoDB.\nImplementing reliable 30-second alert and escalation logic.\nDebugging Flutter + FastAPI + CV pipeline across multiple devices.\n\nAccomplishments: A working fall-detection pipeline with real-time inference.\nClean integration from vision model to backend to mobile app.\nLow latency detection with stable performance.\nA usable solution with real-world impact potential.\nA polished UI and smooth user experience.\nIntegrated SMS + voice calls with Twilio.\n\nWhat we learned: Vision-based fall detection using pose keypoints and motion metrics.\nFull-stack integration using Flutter, FastAPI, and MongoDB.\nDesigning safety-critical notification flows.\nEffective teamwork and fast iteration during a hackathon.\n\nWhat's next: Deploy on edge devices (Jetson Nano / Raspberry Pi + Coral).\nTrain a custom dataset for improved fall accuracy.\nBuild a caregiver dashboard for monitoring trends.\nExpand to detect inactivity, wandering, or health anomalies.",
    "hackathon": null,
    "prize": "Winner People's Choice Award",
    "techStack": "dart, fastapi, flutter, mongodb, numpy, opencv-python, pydantic, python, python-multipart, twilio, ultralytics, uvicorn[standard], websockets",
    "github": "https://github.com/MasterHasan095/ResQ.ai.git",
    "youtube": "https://www.youtube.com/watch?v=t9aaD7Sp1Vg",
    "demo": null,
    "team": "Gurleen kaur, Priyanshu Kaushik, Yashika Saini, DataDrivenIshan",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/resq-ai-tlied5"
  },
  {
    "title": "REAL Hackathon Browser Agent",
    "summary": "The REAL Hackathon Browser Agent is a software tool designed to enhance web browsing experiences by leveraging advanced AI models. It likely integrates functionalities such as intelligent content summarization, customized browsing assistance, and enhanced user interaction through natural language processing, streamlining user engagement with online content.\n\nKEY ACHIEVEMENTS\nThis project stood out in the competition due to its innovative approach to merging AI capabilities with browser functionalities, winning the REAL Agent Challenge. Its ability to deliver meaningful insights and personalized browsing experiences likely contributed to its recognition, showcasing both utility and creativity.\n\nThe project utilizes openrouter for seamless AI model integration, Python for robust backend development, and qwen3 for advanced natural language processing tasks. Additionally, the real-bench framework may have been employed to optimize performance metrics and ensure efficient operation, allowing for real-time processing and user interaction.\n\nPOTENTIAL CHALLENGES\nThe development team likely faced challenges such as ensuring compatibility across different web browsers, managing the complexity of integrating multiple AI models, and addressing privacy concerns related to user data. Additionally, achieving a balance between AI-driven recommendations and user autonomy could have posed a significant design challenge.\n\nFUTURE POTENTIAL\nThe REAL Hackathon Browser Agent has the potential to evolve into a comprehensive browser extension or standalone application that adapts to user preferences over time. Future iterations could incorporate more advanced machine learning algorithms for personalization, expand to support additional languages, and include collaborative features for sharing insights among users, further enhancing its utility and appeal.\n\nWhat it does: Our agent can autonomously perform everyday browser operations such as: Applying to jobs or filling out forms\nSending emails\nSetting calendar events\nBooking hotels\nNavigating links and multi-page flows Essentially, it acts as a general-purpose browser automation assistant powered by vision-language reasoning.\n\nInspiration: This hack challenge pushed us to build an agent that can operate reliably under real-world constraints ‚Äî fast, accurate, and versatile enough to handle practical browser tasks. The competitive setup and the REAL benchmark motivated us to push for an agent that could function like a true digital assistant.\n\nHow it was built: We experimented with two approaches: Enhanced Prompt Engineering + Reflection Loop\nWe began with an existing agent framework and attempted to strengthen reliability using a self-reflection feedback loop.\nThe goal: enable the agent to critique its past actions and improve.\nOrchestrator-Based Architecture\nWhen reflection alone didn‚Äôt yield stable performance, we added an orchestrator model to structure tasks, guide decisions, and maintain coherence through complex multi-step interactions. This combination gave us a more stable and efficient agent pipeline.\n\nChallenges: API credit limitations restricted our ability to extensively test iterations.\nFlow connection issues, especially when integrating multiple models.\nDebugging multi-agent control with a VLM was harder than expected.\nWe spent nearly 4 hours debugging a ‚Äúheadless: false‚Äù issue in the browser runtime.\n\nAccomplishments: Our agent successfully completed a large variety of REAL-style tasks.\nDespite limited time and compute, we were able to create a pipeline that runs reliably across multiple task types.\nWe validated that a hybrid approach (reflection + orchestrator) can significantly improve consistency.\n\nWhat we learned: Multi-agent systems with VLMs are extremely hard to synchronize ‚Äî keeping track of state, screenshots, and browser feedback loops is non-trivial.\nReal-world agents require tight control, error recovery, and robust interface mapping.\nDebugging browser automation under time pressure teaches patience and resilience.\n\nWhat's next: We plan to: Polish the agent into a long-term entrant for the 3-month REAL global leaderboard challenge.\nImprove error recovery, latency, and batching.\nAdd deeper reasoning layers and stronger UI element detection.\nPush toward a production-grade autonomous browser assistant.",
    "hackathon": null,
    "prize": "Winner The REAL Agent Challenge",
    "techStack": "openrouter, python, qwen3, real-bench",
    "github": "https://github.com/maxxie114/REAL-Hackathon-browser-agent-benchmark",
    "youtube": "https://www.youtube.com/watch?v=23QXpfPQxjo",
    "demo": null,
    "team": "Peixi Xie, DHRUV PATEL, Yuvraj Gupta",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/real-hackathon-browser-agent"
  },
  {
    "title": "Factify",
    "summary": "Factify Project Summary\n\nFactify is an innovative application designed to enhance information accuracy by providing users with fact-checking capabilities. By leveraging advanced technologies, it helps users verify the authenticity of claims and sources in real-time, promoting informed decision-making.\n\nKEY ACHIEVEMENTS: Factify stood out in the hackathon for its effective integration of multiple technologies and its user-friendly interface, which allows seamless fact verification. The project won both the overall winner and the second-place prize at MadHacks, highlighting its impact and potential utility in combating misinformation.\n\nThe project utilizes a combination of Chromium for rendering web content, Electron for desktop application development, and a robust backend built with Python. The integration of GeminiADK enhances its capabilities, allowing for sophisticated data handling and processing, while a responsive design achieved through CSS and HTML ensures an intuitive user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data accuracy and the reliability of sources for fact-checking. Additionally, ensuring real-time performance and scalability as user demand grows could have posed significant technical hurdles.\n\nFUTURE POTENTIAL: Factify has the potential to evolve into a comprehensive platform that not only verifies facts but also educates users on critical thinking and media literacy. Future iterations could include features like community-driven content moderation, personalized fact-checking feeds, and partnerships with news organizations to enhance content credibility.",
    "hackathon": null,
    "prize": "Winner MadHacks Second Place",
    "techStack": "chromium, css, electron, geminiadk, html, javascript, python",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Fg-sF2ptWUQ",
    "demo": null,
    "team": "Wylie Dituri, Nikhil Tiwari, Patrick Wang, Rishabh Dubey",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/factify-yfn86c"
  },
  {
    "title": "A Muted Melody",
    "summary": "A Muted Melody Project Summary\n\n\"A Muted Melody\" is an innovative project that likely combines interactive storytelling or gameplay with music elements, allowing users to experience a unique auditory journey. The project emphasizes the emotional impact of sound and silence, creating a captivating narrative that engages players through their choices.\n\nKEY ACHIEVEMENTS: This project stood out by winning the \"Best Overall\" prize, indicating its exceptional execution and creativity. The integration of C# and Godot showcases a polished user experience and effective gameplay mechanics, highlighting the team's ability to deliver a compelling product within the hackathon timeframe.\n\nNotable technical implementations likely include custom audio systems that manipulate sound to enhance storytelling, seamless integration of visual and auditory elements using Godot's capabilities, and potentially innovative gameplay mechanics that respond to user interactions in real-time.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as time constraints typical in hackathons, ensuring the audio-visual synchronization was flawless, and balancing gameplay mechanics with narrative depth. Additionally, debugging issues in a short period could have posed significant hurdles.\n\nFUTURE POTENTIAL: \"A Muted Melody\" has the potential to evolve into a fully-fledged game or interactive experience, expanding its narrative depth and exploring various themes related to sound and silence. Future iterations could incorporate more complex gameplay mechanics, multiplayer features, or even VR/AR elements to enhance user immersion and engagement.\n\nHow it was built: This game was built from the ground-up with pixel drawings of the backgrounds, decorations and characters using apps such as Procreate and Krita, as well as our own original soundtrack composed on the piano. In terms of coding, we used Godot.\n\nChallenges: One notable challenge we faced were the time constraints. 48 hours is a very short time to develop a game, and we may have been overambitious in places, leading to some cuts during the process. Additionally, it took us a lot more effort than expected to record, edit and properly implement the music, but we believe that the ambience that it created was definitely worth it. Another setback that arose during the creative process was a series of miscommunications caused by a misalignment in the perception of the gameplay, resulting in minor delays in the asset creation and level design.\n\nWhat we learned: This project has taught a variety of valuable lessons and skills, such as an improved proficiency in art design and coding, accompanied by an improvement to our teamworking and organisational skills. Furthermore, Additionally, we also learned how to adequately communicate with our peers in order to convey our ideas in a clear and effective manner, ensuring that we avoid any miscommunications or misinterpretations that may lead to potential setbacks. By researching the topic of grief, we learned a variety of information about how the stages progress and are displayed in other people, allowing us to replicate this in our protagonist Melody.\n\nWhat's next: In the future, we aim to employ the meanings of different colours in order to further convey emotion in our scenes by covering them in a ‚Äòtint‚Äô. For example, the colour red is often associated with anger and therefore was a perfect choice for the second stage of grief, whereas blue was useful for building a good atmosphere for the depression stage. Our original intent for the true ending of Melody‚Äôs story is much darker than it may seem. Indeed, the death of Elise was completely false, given that Elise does not exist. Instead, the story that plays is a metaphorical grief that Melody feels for herself, as she is currently in a coma. Elise represents her conscious or ‚Äòalive‚Äô self that she believes has been lost ever since she has entered the coma, acting as a coping mechanism. Eventually, we",
    "hackathon": null,
    "prize": "Winner Best Overall",
    "techStack": "c#, godot",
    "github": "https://github.com/GitMarkedDan/AMutedMelody",
    "youtube": "https://www.youtube.com/watch?v=xPCTwyVCGZo",
    "demo": null,
    "team": "GMD G",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/a-muted-melody"
  },
  {
    "title": "insAIght",
    "summary": "insAIght Project Summary\n\ninsAIght is an innovative project designed to leverage artificial intelligence and hardware integration to enhance user experiences in specific environments, potentially focusing on surveillance, monitoring, or automated assistance. Utilizing a combination of Arduino, ESP32-CAM, and AI technologies, it aims to offer real-time insights or automation through intelligent data processing.\n\nKEY ACHIEVEMENTS:  \n   The project stood out due to its effective integration of multiple technologies, including AI-driven decision-making and real-time data capture via camera systems. Winning both the main prize and the third place indicates a strong execution of its concept, likely demonstrating a unique solution to a pressing problem or an impressive user experience.\n\n- Multi-Technology Integration: Effective use of Arduino and ESP32-CAM for hardware connectivity and data collection.\n   - AI Utilization: Implementing C++ and Python for developing AI algorithms that process and analyze data in real-time.\n   - Cloud Services: Utilization of Gemini and Vultr for backend support and data management, ensuring scalability and reliability.\n\nPOTENTIAL CHALLENGES:  \n   Challenges the team may have faced include hardware limitations in processing power and memory, difficulties in real-time data transmission, as well as optimizing AI algorithms for accuracy and speed. Additionally, ensuring user privacy and data security in AI applications could have posed significant hurdles.\n\nFUTURE POTENTIAL:  \n   insAIght has the potential to evolve into a more comprehensive solution by expanding its applications to various industries, such as smart homes, healthcare monitoring, or urban surveillance. Integrating additional features such as mobile app connectivity or enhanced machine learning capabilities could further increase its value and user base.\n\nWhat it does: insAIght captures an image using an ESP32-CAM or a smartphone and sends it to our cloud server. The server processes the photo with Gemini Vision, generates a detailed description, and then converts that text into natural speech using ElevenLabs.\nThe user receives an audio message that explains what the camera sees ‚Äî objects, people, signs, obstacles, text, and general context of the scene.\nThe system works both with the ESP32-CAM device and a simple mobile interface, making it accessible to everyone.\n\nInspiration: In Mexico, more than 2.69 million people live with visual disabilities (INEGI, 2020). Many assistive devices that help visually impaired individuals understand their surroundings are expensive or hard to obtain.\nWe wanted to build something affordable, portable, and powered by modern AI, so anyone could point a camera, take a picture, and instantly hear what is around them.\nThat idea became insAIght ‚Äî an intelligent, low-cost assistant designed to make visual information accessible through audio.\n\nHow it was built: We developed a three-part system: ESP32-CAM device\nCaptures photos on command\nSends compressed JPEG images over WiFi\nCommunicates with our backend through HTTP\nCloud backend\nReceives images from ESP32-CAM or phone\nSends the image to Gemini Vision API\nProcesses and formats the model‚Äôs response\nForwards the description text to ElevenLabs\nAudio delivery\nElevenLabs generates a clear, natural voice\nThe audio is returned to the user‚Äôs device\nDesigned for quick response and low friction.\n\nChallenges: ESP32-CAM instability (WiFi drops, memory limits)\nEnsuring images uploaded correctly without corruption\nReducing latency between picture ‚Üí analysis ‚Üí audio\nHandling low-light or noisy images\nBalancing clarity and length in AI descriptions.\n\nAccomplishments: Building a complete end-to-end assistive tool in limited hackathon time\nSuccessfully integrating Gemini Vision + ElevenLabs + IoT\nMaking the system work both on hardware and mobile\nCreating a prototype that is actually useful for visually impaired users\n\nWhat we learned: How to optimize microcontrollers for image capture\nHow to design AI pipelines combining vision + audio\nPractical IoT communication and error handling\n\nWhat's next: Add real-time object tracking with continuous audio feedback\nIntegrate OCR + text reading for signs, menus, labels\nImprove night-mode performance with better image preprocessing\nBuild a wearable version (e.g., a small clip-on camera",
    "hackathon": null,
    "prize": "Winner Tercer Lugar",
    "techStack": "arduino, c++, elevenlabs, esp32cam, gemini, godaddy, python, vultr",
    "github": "https://github.com/xalbertho/Polihacks.git",
    "youtube": "https://www.youtube.com/watch?v=sEJ19GMV-Zk",
    "demo": "http://insaight.club/",
    "team": "Electronics and esp32, Jose Alberto Barrios Mendez",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/insaight"
  },
  {
    "title": "EcoPet AI",
    "summary": "EcoPet AI Project Summary\n\nEcoPet AI is an innovative project designed to enhance pet ownership through AI-driven insights and tools that promote sustainable practices. It aims to educate pet owners about eco-friendly pet care options, optimizing their choices for the well-being of both pets and the environment.\n\nKEY ACHIEVEMENTS: The project stood out for its strong social impact focus, winning both the overall hackathon and the Most Make Waves (Social Impact) prize. Its unique integration of AI to provide personalized recommendations for eco-friendly pet products and practices resonated with judges, highlighting its potential to influence positive behavioral change among pet owners.\n\nEcoPet AI leveraged a robust tech stack, including FastAPI for backend development, React for a dynamic front-end experience, and various libraries such as NumPy and Pandas for data analysis. The use of Tailwind CSS ensured a modern and responsive design, while deployment on Vultr facilitated scalability and performance.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data acquisition and accuracy, ensuring that the AI models could provide reliable recommendations based on user input. Additionally, balancing user engagement with educational content while maintaining an intuitive user interface could have presented design challenges.\n\nFUTURE POTENTIAL: EcoPet AI could evolve into a comprehensive platform that not only provides recommendations but also connects users with sustainable pet product suppliers, local pet services, and community initiatives. Integration of user feedback mechanisms and partnerships with environmental organizations could further enhance its impact and reach within the pet ownership community.\n\nWhat it does: This tool allows users to input a prompt intended for a GenAI model, then recommends the most energy-efficient LLM models from the top 50 based on the category of their task. Our application shows users the CO‚ÇÇ cost of different model choices, calculated through machine-learning data prediction score. This hopes to help them understand and be aware of more sustainable and efficient options. To incentivize better choices, it features a virtual pet which grows more energetic as you conserve CO‚ÇÇ through efficient model and prompt usage, and the energy you save can be spent to unlock cute outfits and accessories‚Äîwhile wasteful or excessive usage makes the pet tired.\n\nInspiration: We were inspired by the rapid, accelerating use of GenAI models from millions of consumers and the environmental challenges and human impact that current models present to our tech ecosystem. Training a large language model (LLM) comparable to GPT‚Äë3 has been estimated to use around 1,200‚Äì1,300 megawatt-hours of electricity and emit roughly 500‚Äì600 tonnes of CO‚ÇÇ, comparable to the lifetime emissions of several average cars or hundreds of long‚Äëhaul flights. Studies and scenario analyses suggest that, if current growth continues, AI‚Äëdriven data centers could emit on the order of 2.5 billion tonnes of CO‚ÇÇ annually by 2030, roughly 40% of current annual U.S. emissions, absent aggressive decarbonization and efficiency gains. In addition, communities near large AI driven data centers face increas\n\nHow it was built: Data prediction & Calculating Co2 score: we used HuggingFace and scraped top 50 ranked LLM models for overall tasks. Then we trained XGBoost prediction model on data we found on token/inference speed and data center locations for each of the companies to create a CO2 emissions score. FastAPI: Task classification and recommendation algorithm; endpoints React + Vite + Tailwind: UI components; Vercel for frontend deployment\n\nChallenges: Main challenge was not finding enough research or publicly available data around LLM model efficiency based on prompt use and a direct connection to environmental impact. This pushed us to use machine learning model to predict what the estimated grams of CO2 generated by different requests.\n\nAccomplishments: Deployed a production app with SSL at ecopet.boston - our first real domain!\nFull stack application development in short amount of time\nApplying ML data prediction to a real life problem\nIncentivizing product design and user intervention \nPotential for real, tangible impact; if 1,000 users save 10g CO‚ÇÇ daily = 3.65 tons/year!\n\nWhat we learned: Many things! Using ML data prediction model pipeline \nAPI deployment \nBetter git management and version control\nFull stack integration and product management\n\nWhat's next: Creating a browser extension for more integration in how people use in their day-to-day. \nUser Leaderboard and local/global ranking of how much each user is saving in co2 based use of app; team challenges, share your pet's outfits; community focus \nIntegration and support for local LLM download for users -- would further decrease user carbon footprint\nImage/text/video support\nUnlock different creatures based on total CO‚ÇÇ saved\nPartnerships where we convert saved CO‚ÇÇ into real tree-planting credits - so exciting!\nAPI for developers so other can apps integrate our recommendation engine\nMore accurate and specific CO‚ÇÇ measures for each prompt employing deeper research findings and building datasets for this use-case.",
    "hackathon": null,
    "prize": "Winner Most Make Waves (Social Impact) Hack",
    "techStack": "css, fastapi, javascript, lovable, numpy, pandas, python, react, tailwind, vite, vultr",
    "github": "https://github.com/kittyb116/WHackProject",
    "youtube": "https://www.youtube.com/watch?v=Y2egIG2HNYs",
    "demo": "http://ecopet.boston/",
    "team": "Kitty Boakye, Dana Hammouri, Qiyan Su, Sara George",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/ecopet-ai"
  },
  {
    "title": "Journey to the Center of the Library",
    "summary": "Journey to the Center of the Library\n\n\"Journey to the Center of the Library\" is an interactive board game that transforms the experience of exploring a library into an engaging adventure. Players navigate through various library sections, encountering challenges and trivia that enhance their understanding of literature and library resources.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative blend of entertainment and education, making library exploration fun and interactive. Winning both the overall hackathon and the Best Board Game prize highlights its creative design, effective gameplay mechanics, and educational value.\n\nThe project was developed using ebitengine, a game development framework for Go, which allowed for efficient rendering and game mechanics implementation. The use of Go's concurrency features likely facilitated smooth gameplay and real-time interactions, enhancing the overall user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in balancing educational content with engaging gameplay, ensuring that the game remains enjoyable while effectively conveying library-related knowledge. Additionally, technical challenges related to game design and implementation within the ebitengine framework may have arisen.\n\nFUTURE POTENTIAL: This project could evolve into a digital version or a mobile app, expanding its reach to a wider audience. Incorporating online multiplayer capabilities could create a community around library exploration, and integrating additional features like augmented reality could enhance interactivity and engagement further.",
    "hackathon": null,
    "prize": "Winner Best board game",
    "techStack": "ebitengine, go",
    "github": "https://github.com/Hatchibombotar/journey-game-jam",
    "youtube": "https://www.youtube.com/watch?v=fgq4Z10Xp1U",
    "demo": "https://hatchibombotar.itch.io/journey-to-the-center-of-the-library",
    "team": "Ted Brunner",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/journey-to-the-center-of-the-library"
  },
  {
    "title": "Electric Chair",
    "summary": "Electric Chair Hackathon Project Summary\n\nThe Electric Chair project creatively combines elements of robotics and interactive technology to create an engaging and potentially shocking experience. It likely involves a chair equipped with sensors and actuators that simulate an electric shock effect, leveraging user interaction and advanced programming to enhance the thrill.\n\nKEY ACHIEVEMENTS: This project stood out by winning the titles of \"Winner\" and \"Most Dangerous Hack,\" indicating its innovative approach and bold concept. The integration of safety measures alongside the thrill factor showcases a unique balance between entertainment and engineering.\n\nThe project utilized a diverse array of technologies, including 3D printing for custom parts, Arduino for hardware control, and C++ and Python for software programming. The use of Kinect for motion tracking possibly allowed for interactive gameplay, while Bluetooth connectivity enabled wireless communication, enhancing user engagement.\n\nPOTENTIAL CHALLENGES: The team likely faced various challenges, including ensuring user safety while delivering an intense experience, dealing with hardware integration complexities, and managing the risks associated with using high-power components like an angle grinder. Additionally, debugging the software and ensuring smooth interactions between the hardware and software components would have been critical.\n\nFUTURE POTENTIAL: The Electric Chair project could evolve into a more comprehensive entertainment experience, potentially integrating virtual reality elements for immersive gameplay. Future iterations might also explore gamification aspects, allowing users to compete or collaborate in challenges, and could focus on improving safety features to broaden its appeal to a wider audience.\n\nInspiration: As a group of four mechanical engineers in a mostly computer science-focused competition, we wanted to build something that leaned heavily into hardware, since that‚Äôs not something you see very often at events like this. Being part of Wisconsin Robotics club, we also liked the idea of creating a project that could serve as a fun outreach demo. Using spare parts in our lab, we were inspired to challenge ourselves by attempting to create a motorized chair controlled by an xbox kinect within the time frame of the hackathon. Not only would this idea challenge our knowledge of engineering fundamentals but it would also force us to deal with software issues that arise from using an unique form of input to control a robot.\n\nHow it was built: The first thing we decided was to determine what microcontroller we were using, because we wanted bluetooth support we decided to use an ESP32 and the Bluepad library to use an Xbox controller as well as a Microsoft kinect to control our robot remotely through bluetooth. We also had to select a motor controller to control our brushed dc motors. We decided on using the Pololu RoboClaw 2x60A motor controllers in order to provide the most power to our motors. Then we designed our circuit using a separate battery and buck converter to provide a stable 5V power supply. We used screw terminals to distribute the 12V battery power to our motor controllers and included an emergency stop switch. Designing the physical robot was a unique challenge in that there wasn‚Äôt enough time to design and fabric\n\nChallenges: One of the biggest challenges we encountered during this hackathon was integrating our motor controllers with our microcontroller. Since they were random spare parts we found they used incredibly outdated firmware and we struggled to make sure the serial addresses did not change. We were able to address this issue through trial and error and finding the right settings to prevent our serial addresses from being overwritten after a shutdown. Another software issue was observed when we pushed the motors to max speed, as they would suddenly stall. We fixed this by constraining our motor speeds. We had a really fun experience working on this project and we learned a lot (from integrating hardware and software, as well as some bluetooth communication). Working with the Xbox Kinect presented its\n\nAccomplishments: Overall, there were ups and downs, as any hackathon. Yet, we set a goal for ourselves and were able to accomplish it in the time we wanted. It was, in the end, a really good fricking time.\n\nWhat's next: For the future of the electric chair it would be nice to provide the Kinect its own Raspberry Pi and power supply such that it can be mounted to the current robot. This is so we can use gestures to control the robot while riding it, instead of having one passenger and a different operator.",
    "hackathon": null,
    "prize": "Winner Most Dangerous Hack",
    "techStack": "3d-printing, angle-grinder, arduino, bluepad, bluetoothserial, c++, cad, drill-press, kinect, python, roboclaw, windowssdk",
    "github": "https://github.com/Balabalu-VE/Chair_Bot",
    "youtube": "https://www.youtube.com/watch?v=MXTYrrQDJWY",
    "demo": null,
    "team": "hardware and testing",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/electric-chair"
  },
  {
    "title": "Xploit",
    "summary": "Xploit Project Summary\n\nXploit is a web application designed to streamline the process of discovering and exploiting vulnerabilities in software systems. By utilizing advanced machine learning algorithms, it aims to provide users with actionable insights and automated tools to enhance their security posture.\n\nKEY ACHIEVEMENTS: Xploit stood out by effectively merging vulnerability assessment with user-friendly interfaces and automated exploitation tools. Its recognition as a winner in two categories highlights the innovative approach and practical utility of the project, showcasing a unique blend of creativity and technical execution.\n\nThe project employs FastAPI for rapid API development, ensuring high performance and scalability. It integrates OpenRouter for dynamic routing of requests, while Pydantic is used for data validation, enhancing reliability. The combination of Python and TypeScript allows for a robust backend and a responsive frontend, respectively.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the security of the application itself, managing the complexity of vulnerability assessments, and creating an intuitive user experience. Balancing automation with ethical considerations in exploitation techniques would also have been a significant concern.\n\nFUTURE POTENTIAL: Xploit could evolve into a comprehensive security platform, incorporating real-time threat intelligence and collaborative features for security teams. Expanding its capabilities to include educational resources and community-driven vulnerability databases could further enhance its value to users and the broader cybersecurity community.\n\nWhat it does: Xploit is an automated red-teaming suite that acts as a digital sparring partner for your AI workforce. Instead of manual testing, we deploy a coordinated team of adversarial agents that actively try to jailbreak your system, specifically testing if your agent can be tricked into misusing tools like executing unauthorized SQL or sending refunds. You can see exactly how your agent performs against our attacks in real time via a dynamic graph that highlights where the security holds and where it breaks.\n\nInspiration: It is late 2025, and while companies are deploying autonomous AI agents to handle real operations and money, nobody helped developers secure the actual applications. We realized that while big labs spent billions hardening the models, a social engineering trick could convince a deployed Sales Agent to authorize a 99% discount with no alarms going off. We built Xploit because static code scanners fail on dynamic AI behavior, and we needed a way to expose these invisible vulnerabilities before a malicious actor destroys business value.\n\nHow it was built: We architected Xploit as a FastAPI backend orchestrating multiple specialized AI agents powered by Pydantic AI, paired with a real-time React frontend that visualizes the attack as it unfolds. The backend runs four distinct attacker agents working in concert: a Strategist that selects high-level jailbreak approaches (social engineering, JSON injection, developer mode simulation), a Planner that breaks strategies into concrete steps, an Executor that crafts individual prompts to manipulate the victim, and an Analyst that evaluates responses and decides whether to continue, revise, or declare victory. These agents operate in a loop, persisting every node and message to SQLite via SQLModel so sessions survive restarts. We built two reference victims: RefundBot (a customer service agent with a\n\nChallenges: The biggest challenge was making the multi-agent attacker system actually converge on wins instead of spinning in circles. Early versions would pick a strategy, fail once, then give up or repeat the same broken approach. We solved this by giving the Analyst agent explicit instructions to return structured feedback when a step fails, then feeding that feedback back to the Planner so it could revise only the remaining steps instead of starting from scratch. We also added a replanning budget (three retries per strategy) to prevent infinite loops while still allowing adaptive behavior. Coordinating real-time updates between the backend and frontend without race conditions was brutal. Our first approach used manual WebSocket broadcasts scattered across the codebase, which led to dropped message\n\nAccomplishments: We built the entire system from scratch in just 18 hours.\n\nWhat we learned: We learned that even if the underlying model is safe, giving it access to tools like APIs and databases introduces massive vulnerabilities where context becomes the new attack vector. We discovered that system prompts looking secure on paper often fall apart when subjected to social engineering by another AI, proving that guardrails behave very differently in the wild. Ultimately, we realized that red-teaming cannot be a one-time event because as soon as prompts evolve, new security holes open up immediately.\n\nWhat's next: We aim to make Xploit the standard security certification for the Agentic web by integrating directly into CI/CD pipelines to run a security gauntlet every time a developer pushes a prompt change. We are also building custom attack personas so users can test against specific threats like Angry Customer or Competitor Spy to see how their bots handle stress. Finally, we plan to move beyond just breaking agents to automatically suggesting patches for the system prompts to fix the holes we find.",
    "hackathon": null,
    "prize": "Winner New Idea",
    "techStack": "fastapi, openrouter, pydantic, python, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=FA7hPeBVyMw",
    "demo": null,
    "team": "Dalu Okonkwo, Tony Okeke, Michael Moemeke",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/xploit"
  },
  {
    "title": "United we Stand!",
    "summary": "United We Stand! Project Summary\n\nUnited We Stand! is a web-based application designed to foster community engagement and collaboration among individuals and organizations. By providing a platform for users to share resources, skills, and support, the project aims to strengthen local communities and promote collective action.\n\nKEY ACHIEVEMENTS: This project won the hackathon and received the Wolfram Award for its innovative approach to community building. It stands out due to its intuitive user interface, effective use of technology to connect users, and the potential for real-world impact in enhancing social cohesion.\n\nBuilt using CSS, HTML, and JavaScript, the project showcases a responsive design that ensures accessibility across devices. The use of JavaScript for dynamic interactions enhances user experience, while CSS is leveraged to create an aesthetically pleasing layout that encourages user engagement.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user adoption and engagement, ensuring that the platform effectively meets the diverse needs of the community. Additionally, integrating features for real-time communication and collaboration might have posed technical difficulties, particularly in maintaining performance under high user loads.\n\nFUTURE POTENTIAL: The project could evolve by incorporating machine learning algorithms to personalize user experiences, facilitating better connections based on interests and skills. Furthermore, expanding the platform to include mobile applications and integrating features such as event planning or collaborative projects could enhance its utility and reach within communities.\n\nWhat it does: United We Stand! is a calming, minimalist website that teaches the basics of unity, provides a workplace unity checklist, gives uplifting quotes, and offers a ‚ÄúHow To‚Äù guide to help people build more supportive environments. It‚Äôs designed to gently inspire connection and teamwork.\n\nInspiration: I noticed how easily people get disconnected in the workplace, even though teamwork and understanding are the foundation of unity. I wanted to create something simple that reminds people that unity starts with everyday interactions around us. We have to maintain unity and peace starting with ourself and people around us. This lays the foundation for global unity and peace.\n\nChallenges: My main objective was to keep the design minimal but still engaging. I also worked on making the layout consistent across the pages and ensuring everything felt calming and user-friendly.\n\nAccomplishments: I‚Äôm proud of creating a simple tool that encourages people to reflect on unity and build healthier workplace relationships. I also maintained clean, organized, and scalable code throughout the project, making it easy to expand in the future.\n\nWhat we learned: I strengthened my frontend development skills and learned the value of designing with intention rather than complexity. This project reminded me that even small digital tools can create emotional impact when built thoughtfully.\n\nWhat's next: I plan to expand the project with features such as: A personal unity progress tracker\nAdditional educational modules\nTeam-building activity suggestions\nA registration system for organizations to monitor compassion, unity, and inclusion within their workplace",
    "hackathon": null,
    "prize": "Winner Wolfram Award Winners",
    "techStack": "css, html, javascript",
    "github": "https://github.com/Jyothsna-L/Hack4Unity-Hackathon-Project",
    "youtube": "https://www.youtube.com/watch?v=1isOdWZJwZQ",
    "demo": "https://jyothsna-l.github.io/Hack4Unity-Hackathon-Project/",
    "team": "Jyothsna Lakshminarayanan",
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/united-we-stand"
  },
  {
    "title": "Vril",
    "summary": "Vril Hackathon Project Summary\n\nVril is an innovative application that leverages the Gemini API to create a dynamic and interactive user experience, integrating advanced web technologies to facilitate real-time data visualization and user engagement. While the specific functionalities are not detailed, the project likely focuses on enhancing user interaction through a seamless interface developed with modern frameworks.\n\nKEY ACHIEVEMENTS: Vril distinguished itself by winning multiple awards, including First Place Overall and the Best Use of Gemini API from MLH. This recognition suggests that the project effectively integrated the Gemini API in a unique and impactful way, showcasing creativity and technical prowess that resonated with judges.\n\nThe project utilizes a robust tech stack, including FastAPI for backend operations, React.js and Next.js for a responsive front-end experience, and Three.js for 3D graphics. The incorporation of Docker enhances deployment efficiency, while Redis likely optimizes data handling and real-time updates, showcasing a well-architected solution.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges such as ensuring seamless integration of multiple technologies, managing real-time data flows with Redis, and creating an intuitive user interface that effectively utilized the Gemini API. Additionally, debugging and optimizing performance across diverse platforms could have posed significant hurdles.\n\nFUTURE POTENTIAL: Vril has significant potential for evolution by expanding its features to include more interactive data visualizations or integrations with additional APIs. The project could also explore user customization options, scalability for larger datasets, and enhancing its application in various domains such as education, finance, or gaming, providing numerous pathways for growth and development.\n\nWhat it does: Vril lets users create and edit full 3D product models and packaging by describing what they want in plain English or uploading a reference. It also includes an editor where users can iterate the shape, materials, and packaging with AI.\n\nInspiration: People have ideas for physical products but lack the 3D modeling skills or time to design them. Even simple items like mugs or boxes require specialized tools, long iteration cycles, and expensive designers. We wanted to make product design something anyone could do. Like how Lovable makes coding accessible for non-technical people.\n\nHow it was built: Generates editable 3D models from text or reference images.\nIncludes a prompt-driven editor similar to Cursor for adjusting geometry and materials.\nGenerates packaging using AI, including textures, dielines, and artwork that can be edited panel by panel. \nSupports exporting the product and packaging files in standard formats.\n\nChallenges: Generating 3D models that are editable, not just static meshes.\nBuilding consistent UV maps for textures and packaging artwork.\nAllowing users to target specific panels or surfaces for localized edits.\nKeeping the editing workflow simple while still giving users control.\n\nAccomplishments: We built an end-to-end workflow from text prompt to full 3D product and packaging export.\nWe enabled non-designers to iterate on shape and artwork without needing modeling skills.\nWe solved panel-level editing for packaging, which is usually difficult for AI tools.\n\nWhat we learned: People want to iterate quickly, so the editing loop must be fast and reversible.\nGood UV layouts and dielines matter a lot for packaging quality.\nNatural language is enough for most users to describe designs if the system interprets it correctly.\n\nWhat's next: Adding physics-aware models for more accurate product dimensions.\nSupporting multi-material and multi-part products.\nIntegrating with manufacturing partners so users can order prototypes directly.\nExpanding the editor with finer control for advanced users while keeping the core workflow simple.",
    "hackathon": null,
    "prize": "Winner First Place Overall; Winner [MLH] Best Use of Gemini API",
    "techStack": "css3, docker, fastapi, gemini, html5, next.js, python, react.js, redis, three.js, trellis, typescript, uvicorn",
    "github": "https://github.com/akuwuh/Vril",
    "youtube": "https://www.youtube.com/watch?v=pGwVC0TzbDQ",
    "demo": null,
    "team": "Isaac Nguyen",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/vril"
  },
  {
    "title": "Custos (6-9 Category)",
    "summary": "Custos is a project designed to enhance user security and privacy while browsing the internet. It leverages advanced web technologies to provide users with a seamless experience in managing their online data and protecting their digital identity.\n\nKEY ACHIEVEMENTS\nCustos stands out due to its innovative approach to user data protection, integrating real-time monitoring and alerts for potential privacy breaches. Its user-friendly interface and effective functionality likely contributed to its success in the hackathon, showcasing a strong commitment to user empowerment in online security.\n\nThe project is built using Chrome extensions, Gemini for backend services, HTML for structure, and JavaScript for interactivity. Notable implementations may include the integration of real-time data encryption, user activity monitoring, and a responsive design that adapts to various devices, enhancing overall user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring compatibility across different browsers and devices, maintaining performance while implementing security features, and navigating the complexities of user privacy laws and regulations. Additionally, user adoption and education on the importance of online security could have posed hurdles.\n\nFUTURE POTENTIAL\nCustos has significant potential for evolution, including expanding its features to incorporate AI-driven insights for users, enhancing its capabilities to cover emerging threats, and potentially integrating with other platforms or services for a comprehensive security solution. Additionally, building a community around user feedback can help refine its offerings and increase its impact.\n\nWhat it does: Custos scrapes web pages for likely policy links and inline policy sections, extracts visible text, and sends that text to an analysis endpoint. Custos then returns flagged clauses which are shown in a popup. Users can run automatic scans on newly visited sites or trigger a manual scan from the extension popup.\n\nInspiration: We noticed people skip reading long Terms of Service and Privacy Policies, yet these documents often contain important clauses affecting privacy and liability. The idea for Custos came from wanting a tool that helps everyday users stay informed without having to read through extremely long documents.\n\nHow it was built: We used a Chrome extension architecture with Manifest V3. We used Chrome API Documentation to learn and create Custos. We used Perplexity AI to make the basic HTML structure of all HTML pages. We made an API endpoint and an extension script. The API endpoint uses Gemini API to analyze and flag suspicious clauses.\n\nChallenges: Many sites have CORS headers that prevent text scraping. We were still not able to fix this. Notes: We used Perplexity API to make the basic structure of our HTML documents.",
    "hackathon": null,
    "prize": null,
    "techStack": "chrome, gemini, html, javascript",
    "github": "https://github.com/Secrethack0/custos-web",
    "youtube": "https://www.youtube.com/watch?v=k1Q4QfafZpE",
    "demo": null,
    "team": "Devang Upadhyay, V0rt3X, Secrethack Ganapathiraju, Atharva N",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/custos-fnuirj"
  },
  {
    "title": "Voxel World",
    "summary": "Voxel World is an innovative project that utilizes the Godot game engine to create a 3D voxel-based environment, potentially integrating elements like artificial intelligence through Python and YOLO (You Only Look Once) for object detection. The project aims to provide an immersive and interactive experience, showcasing the capabilities of voxel graphics and real-time object recognition.\n\nKEY ACHIEVEMENTS\nVoxel World stood out by effectively combining advanced technologies like YOLO for real-time object detection within a voxel framework, enhancing user engagement and interactivity. Its recognition as a winner at the hackathon, including the Premi Escola Polit√®cnica Superior, signifies its innovative approach and technical execution, which resonated well with judges.\n\nNotable technical implementations include the seamless integration of the Godot game engine with Python scripts for dynamic gameplay elements and the application of YOLO for efficient object detection and recognition in real-time. This allows for interactive gameplay where users can interact with various elements in the voxel world, enhancing the overall experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance, particularly in balancing the rendering of complex voxel graphics with real-time object detection. Additionally, ensuring smooth user interaction and maintaining a low latency experience could have posed significant technical hurdles during development.\n\nFUTURE POTENTIAL\nVoxel World has the potential to evolve into a comprehensive platform for educational purposes, game development, or virtual simulations. Future iterations could incorporate multiplayer capabilities, enhanced AI features, and expanded environments, making it a versatile tool for various applications in gaming and interactive storytelling.\n\nWhat it does: It mimics a persons movement, and reproduces in Godot as a 3D model.\n\nInspiration: We were inspired by hologrames and videogames.\n\nHow it was built: We built by using python, YOLOv8-seg and Godot.\n\nChallenges: Listed in a document we've made. (https://docs.google.com/document/d/18xMdQGUwago8FXt2Niuzot_-8lCSQm8erzVcN_oBrGY/edit?usp=sharing)\n\nAccomplishments: We're proud of the result, design changes and the optimizations we've made along the way.\n\nWhat we learned: How to send data via UDP, how to use the basics of YOLO, how to use Godot and how to 3D render.\n\nWhat's next: Up next, we're going to pitch this project in front of the judges!",
    "hackathon": null,
    "prize": "Winner Premi Escola Polit√®cnica Superior",
    "techStack": "godot, python, yolo",
    "github": "https://github.com/FadaBoop/H-LleidaHack-2025-InGroup",
    "youtube": "https://www.youtube.com/watch?v=zYZzLieB45U",
    "demo": null,
    "team": "FadaBoop Madomilov, facthhor",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/voxel-world-jt9ih6"
  },
  {
    "title": "Scholarly",
    "summary": "Scholarly Project Summary\n\nScholarly is an innovative platform designed to enhance academic research and collaboration. It leverages advanced AI capabilities to streamline the process of finding, organizing, and sharing scholarly resources, making it easier for researchers and students to connect and collaborate on academic projects.\n\nKEY ACHIEVEMENTS: The project stood out by winning the 1st Place at the Agentiiv Scholarship Challenge, showcasing its unique approach to addressing common challenges in scholarly research. Its ability to effectively integrate AI for resource management and collaboration likely contributed to its recognition as a top solution among competitors.\n\nScholarly was built using Claude for AI functionalities, Next.js for server-side rendering and performance optimization, and Tailwind CSS for responsive, customizable styling. This tech stack allows for a seamless user experience and efficient data handling, enabling real-time collaboration and resource sharing.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data integration from various academic sources, ensuring the accuracy and relevance of AI-generated suggestions. Additionally, fostering user adoption and maintaining a user-friendly interface while implementing complex features could have posed significant hurdles.\n\nFUTURE POTENTIAL: Scholarly has the potential to evolve into a comprehensive research ecosystem by incorporating features such as citation management, peer review systems, and integration with existing academic databases. Expanding its AI capabilities to offer personalized research recommendations could further enhance its value to users, fostering a more collaborative academic environment.\n\nInspiration: Every year, billions of dollars in scholarships go unclaimed - not because students are unqualified, but because the application process is confusing, exhausting, and overwhelmingly generic. Students are told:\n‚ÄúWrite a compelling essay.‚Äù\nBut they‚Äôre never told what makes an essay compelling for a specific scholarship. A merit scholarship wants academic rigor. A service scholarship wants community impact. A leadership scholarship wants initiative. Yet students write one essay and send it to everything. We asked ourselves:\n‚ÄúWhat if AI could read a scholarship the way a human judge does? What if it could uncover what the scholarship truly values - and then help students tell the right story for the right audience?‚Äù That insight became the foundation of Scholarly - the AI-powered system that h\n\nHow it was built: We built ScholarAI as a full-stack platform:\n\nWhat's next: LLMs are incredible at pattern recognition, but only when given structure.\nExplainability matters. Students trust AI more when they understand why it writes what it writes.\nModularity beats mega-prompts when building reliable AI systems.\nUX is everything - especially for stressed students applying to scholarships.",
    "hackathon": null,
    "prize": "Winner 1st Place: Agentiiv Scholarship Challenge",
    "techStack": "claude, nextjs, tailwind",
    "github": "https://github.com/KlausMikhaelson/hack-the-scholarship",
    "youtube": "https://www.youtube.com/watch?v=thurtJj5uuI",
    "demo": "https://hack-the-scholarship-frontend.vercel.app/",
    "team": "Ryan Ning, Sophie Yuen, Satyam Singh, SophieYuen0924",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/scholarly-gayw31"
  },
  {
    "title": "Collaboard",
    "summary": "Collaboard Project Summary\n\nCollaboard is a collaborative platform designed to facilitate teamwork and project management by enabling users to share ideas, documents, and feedback in real-time. It aims to enhance productivity and streamline communication among team members, making remote collaboration more efficient and effective.\n\nKEY ACHIEVEMENTS: The project stands out due to its innovative approach to collaboration, integrating intuitive user interfaces and real-time updates, which resonate well with the needs of modern teams. Winning both the MadHacks Third Place and overall winner indicates recognition from judges for its practicality, user experience, and potential impact on collaborative work environments.\n\nBuilt using Next.js, Node.js, and TypeScript, Collaboard leverages Next.js for server-side rendering and performance optimization, while Node.js facilitates seamless backend operations. The use of TypeScript enhances code quality and maintainability, allowing for better developer collaboration and fewer runtime errors.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring real-time synchronization across multiple users, managing scalability to support growing user bases, and maintaining user data security and privacy. Additionally, integrating various features while keeping the user interface intuitive may have posed design and technical hurdles.\n\nFUTURE POTENTIAL: Collaboard could evolve by incorporating advanced features such as AI-driven insights for project management, enhanced integrations with other productivity tools, and mobile app development to cater to users on-the-go. Expanding its user base through marketing efforts and exploring partnerships with educational institutions or businesses could also provide growth opportunities.\n\nWhat it does: Collaboard is an AI study partner that lives inside your handwritten iPad notes. It watches your problem-solving flow and steps in at the right moments - adding the first line of an integral in natural handwriting, sketching a diagram, structuring a table, or nudging you in the right direction with a hint - so you stay in the zone without switching tools or thinking about context. What makes Collaboard different is its fusion of cutting-edge LLM reasoning with generative image models. Because it can both think and draw, it produces adaptive visuals: diagrams, graphs, tables, labeled sketches, that match your notes in real time. Applying generative imaging to academics expands what‚Äôs possible far beyond calculator-style helpers: turning rough sketches into clean circuits, free-body diagrams\n\nInspiration: As college students, we constantly switch between mediums - solving problems in handwritten iPad notes, then jumping into AI tools like ChatGPT for guidance. That back-and-forth breaks flow and loses context. With the recent release of Gemini 3.0 Pro and Nano Banana Pro, we wondered: what if your notes themselves could become a virtual, AI-powered study partner?\n\nHow it was built: We used Next.js for the web framework, Supabase for the database, TailwindCSS + shadcn/ui for components, and AI SDK + OpenRouter to access AI models. For the canvas, we chose tldraw for its modularity. We validated key capabilities early, by testing Gemini 3.0 Pro and Nano Banana Pro via OpenRouter, then locked scope and built toward a focused MVP.\n\nChallenges: After implementing AI ‚Äúautocomplete,‚Äù we added a voice agent so you could converse with your notes - request edits, feedback, and see live updates. That ambition cost time we could have spent on debugging and refining our pitch. Balancing scope against new features was a real challenge.\n\nAccomplishments: Even if current model costs make consumer rollout tough, the experience feels magical: collaborating with AI directly in your handwritten space. It‚Äôs far more intuitive and engaging than bouncing between separate tools and formats.\n\nWhat we learned: Define scope early, and leave real time for debugging.\nUnderestimate timelines. Things always take longer than you think.\nFilter out flashy features and focus on the core, differentiating value.\n\nWhat's next: We‚Äôll continue refining, prototyping, and exploring cost-efficient pathways. The interaction model shows real promise for students and anyone who learns or thinks in a visually format. We believe we‚Äôve tapped into something really special, and we‚Äôre excited for people to try it.",
    "hackathon": null,
    "prize": "Winner MadHacks Third Place",
    "techStack": "next.js, node.js, typescript",
    "github": "https://github.com/ReedG20/madhacks",
    "youtube": "https://www.youtube.com/watch?v=D0-QV8mT8hM",
    "demo": null,
    "team": "Reed Grenager, Ife Solarin",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/collaboard-8ze9tf"
  },
  {
    "title": "SecureVista",
    "summary": "SecureVista Project Summary\n\nSecureVista is a security-oriented application that leverages advanced computer vision and face recognition technologies to enhance surveillance and user safety. By integrating real-time tracking and notification systems, it aims to provide proactive security measures for various environments.\n\nKEY ACHIEVEMENTS: The project stood out for its innovative integration of multiple technologies to create a comprehensive security solution. Winning the hackathon demonstrates its effectiveness and potential impact, showcasing real-time tracking capabilities and user-friendly communication features that enhance situational awareness.\n\nSecureVista utilizes a combination of powerful libraries such as dlib for face recognition, OpenCV for image processing, and FastAPI for a robust backend. The use of YOLOv8 for object detection ensures high accuracy in tracking, while Twilio and WebSockets facilitate real-time notifications, enhancing user engagement and responsiveness.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to ensuring high accuracy in face recognition under varying lighting and environmental conditions. Furthermore, managing real-time data processing and maintaining system performance while scaling the application could have posed significant technical hurdles.\n\nFUTURE POTENTIAL: SecureVista has the potential to evolve into a comprehensive security platform by incorporating features such as AI-driven anomaly detection, integration with smart home devices, and enhanced user analytics. Furthermore, expanding its application to different sectors, like retail or public safety, could broaden its market reach and impact.\n\nWhat it does: SecureVista is an autonomous surveillance pipeline that transforms standard IP cameras into intelligent guardians. Automated Threat Detection: Instantly flags weapons, unauthorized entry, and aggressive behavior.\nBehavioral Analytics: Detects loitering in restricted zones and analyzes shadow/motion patterns to reduce false positives.\nHealth & Safety: Uses pose estimation to detect accidental falls (e.g., for elderly care or hospitals) and triggers immediate medical alerts.\nReal-Time Dashboard: A low-latency React dashboard that streams alerts and evidence snapshots to security personnel via Twilio SMS/WhatsApp.\n\nInspiration: We identified a critical flaw in modern security infrastructure: the Human Latency Gap. Traditional CCTV is passive, and data suggests that operators miss up to 95% of security events after just 20 minutes of continuous monitoring due to fatigue. We viewed this not as a security problem, but as a data throughput and latency problem. Our goal with SecureVista was to transition from reactive forensics to deterministic real-time prevention, minimizing the time delta between event and response (Œît) to near zero.\n\nHow it was built: We engineered a Distributed Edge-Inference Pipeline designed for high throughput.\n\nChallenges: The \"Flicker\" False Positive: Initial models would flag a threat for 1 frame due to lighting noise. We implemented a Temporal Consistency Algorithm that only triggers an alert if detection confidence exceeds a threshold œÑ for k consecutive frames.\nShadow Noise: In outdoor settings, moving shadows were often misclassified as intruders. We implemented a Shadow Analysis module (using cv2.createBackgroundSubtractorMOG2) to differentiate between solid objects and transient light artifacts.\nDependency Hell: Integrating MediaPipe (CPU-bound) with YOLOv8 (GPU-bound) caused resource contention. We solved this by containerizing the services and optimizing the thread allocation.\n\nAccomplishments: üèÜ Winner of CodeVeda: Secured top spot in the 48hr Hackathon organized by IIT Madras BS and Manipal University Jaipur.\nReal-Time Performance: Achieved <150ms end-to-end latency (Camera ‚Üí Server ‚Üí Dashboard).\nScalability: The system successfully runs 4 concurrent 1080p streams on a single node without significant frame drops.\nPrivacy-First: By processing at the edge, no raw video footage needs to be sent to the cloud‚Äîonly the metadata and alert snapshots are transmitted.\n\nWhat we learned: Data Drift: Models trained on well-lit datasets fail in low-light corridors. We learned the importance of Gamma Correction preprocessing to normalize input feeds.\nBottlenecks: We discovered that in high-FPS computer vision, the bottleneck is often not the GPU compute, but the CPU-bound video decoding and memory copying between RAM and VRAM.\n\nWhat's next: Action Recognition: Moving beyond bounding boxes to Video Vision Transformers (ViViT) to detect complex interactions like fights.\nFederated Learning: Implementing a decentralized training loop where edge nodes update the global model weights without sharing privacy-sensitive video data.\nBlockchain Identity: Integrating a blockchain-based identity management system for tamper-proof access logs.",
    "hackathon": null,
    "prize": null,
    "techStack": "centroidtracker, dlib, face-recognition, fastapi, flask, mediapipe, numpy, opencv, pillow, python, pywhatkit, smtp, twilio, websockets, yolov8",
    "github": "https://github.com/Aashu-11/AI-Campus-CCTV-System/tree/main",
    "youtube": "https://www.youtube.com/watch?v=9wF1_5OCdTk",
    "demo": "https://drive.google.com/drive/folders/1TBLpqiNNV515c3SwUC1AZMC6FI7SQXyW?usp=sharing",
    "team": "Akshat Patil, Aayush Kolte",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/securevista"
  },
  {
    "title": "StablePay",
    "summary": "StablePay is a financial application that likely leverages blockchain technology to facilitate stablecoin transactions, ensuring fast and secure payments. The project aims to enhance user experience in digital payments by providing a reliable platform for transferring value without the volatility typically associated with cryptocurrencies.\n\nKEY ACHIEVEMENTS\nStablePay's standout features include its successful integration of multiple technologies, such as Solana for high-speed transactions and FastAPI for efficient backend services. Winning the Capital One Prize indicates its strong potential in the fintech space, showcasing innovation in payment solutions and possibly addressing issues like transaction speed and cost-effectiveness.\n\n- Expo.io: Utilized for creating a cross-platform mobile application, ensuring a seamless user experience on both iOS and Android.\n- FastAPI: Chosen for its performance and ease of use, enabling rapid development of the backend APIs.\n- Solana: Leveraged for its high throughput and low transaction fees, making it suitable for real-time payment processing.\n- React-Native: Implemented for building a user-friendly interface, enhancing the overall usability of the application.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in integrating multiple technologies while ensuring optimal performance and security. Additionally, navigating regulatory compliance in the fintech space and managing user trust in cryptocurrency-based transactions may have posed hurdles during development.\n\nFUTURE POTENTIAL\nStablePay has the potential to evolve into a comprehensive payment ecosystem by incorporating features such as multi-currency support, loyalty rewards, or integration with traditional banking systems. Expanding its user base and partnerships within the fintech industry could also enhance its market reach and functionality. Further developments may include advanced security measures and features aimed at enhancing user privacy and transaction transparency.\n\nWhat it does: The goal of StablePay is to empower people in volatile economies to use their stablecoin savings for real-world expenses without losing money to bad exchange rates, hidden fees, or confusing conversion steps. The app connects to a Solana wallet to read stablecoin balances like USDC, and when a user wants to convert or send funds, we route the transaction through our optimization engine. This engine traverses a graph of exchanges and networks, each edge representing a conversion cost, and computes the cheapest possible path from the user‚Äôs stablecoin to their target local currency. By leveraging intermediate assets like BTC, ETH, or SOL only when advantageous, StablePay automatically executes the optimized sequence of swaps across chains, ensuring users get the highest value for every conve\n\nInspiration: We were inspired to build StablePay after learning how many families in countries with volatile currencies turned to stablecoins as their lifeline to protect against hyperinflation. However, we found that they often had to pay expenses like rent or taxes in their local currency, forcing them to go through a complicated process involving high fees, several exchanges and many different wallets to convert their stablecoin to their local currency.\n\nHow it was built: We built StablePay as a full-stack system combining a smooth mobile experience with a high-performance backend optimizer. The frontend is a React Native Expo application that connects directly to a user‚Äôs Solana wallet, displays real-time stablecoin balances, and provides a clean interface for viewing transactions, initiating conversions, and executing transfers across chains. On the backend, we designed a FastAPI service in Python that performs the heavy lifting. We modeled the currency ecosystem as a weighted graph where nodes represent assets (fiat or crypto) and edges represent real-time exchange fees pulled from P2P, Swapzone, and market price APIs. To minimize conversion loss, we implemented a custom traversal using Dijkstra‚Äôs algorithm in log-space, allowing us to find the optimal m\n\nChallenges: One of the first major challenges we faced was the lack of a reliable, unified API for fiat-to-crypto and crypto-to-crypto conversions. Many providers only support crypto-to-crypto pairs, while fiat endpoints often require paid enterprise tiers or permission from the sales team. This severely restricted our ability to gather real-time conversion routes and fee structures. So we shifted to peer-to-peer (P2P) exchange data, which gave us a more flexible and globally accessible source of fiat on-ramps/off-ramps. This required refactoring our conversion graph to support user-driven markets instead of centralized liquidity. Midway through the project, we discovered that most providers don‚Äôt expose transparent transaction fees. Instead, they embed these fees inside the quoted conversion amount.\n\nAccomplishments: One of our biggest technical accomplishments is the conversion optimizer, which is a graph-based system that analyzes multiple conversion paths to determine the cheapest possible route between currencies. We used real-time fee data from P2P and Swapzone, transformed percentage-based trading fees into log-space weights, and implemented Dijkstra‚Äôs algorithm to find optimal multi-hop conversions. This approach lets us discover routes that aren‚Äôt obvious (like USDE to SOL to ARS) when those paths result in lower net fees than direct markets. The optimizer abstracts away all the complexity of cross-exchange pricing, cross-network pricing, and fee structures, giving users a simple, actionable output, which is the path that preserves the most value. Building this engine required algorithmic think\n\nWhat we learned: We learned about the difficulties faced when trying to find exchange rates for different currencies depending on the exchanging method whether that be an aggregate crypto to crypto exchange, or a p2p crypto to fiat exchange. We had to use multiple apis to find the exchange rates.\n\nWhat's next: Looking ahead, we plan to expand StablePay into a fully autonomous financial assistant for users in volatile economies. The next major step is integrating on-chain execution across multiple networks, allowing our optimizer not just to compute the cheapest route, but to actually perform all swaps atomically through smart contracts or cross-chain protocols. We also aim to incorporate MoonPay and additional fiat on/off-ramp providers, enabling users to move seamlessly between traditional currencies and stablecoins regardless of their region. We‚Äôre also exploring features such as risk-aware routing, where users can choose between the absolute cheapest path or the safest one based on exchange reliability and liquidity. Long-term, we hope to introduce financial intelligence tools like budgeting",
    "hackathon": null,
    "prize": "Winner Capital One Prize",
    "techStack": "expo.io, fastapi, python, react-native, solana",
    "github": "https://github.com/devmenon23/stablepay-backend",
    "youtube": "https://www.youtube.com/watch?v=ZlqqYAGr3aI",
    "demo": null,
    "team": "Daniel Yang, Michael Nwaigwe, Dev Menon",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/stablepay-5l8jeu"
  },
  {
    "title": "FlowBoard",
    "summary": "FlowBoard Project Summary\n\nFlowBoard is an innovative platform designed to facilitate collaborative brainstorming and project management. It aims to enhance user interaction through a visually engaging interface that integrates real-time editing and organization of ideas, making it easier for teams to conceptualize and track their projects.\n\nKEY ACHIEVEMENTS:  \n   FlowBoard distinguished itself by winning both the overall second place and the top prize, showcasing its exceptional user experience and functionality. The project effectively addressed common pain points in collaboration, offering seamless integration of various tools, which resonated well with judges and participants alike.\n\nThe project leverages a modern tech stack, including React for dynamic user interfaces, TypeScript for type safety, and Redis for efficient data management. The use of indexedDB ensures that data can be stored client-side for offline access, while Tailwind CSS enhances the visual appeal and responsiveness of the application. The incorporation of tldraw allows for intuitive drawing and diagramming capabilities, enriching the collaborative experience.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges related to ensuring real-time synchronization among multiple users, which is critical for collaborative tools. Additionally, optimizing performance for complex drawings and maintaining a user-friendly interface while managing a rich feature set could have posed significant hurdles.\n\nFUTURE POTENTIAL:  \n   FlowBoard has substantial growth potential by incorporating AI-driven features for idea generation and organization. Future iterations could integrate additional collaboration tools, such as video conferencing and task management, and expand into mobile applications to reach a broader audience. Scaling the platform to accommodate larger teams and integrating with existing project management software could further enhance its utility.\n\nWhat it does: FlowBoard is an AI-powered storyboard tool that turns sketches and text prompts into context-aware connected video clips that extend forever. Users can: Draw directly on 16:9 frames on an interactive canvas\nAdd written prompts  to generate video clips from each frame\nConnect frames with arrows to build a sequence\nEnhance frame drawings with AI\nMerge multiple clips into one final output It turns rough visual ideas into coherent videos of any length in minutes.\n\nInspiration: Traditional animation is slow. You have to redraw every frame and refine motion by hand. We built FlowBoard to automate that process. You sketch once, refine your image with AI, write a quick prompt, and AI generates the moving sequence for you.\n\nHow it was built: Frontend: React + TypeScript\nTldraw 3.15 for the interactive canvas and custom frame shapes\nTailwind CSS 4.1 and Radix UI for styling\nVite for development and builds\nIndexedDB storage Backend: Python (BlackSheep async web framework)\nGemini AI\n\nVeo 3.1 for video generation\nNano Banana Pro Flash for frame enhancement\n\nGoogle Cloud Storage for file output\nRedis for job queue and caching\nUvicorn ASGI server\n\nChallenges: Enhancing images and creating accurate videos based on user's description \nMaking sure context is saved and preserved across scenes\nManaging deployment architecture\n\nAccomplishments: Seamless end-to-end AI video generation\nA polished UI that feels like a real storyboard editor\nA fully functional drawing canvas with custom shapes\nVideo thumbnails displayed directly on connecting arrows\nSmooth merging of multiple generated clips into a final render\n\nWhat we learned: How to manage long-running AI operations using Redis and polling\nHow to orchestrate a full pipeline between drawing ‚Üí image ‚Üí video\nThe importance of UX features like loading states, blur overlays, and thumbnail previews\n\nWhat's next: Making UI/UX more smooth to deploy and scale",
    "hackathon": null,
    "prize": "Winner Second Place Overall",
    "techStack": "gemini, indexeddb, python, react, redis, tailwind-css, tldraw, typescript, vite",
    "github": "https://github.com/austinjiann/FlowBoard",
    "youtube": "https://www.youtube.com/watch?v=HGY6VDGjD7U",
    "demo": null,
    "team": "i made the board flow, I flowed on the board",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/flowboard-bdpqzg"
  },
  {
    "title": "Unsilenced",
    "summary": "Unsilenced is a project designed to enhance audio interaction by leveraging facial recognition and voice analysis technologies. It aims to create a more immersive and responsive audio experience, allowing users to engage with content through their facial expressions and vocal inputs.\n\nKEY ACHIEVEMENTS\nUnsilenced distinguished itself by winning both the overall hackathon and the Fish Audio Prize, indicating its innovative approach and effective use of audio technology. The successful integration of face recognition and audio processing likely provided a unique user experience that resonated with judges.\n\nThe project utilizes a diverse tech stack including face-api.js for facial recognition, fish-audio-api for audio processing, and a combination of Flask, Next.js, React, and Tailwind for front-end and back-end development. Notably, the use of MediaPipe for real-time face tracking and the implementation of TypeScript for improved code quality are significant technical strengths.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the accuracy and responsiveness of facial recognition in various lighting conditions and user environments. Additionally, ensuring seamless integration between the multiple frameworks and APIs used may have posed technical hurdles, particularly in maintaining performance and user experience.\n\nFUTURE POTENTIAL\nUnsilenced has the potential to evolve into a comprehensive platform for interactive audio experiences in various applications, such as gaming, virtual reality, and online education. Future enhancements could include broader language support, integration with smart home devices, and advanced machine learning algorithms for personalized user interactions.\n\nWhat it does: Unsilenced is a hands-free AAC communication tool that uses the nose as a pointer to type and detect facial emotions in real time with the help of machine learning. It combines your typed text with detected facial emotions to generate natural-sounding speech through Fish Audio's AI. Fish Audio contextualizes the sentence for tone and modulation. Through the Fish Audio API, users can clone their own voice, search from thousands of curated AI voices, or create custom voice profiles tailored to themselves.\n\nInspiration: Mainstream Augmentative and Alternative Communication (AAC) tools strip away the emotion and humanity from communication. This leads to robotic, monotone speech. A well-known case of this would be the AAC System connected to Dr. Stephen Hawking's famous chair. We wanted to help paralyzed and specially abled individuals not just communicate words, but express feelings in their assisted speech. This would help them reclaim the emotional connection that makes our conversations human.\n\nHow it was built: The frontend is built using NextJS with MediaPipe Face Mesh for UV mapping and nose tracking. The emotion detection system is handled by Face API, which is powered by machine learning, for real-time facial expression analysis. This is where our track comes in: the voiceover is constructed by Fish Audio API for expressive TTS with emotion modulation. The backend is made with a Flask proxy server for voice cloning, as to avoid CORS errors from the browser. We also used the Fish Audio API to let users upload or record their own voices for use in the tool.\n\nChallenges: About halfway through the project, we ran into a critical error. It was a CORS error that was caused by trying to access the Fish Audio API in a different domain without permission, and the error was being thrown by the browser for security reasons. To get past this, we created a proxy using Flask to act as a 'mediator' and the requests are passed to this proxy instead of directly to Fish Audio.\nAnother challenge we ran into was that the AI had to output the same voice but with different tones depending on emotion. We tried a variety of options in the Fish Audio API to get exactly what we wanted, but in the end, modulating the speed and volume of the speech via the prosody object depending on the emotion was what worked for us in the end.\nThe final major challenge we ran into was during th\n\nAccomplishments: We integrated a fully functional Computer Vision model that identifies emotions with ~84% accuracy, which is well above the average for even production-level models with long-term testing. Their average accuracy ranges from 75 to 80%. Both our model and the standard are measured on 6 classes and standard datasets.\nWe built a back-end pipeline quick enough to keep up with real-time human conversations. We even ran a test to demonstrate this by comparing it to Dr. Stephen Hawking's voice synthesizer.\nOur question to the model was \"Do you think we are alone in the Milky Way Galaxy as a civilization of our intelligence or higher?\" The total time between typing and getting the output was 3 minutes and 21 seconds. At TED, Chris Anderson asked Hawking the same question. Hawking‚Äôs answer (about 35\n\nWhat we learned: We learned to work around browser CORS errors by building a Flask proxy, which acts as a server-side bridge for API calls. We also learned that the best way to pass the emotions to the Fish Audio API for our use case is through a prosody object. \nCombining MediaPipe, Face API, and Fish Audio required careful timing and state management to sync nose tracking, emotion detection, and speech synthesis. Processing facial recognition entirely client-side maintained privacy and improved performance. We found that small UX details like progress bars that cap at 95% until completion build more user trust than indefinite spinners even if it was not possible to make a fully accurate system. We also learned that breaking problems into smaller steps helped us pivot quickly when major obstacles appeared\n\nWhat's next: We plan to continue developing Unsilenced. We will be adding support for multiple languages. This will continue utilizing Fish Audio API and its latest audio translation capabilities, and will make an impact on a much larger range of people around the world. It will ensure that people get access to our tool regardless of their ability to speak in English. We also plan to scale the tool into a full-blown application/product. Our first goal will be hitting 500 active users per month.",
    "hackathon": null,
    "prize": "Winner Fish Audio Prize",
    "techStack": "css, face-api.js, fish-audio-api, flask, javascript, mediapipe, next.js, python, react, tailwind, typescript",
    "github": "https://github.com/anishsrinivasa/MadHacksSubmission2025",
    "youtube": "https://www.youtube.com/watch?v=8i7SANGF0nw",
    "demo": "https://docs.google.com/presentation/d/1pQG4aq-pVrumjdYKpBIKiOlAyXEgv5dl/edit?usp=sharing&ouid=107763578233382591207&rtpof=true&sd=true",
    "team": "Aditya Harshavardhan, Anish Srinivasan, FayzaanV",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/unsilenced-oml4g5"
  },
  {
    "title": "GreenLens AI ‚Äì Dynamic ESG & Credit Intelligence",
    "summary": "GreenLens AI is an innovative project that integrates artificial intelligence to provide dynamic insights into Environmental, Social, and Governance (ESG) metrics as well as credit intelligence. The tool aims to assist businesses and investors in making informed decisions by analyzing and interpreting ESG data alongside credit risk assessments.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its unique focus on combining ESG metrics with credit intelligence, a relatively underserved area in financial technology. Its ability to provide real-time, actionable insights likely impressed judges, leading to its recognition as the winner of the hackathon.\n\nGreenLens AI was developed using Java, leveraging its robust libraries for data processing and machine learning. Notable implementations may include the use of natural language processing for analyzing ESG reports and automated algorithms for credit scoring, enabling real-time data integration and decision-making.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in sourcing reliable ESG data and ensuring its accuracy and relevance. Additionally, integrating diverse datasets and creating a user-friendly interface that effectively communicates complex analyses could have posed significant hurdles.\n\nFUTURE POTENTIAL\nGreenLens AI has the potential to evolve into a comprehensive platform that not only analyzes ESG and credit data but also predicts trends and provides strategic recommendations for businesses. Future iterations could incorporate broader datasets, machine learning models for predictive analytics, and user customization features to enhance its utility for various stakeholders in the financial ecosystem.",
    "hackathon": null,
    "prize": null,
    "techStack": "java",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=S1-MCvsTB6Y",
    "demo": "https://app-7qxo3tzzssu9.appmedo.com/portfolio",
    "team": "Tanush shah",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/greenlens-ai-dynamic-esg-credit-intelligence"
  },
  {
    "title": "AvaPass",
    "summary": "AvaPass Project Summary\n\nAvaPass is a blockchain-based platform designed to streamline event ticketing through the use of cryptocurrency and smart contracts. By leveraging decentralized technology, it aims to enhance transparency, reduce fraud, and simplify the ticket purchasing process for users.\n\nKEY ACHIEVEMENTS: The project stood out by effectively addressing common issues in traditional ticketing systems, such as ticket scalping and counterfeit tickets. Its innovative use of blockchain technology for secure transactions and ownership verification earned it recognition as a winner in the hackathon, showcasing its potential impact on the event industry.\n\nAvaPass was built using a combination of advanced technologies, including React for the front-end interface, Solidity for smart contract development on the Ethereum blockchain, and TypeScript for maintainable code. The integration of crypto payments directly into the ticketing process exemplifies a seamless user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user adoption, as transitioning users from traditional ticketing methods to a blockchain solution can be difficult. Additionally, ensuring the security of smart contracts and managing the complexities of cryptocurrency transactions would have required rigorous testing and validation.\n\nFUTURE POTENTIAL: AvaPass has significant growth potential by expanding its platform to include features such as dynamic pricing, NFT ticketing, and partnerships with event organizers. As blockchain technology gains traction, AvaPass could position itself as a leading solution in the evolving landscape of digital ticketing.\n\nWhat it does: AVAPASS gives users a minimum amount of gas tokens for any Avalanche subnet so they can immediately explore it.\nOur web app lets users: Connect any major crypto wallet\nView and jump into different subnets\nAccess dApps inside each subnet\nClaim a small ‚Äústarter fuel‚Äù amount of gas tokens to perform basic actions within those subnets\n\nInspiration: Avalanche has one of the most innovative architectures in crypto, subnets. Yet most users can‚Äôt even enter them. Every subnet runs on its own gas token, which means users get stuck before they can even try a new dApp or test a new network. This onboarding friction stops exploration, stops innovation, and stops new subnets from getting real users.\nWe wanted to fix that.\nOur inspiration was simple:\nWhat if discovering a new subnet was as easy as opening an app?\nNo bridges, no swaps, no friction, just instant access.\n\nHow it was built: We created multiple local Avalanche subnets to simulate real multi-subnet onboarding.\nIntegrated wallet connectivity supporting various wallet types, including Avalanche wallets.\nBuilt the UI/UX in a web app where users select a subnet, view its dApps, and receive their minimum gas allocation\nAll using React for the frontend, coupled with typescript, solidarity, and blockchain technologies in the backend.\n\nChallenges: Understanding the Avalanche ecosystem and exploring the L1 subnet architectures. \nIncorporating testnet chain tokens from Fuji for the purpose of experimenting. \nIntegrating the frontend to the elements of the backend such as the smart contracts, and communication from the hub to faucet contracts.\ndApp creations and wallet/gas token logic associated with it.\n\nAccomplishments: Teaching ourselves all the content in just a few days, going from absolute beginners to building a product that addresses a real onboarding issue in the Avalanche ecosystem.\nDeveloping a working prototype for a universal subnet onboarding platform that gives users the minimum gas needed to explore any subnet.\nIncorporating 4 different dApps with unique functionalities for users to experiment with using their newly acquired gas tokens.\nSuccessfully setting up and connecting multiple local subnets, integrating wallets, and creating a seamless user flow from wallet ‚Üí subnet ‚Üí dApp ‚Äî all within one weekend.\n\nWhat we learned: How Avalanche subnets function at a deeper technical level, including how gas tokens and local subnet environments work.\nThe challenges of onboarding users into multi-network ecosystems, especially when every subnet has its own token.\nHow to integrate and manage multiple wallets, networks, and dApps inside a single application.\nThe importance of simple UX in web3. Users shouldn‚Äôt need to understand complex token mechanics just to explore new technologies.\n\nWhat's next: Onboarding a wider variety of real Avalanche subnets into our platform for users to explore.\nPartnering with subnet creators to sponsor gas credits so new users can enter their subnet\nImprovements in UI for more seamless navigation across wallets, subnets, and dApps.",
    "hackathon": null,
    "prize": "Winner Crypto",
    "techStack": "blockchain, crypto, react, solidity, typescript",
    "github": "https://github.com/hasan1970/AvaPass-backend.git",
    "youtube": "https://www.youtube.com/watch?v=IDMTvUh7Gxo",
    "demo": "https://docs.google.com/presentation/d/1gytveG_GrsBhKUl0MmdhZlWgSMCdEio3NfuKYVddm8Y/edit?usp=sharing",
    "team": "Hasan Shameer Muhammed, Samaah Shameer Muhammed, Spoorthi Pyarilal",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/fskjdh"
  },
  {
    "title": "TrueBite",
    "summary": "TrueBite Project Summary\n\nTrueBite is an application designed to enhance the dining experience by leveraging the Yelp AI API for personalized restaurant recommendations. It likely aims to connect users with dining options that align with their tastes and preferences, making it easier to discover new places to eat.\n\nKEY ACHIEVEMENTS: TrueBite stood out due to its innovative use of AI to tailor recommendations based on user preferences, setting it apart from traditional restaurant review platforms. Winning first place indicates that the project effectively addressed user needs and provided a seamless and engaging user experience.\n\nThe application employs modern web technologies, including CSS3, HTML5, and JavaScript, to create a responsive and user-friendly interface. The integration of the Yelp AI API demonstrates advanced technical implementation, allowing for real-time data retrieval and personalized suggestions.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to API integration, such as managing response times and ensuring data accuracy. Additionally, designing an intuitive user interface that effectively communicates personalized recommendations could have posed difficulties.\n\nFUTURE POTENTIAL: TrueBite could evolve by incorporating user feedback mechanisms, expanding its database with more localized options, or even integrating social features that allow users to share their dining experiences. Future iterations could also explore partnerships with restaurants for exclusive offers or enhanced services.\n\nWhat it does: TrueBite is a Chrome extension that uses Yelp AI API to introduce trusted ratings, AI-generated review insights, and better restaurant alternatives directly within DoorDash, Uber Eats, and Grubhub. It analyzes Yelp data in real time, summarizes what people love or dislike, and allows users to chat and ask follow-up questions about restaurants.\n\nInspiration: Food delivery apps often display inflated ratings that don‚Äôt always reflect real customer experiences, forcing users to jump between platforms to verify restaurant quality. We wanted to remove that friction by bringing trusted Yelp insights directly into the ordering flow, helping people choose meals with confidence. TrueBite was inspired by the idea that better information leads to better food decisions.\n\nHow it was built: We built TrueBite as a Manifest V3 Chrome extension with content scripts that detect restaurant pages and extract key details like restaurant name and location. A secure background service worker communicates with the Yelp AI API, retrieves ratings and AI-generated insights, and sends structured results back to the UI. We inject a clean, non-intrusive interface into each platform and added an AI-powered chat layer so users can ask natural language questions about the results.\n\nChallenges: Each delivery platform has a different and frequently changing DOM structure, making restaurant and location detection unreliable at times. Matching delivery app listings with Yelp businesses also required careful fuzzy matching to avoid incorrect results.\n\nAccomplishments: We successfully integrated Yelp AI API as the exclusive data source, built a responsive UI that works across three major delivery platforms, and created an interactive chat experience that turns static reviews into meaningful insights. Most importantly, TrueBite makes it easier for users to trust their food choices in real time.\n\nWhat we learned: We learned to use Yelp AI API to extract data. We also gained hands-on experience handling real-world challenges like fuzzy matching, rate limits, and UX constraints inside third-party websites.\n\nWhat's next: Next, we plan to expand the chat experience with follow-up suggestions based on user preferences, add dietary and lifestyle filters, and introduce personalization over time. We also want to improve alternative recommendations using deeper Yelp AI API insights and explore accessibility and mobile-friendly extensions to reach more users.",
    "hackathon": null,
    "prize": "Winner First Place",
    "techStack": "css3, html5, javascript, yelp-ai-api",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=yaScBHrmiOk",
    "demo": "https://truebiteapp.netlify.app/",
    "team": "Private user",
    "date": "2025-12-15",
    "projectUrl": "https://devpost.com/software/truebite"
  },
  {
    "title": "exposed.tech",
    "summary": "Exposed.tech is an innovative platform that leverages advanced technologies to provide users with enhanced visibility and control over their digital environments. It integrates tools for data management and visualization, utilizing cutting-edge frameworks to deliver a seamless user experience across various devices.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its compelling startup potential, winning accolades for its innovative approach and practical applications in the tech landscape. Its ability to combine multiple technologies effectively while addressing a pressing need for better digital visibility contributed to its recognition as a winner at the hackathon.\n\nExposed.tech is built using a robust tech stack, including Cloudflare for enhanced security and performance, Gemini for artificial intelligence capabilities, and a combination of JavaScript and Python for backend and frontend development. The use of React Native allows for a responsive cross-platform mobile application, ensuring accessibility and a smooth user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating diverse technologies seamlessly and ensuring the platform's scalability to handle varying user demands. Additionally, addressing security concerns in data management and maintaining user privacy would have been critical hurdles.\n\nFUTURE POTENTIAL\nMoving forward, Exposed.tech has the potential to evolve into a comprehensive digital management tool, incorporating AI-driven analytics and additional features for automation. Expanding its user base and exploring partnerships with businesses looking to enhance their data visibility could further solidify its market position and drive growth.\n\nWhat it does: Exposed.tech is an interactive mobile app that analyzes food and drink labels using to provide instant, comprehensive health information. Label Scanning: Users upload an image of an ingredient label.\nChemical Breakdown: The system extracts and identifies individual chemical compounds (e.g., Caffeine, Biotin).\nHealth Analysis: It provides clear pros, cons, and flags specific \"Bad Combos\" (e.g., high sugar combined with certain artificial colors). All backed by peer-reviewed research studies.\nVisual Personalities: Using advanced Generative AI, each identified chemical is transformed into a unique creature based on its properties, giving the ingredient a memorable visual identity, which users can collect.\n\nInspiration: We were inspired by the universal challenge of reading nutrition labels. They're often complex, technical, and overwhelming, making it hard to truly understand what we're consuming. Our goal was to revolutionize this experience by making ingredient analysis transparent, accessible, and fun. We wanted to move beyond dry data and transform chemical compounds into engaging, visual personalities, empowering users to make genuinely informed health choices.\n\nHow it was built: We developed a full-stack solution integrating powerful AI models and robust programming frameworks. Frontend (React): Built a responsive user interface to handle image uploads and display the analysis dashboard.\n Backend (Python): This script orchestrates the entire process. It uses OCR (Optical Character Recognition) to convert the label image into text.\n Analysis Pipeline: The script then cleans the text, cross-references ingredients against a health database, and calculates risk factors. \n Connectivity: We used Cloudflare during development to securely tunnel the Python server,  allowing the React frontend to easily access the analysis API. We also incorporated workers controlled Cloudflare D1 SQLlite databases for our indexing and user authentication.\n\nChallenges: OCR Reliability: Text extraction from curved surfaces (like bottles) or labels with varying fonts presented challenges, requiring complex string manipulation to clean the resulting ingredient list.\nFull-Stack Integration: Setting up the initial communication between the React frontend and the Python API while managing restrictions proved to be a significant early hurdle.\n\nAccomplishments: Successful End-to-End Pipeline: We successfully built a working prototype that takes a raw photo and outputs a complex, informative, and visually stunning analysis in seconds.\nCreative AI Use: Demonstrating a novel, educational application of Generative AI by making intimidating scientific information relatable and fun.\nTechnical Integration: Seamlessly integrating multiple technologies‚ÄîReact, Python, OCR, and the Gemini API‚Äîinto one coherent, high-speed product.\n\nWhat we learned: We deepened our understanding of API management and full-stack deployment, particularly the practical difficulties of linking independent frontend and backend services in a real-world application. We learned how to handle OCRs and the ingredient data.\n\nWhat's next: Database Expansion: Expanding analysis beyond food and drink to include more dietary restrictions and other complex consumer products.",
    "hackathon": null,
    "prize": "Winner Hack with the Best Startup Potential by Morrissette",
    "techStack": "cloudflare, gemini, javascript, python, react-native, vision",
    "github": "https://github.com/PconnorsUWO/HW-12/tree/newCubed",
    "youtube": "https://www.youtube.com/watch?v=fB2axB4Ec0g",
    "demo": "https://exposed.tech/",
    "team": "Matthew Li, Patrick Cauilan Connors, Thierry Huot",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/exposed-tech"
  },
  {
    "title": "SafeSteps AI",
    "summary": "SafeSteps AI Project Summary\n\nSafeSteps AI is an innovative project aimed at enhancing pedestrian safety through the use of AI technology. By analyzing real-time data and environmental factors, the application provides users with safe walking routes, alerts about potential hazards, and personalized safety recommendations.\n\nKEY ACHIEVEMENTS: The project stood out by effectively integrating advanced machine learning algorithms with user-friendly interfaces, enabling real-time hazard detection and route optimization. Its success in the hackathon as both the winner and second-place winner demonstrates its robust functionality and the team's ability to address a critical issue in urban safety.\n\nSafeSteps AI utilizes a stack that includes CSS, HTML, and JavaScript for front-end development, providing a responsive and interactive user interface. The backend features Python and Jupyter for data analysis and machine learning, leveraging libraries like Pandas for data manipulation and Gemini API for real-time data integration.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring data accuracy and reliability, particularly in real-time hazard detection. Additionally, integrating various data sources and maintaining user privacy while delivering personalized recommendations would have required careful consideration.\n\nFUTURE POTENTIAL: SafeSteps AI has significant potential for evolution, including expanding its feature set to incorporate community reporting of hazards and partnerships with local authorities for enhanced data sharing. Future iterations could also explore the integration of augmented reality for navigation and further personalization through user feedback and behavioral data.\n\nHow it was built: Data Sources  \n\n10+ years of Toronto Police Public Safety Data (158 neighbourhoods)\nOpenStreetMap full topology (11,000+ intersections, 13,000+ street segments, POIs, lighting, etc.)\n\nGeospatial Processing (Python)  \n\nPoint-in-Polygon: every intersection inherits its neighbourhood‚Äôs crime rate\nFeature engineering:\n\nDensity of bars/clubs in 100 m radius\nStreet type & hierarchy\nIntersection complexity (number of connected roads)\n\nFinal risk score (0‚Äì100) per intersection using weighted formula\nStreet segments = average of their two endpoints\n\nGraph & Routing  \n\nBuilt weighted graph with NetworkX\nA* algorithm finds the safest path (not the shortest)\n\nWeb App  \n\nLeaflet.js dark-mode interactive map\nStreets colored green / yellow / red\nReal-time route calculation\nClick any street ‚Üí see exa\n\nChallenges: Merging neighbourhood-level crime data with street-level geometry (solved with spatial indexing)\nKeeping the app fast with 11,000+ nodes (vectorized calculations + caching)\nMaking sure the map informs without stigmatizing communities (transparent methodology + focus on infrastructure factors)",
    "hackathon": null,
    "prize": "Winner Second Place",
    "techStack": "css, geminiapi, html, javascript, jupyter, pandas, python",
    "github": "https://github.com/Solarcemir/SafeSteps_AI",
    "youtube": "https://youtu.be/M0kzqOYuxMA",
    "demo": null,
    "team": "Luis Hoi Fan, Honiya Maqsood, Hira Ahmad, Saksham Jain, Amy Zhang",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/sheridan-datathon"
  },
  {
    "title": "Uniwell",
    "summary": "Uniwell Project Summary\n\nUniwell is a web application designed to enhance the well-being of university students by providing resources for mental health, wellness activities, and peer support. The platform aims to foster a supportive community where students can easily access wellness resources tailored to their needs.\n\nKEY ACHIEVEMENTS:  \n   Uniwell stood out by effectively addressing a crucial need among students for mental health resources during a time when such support is increasingly vital. Its user-friendly interface and innovative features, such as peer support forums and wellness tracking tools, likely contributed to its recognition as a winner in the hackathon.\n\nBuilt with HTML and React, Uniwell showcases a modern web architecture that enables dynamic user interactions and a responsive design. The use of React likely allowed for efficient state management and component reusability, enhancing the overall user experience.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges such as ensuring data privacy and security for users discussing sensitive topics. Additionally, balancing the need for a rich feature set while maintaining performance and usability could have been another hurdle.\n\nFUTURE POTENTIAL:  \n   Uniwell could evolve into a comprehensive platform by integrating more advanced features like AI-driven personalized wellness recommendations, partnerships with mental health professionals for live support, and gamification elements to encourage user engagement. Expanding its reach to include alumni and faculty might also enhance its community and support network.\n\nWhat it does: UniWell offers five main features: daily check-ins that track a student‚Äôs mood throughout the week and provide a weekly summary; a habit tracker that promotes healthy routines to support mental well-being; an anonymous chat box that allows students to share their thoughts without hesitation; and a location-based feature that connects students to nearby support services if they need in-person assistance.\n\nInspiration: For some first-year students coming from high school, the transition involves not only a new school environment but also adapting to a new city or even a new country. It is normal to face challenges; however, not having timely access to support and resources can significantly affect their mental health and academic performance.\nThis is also our inspiration for creating UniWell - an application designed to support students experiencing difficulties by offering immediate access to relevant resources, daily well-being check-ins, and a secure, anonymous platform through which they can seek assistance.\n\nHow it was built: We built UniWell based on our own experiences as first-year students, using tools like Gemini to help generate, refine, and organize our code.\n\nChallenges: Initially, we were confused about how to use Gemini because it was a completely new tool for all of us. We weren‚Äôt familiar with its interface, its capabilities, or the best ways to prompt it effectively. At first, even simple tasks felt overwhelming because we weren‚Äôt sure which features to use or how to structure our inputs. However, as we continued experimenting, exploring its functions, and learning from trial and error, we gradually became more confident. Over time, what started out as uncertainty turned into a better understanding of how Gemini works and how we can use it to support our project more efficiently.\n\nAccomplishments: An accomplishment we are really proud of is how effectively we coordinated as a team to solve any challenges that came up. Whenever we faced an issue‚Äîbig or small‚Äîwe communicated openly, shared ideas, and supported each other until we found a solution. Our teamwork became one of our biggest strengths, and it helped us stay organized, motivated, and confident throughout the entire process. This collaboration not only made the work easier but also strengthened our trust and connection as a group.\n\nWhat we learned: We also learned how to integrate AI into our project-building process. At first, it felt unfamiliar, but as we experimented, we discovered how AI could support brainstorming, planning, problem-solving, and even refining our final product. Understanding how to use AI as a tool‚Äînot to replace our work but to enhance it‚Äîbecame a valuable skill. It helped us work more efficiently, think more creatively, and approach challenges with new perspectives.\n\nWhat's next: What‚Äôs next for Uniwell is continuing to grow and refine the app. This includes adding more features that support students‚Äô mental health and wellness, gathering user feedback to understand what can be improved, and increasing our marketing efforts so more university students can discover and benefit from the app. Our goal is to keep evolving Uniwell based on real user needs and ensure it becomes an even more helpful and accessible resource.",
    "hackathon": null,
    "prize": "Winner Prizes",
    "techStack": "html, react",
    "github": "https://github.com/heidicodes/uniwell-yayy",
    "youtube": "https://www.youtube.com/watch?v=Qyem5E7JaEg",
    "demo": null,
    "team": "Cleo Chuu, Shivali Kumar, tiir-dnn, a1shan1",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/uniwell-7uycs1"
  },
  {
    "title": "Echo",
    "summary": "Echo Project Summary\n\nEcho is an innovative application that leverages advanced speech recognition and audio processing technologies to facilitate real-time voice interactions. It integrates various APIs to enhance user experience and provide seamless functionality across platforms, including mobile and web.\n\nKEY ACHIEVEMENTS: Echo stood out by winning both the overall prize and the Best Beginner Hack award, indicating its exceptional design and execution. The project successfully demonstrated a unique combination of accessibility and user-friendly features, appealing to both novice and experienced developers.\n\nThe project utilizes a robust tech stack, including the Google Cloud Speech-to-Text API for accurate voice recognition, Eleven Labs for audio processing, and OpenAI for intelligent responses. Its cross-platform compatibility is achieved through technologies like React and React Native, ensuring a smooth user interface and experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as integrating multiple APIs seamlessly, ensuring accurate voice recognition in diverse environments, and optimizing performance across different devices. Additionally, managing dependencies and maintaining code quality in a rapid development environment might have posed difficulties.\n\nFUTURE POTENTIAL: Echo has significant potential for evolution by expanding its functionality to include more sophisticated AI-driven features, enhancing multilingual support, and integrating with additional smart devices. Future iterations could also focus on personalized user experiences and advanced analytics for user interactions, paving the way for commercial applications.\n\nWhat it does: Echo is a voice-first mobile app that helps seniors access essential support through natural interaction. It offers: Voice Interface: Talk naturally to navigate the app or ask for information, with text input available when needed.\nEmergency Access: One-tap 911 button with an extra confirmation step to prevent accidental calls.\nMedication Management: Users can say ‚Äúremind me to take aspirin at 8am,‚Äù and Echo automatically understands the request, schedules notifications, and suggests appropriate medication categories based on symptoms.\nVolunteer Matching: Connects seniors with volunteers for home repair, mobility assistance, grocery help, companionship, tech support, or medical transportation.\nHealth Services Locator: Uses Google Places API to find nearby hospitals, clinics, and urgent car\n\nInspiration: Older adults face two major access gaps: complex technology and scattered resources. Many apps rely on tiny text, multi-layered menus, and fast interfaces that unintentionally exclude seniors. At the same time, many older adults don‚Äôt know where to find community meals, events, health services, or volunteer help. We wanted to build something that makes technology feel human again. Echo came from the belief that technology should meet people where they are. For seniors, that means being able to simply speak and get the support they need.\n\nHow it was built: Frontend: React Native with Expo for iOS, Android, and Web Backend: Express.js with Google Cloud Speech-to-Text, OpenAI GPT-4o, Google Gemini, ElevenLabs Text-to-Speech, and Google Places API Core Systems:\nCross-platform audio abstraction, \nNatural language understanding and command routing, \nLocation services, \nPush notifications, \nMedication reminder parsing, \nAPI quota monitoring and fallback logic\n\nChallenges: Managing API credits under time pressure\n\nAccomplishments: Building a genuinely intuitive voice-first interface tailored for seniors\nIntegrating medication reminders, events, health services, and volunteer help into one seamless experience\nDesigning interfaces with large touch targets, clear hierarchy, and simple navigation\nCreating a smart routing system that understands user intent rather than rigid commands\nEnabling natural language medication reminders that work even with vague or incomplete phrasing\nDeveloping robust fallback logic to maintain reliability even when APIs fail\nBuilding trust-oriented features and planning for verified volunteers\nWorking effectively as a team, learning how to divide responsibilities, share workload, and communicate under a tight deadline\n\nWhat we learned: How to collaborate efficiently by splitting tasks, documenting decisions, and keeping a fast feedback loop\nHow to structure a project under time pressure with clear milestones and a functional workflow\nThat managing multiple APIs requires layered fallback strategies and continuous quota monitoring\nThat audio features require platform-specific handling even in cross-platform frameworks\nThat designing for seniors means simplifying decision paths, not just enlarging text\nThat flexible intent detection is crucial for natural voice interactions\nHow to communicate, adapt, and troubleshoot together as a team\n\nWhat's next: Offline Mode for essential reminders without internet\nMulti-language support\nCaregiver dashboard for medication adherence\nIntegration with healthcare systems\nExpanded volunteer network with scheduling and verification\nTelehealth consultation features\nUser-created events and community spaces\nFurther accessibility improvements, including larger text options and better screen reader support",
    "hackathon": null,
    "prize": "Winner Best Beginner Hack",
    "techStack": "elevenlabs, expo.io, express.js, gemini, google-cloud-speech-to-text-api, google-places, ios, javascript, node.js, openai, react, react-native, typescript, web-audio-api",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=t7K35F0epPk",
    "demo": "https://echowaves.health/",
    "team": "Vera Li, Wanru Huang, Peijun Chen",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/echo-g1o862"
  },
  {
    "title": "6ixAssist",
    "summary": "6ixAssist is an innovative application designed to enhance user experiences in urban environments by providing real-time assistance and information through an interactive interface. By integrating mapping services and user-friendly navigation, it aims to streamline tasks such as locating amenities, public transport, and local events.\n\nKEY ACHIEVEMENTS\nThe project stood out by successfully integrating multiple technologies to create a seamless user experience, which likely contributed to its recognition as a winner. Its ability to combine real-time data with an intuitive UI demonstrated exceptional functionality and user engagement, setting it apart from other hackathon entries.\n\n6ixAssist leverages cutting-edge technologies such as Gemini for advanced AI capabilities, Leaflet.js for dynamic mapping, React for responsive UI components, and Tailwind CSS for modern styling and layout. The use of TypeScript ensures type safety and enhances code maintainability, enabling efficient development.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating various APIs for real-time data retrieval, ensuring data accuracy, and maintaining performance across different devices. Additionally, optimizing the user interface for accessibility and usability within a limited timeframe may have posed significant hurdles.\n\nFUTURE POTENTIAL\nMoving forward, 6ixAssist could evolve by incorporating machine learning to personalize user experiences based on preferences and behaviors. Expanding its functionality to include community-driven features, such as user-generated content and feedback, could further enhance engagement and utility. Additionally, partnerships with local businesses and services could provide users with exclusive offers and promotions, creating a more comprehensive urban assistance platform.\n\nWhat it does: 6ixAssist helps users find nearby essential services, such as food banks, shelters and community resources. Users can type anything in their own words, and the app translates it into actionable search results, showing the closest support options on an interactive map. This allows users to find support fast.\n\nInspiration: We were inspired by the rising challenges of poverty and homelessness in Toronto. We wanted to build something practical, that could make a real, immediate difference for people who are struggling to find basic support when they need it.\n\nHow it was built: We used the Gemini API to power the searching and Open Street map API to surface the closest shelters, food banks, and essential services. Maps also handles geolocation, routing, and rendering everything directly on the interface. Tailwind CSS allowed us to build a clean, responsive UI quickly, while lightweight JavaScript ties the entire experience together.\n\nChallenges: Our biggest challenges were debugging API key issues, handling errors from both Gemini and Open Street Map, and designing logic that could bridge the gap between natural-language understanding and location-based search. Especially with the time-constraint, it was difficult to work through these issues. But through this, we built collaborative skills.\n\nAccomplishments: We‚Äôre proud that we delivered a complete, end-to-end project under tight time constraints, and built it in a way that can be expanded far beyond its current version. We‚Äôre also proud that our work isn‚Äôt just a technical demo, it‚Äôs a tool that can actually support people in Toronto who need quick access to essential services. Knowing that this project has real-world impact makes the achievement even more meaningful for us.\n\nWhat we learned: Throughout this project, we picked up skills that are essential for modern developers. We learned how to use AI tools like the Gemini API to interpret natural language, how to integrate those results with real-world data from Open Street Maps, and how to manage multiple API keys across environments. We also strengthened our UI/UX design skills by building a responsive interface with Tailwind CSS. These experiences taught us how to combine AI, geolocation, and frontend engineering into one cohesive product.\n\nWhat's next: We hope to keep expanding it by integrating official open-data sources, adding real-time availability for shelters, and supporting additional languages through Gemini‚Äôs multilingual capabilities. We also want to improve the user experience, add bookmarking features.",
    "hackathon": null,
    "prize": "Winner Prizes",
    "techStack": "gemini, leaflet.js, react, tailwind-css, typescript",
    "github": "https://github.com/LoriB14/ellehacks",
    "youtube": "https://www.youtube.com/watch?v=tCAIiEWrvhk",
    "demo": "https://ellehacks.vercel.app/",
    "team": "aizah sadiq, Lori Battouk",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/6ixassist"
  },
  {
    "title": "SustainaBite",
    "summary": "SustainaBite Project Summary\n\nSustainaBite is a platform designed to promote sustainable eating habits by providing users with personalized meal suggestions based on their dietary preferences and environmental impact. It aims to educate users on the benefits of sustainable food choices while making it easy to adopt a greener lifestyle.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative approach to merging sustainability with user-friendly technology, effectively engaging users in eco-conscious eating. Winning the hackathon indicates that it resonated with judges, likely due to its impactful concept and practical implementation that addresses a pressing global challenge.\n\nBuilt with React and TypeScript, SustainaBite benefits from a robust front-end framework that allows for a responsive and dynamic user interface. The use of TypeScript enhances code quality and maintainability, which is crucial for scalability. Notable implementations may include a recommendation algorithm that tailors meal suggestions to individual user profiles based on preferences and dietary restrictions.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the accuracy of the sustainability metrics used for meal suggestions, integrating diverse dietary data, and managing user engagement to maintain long-term interest in the application. Additionally, balancing user experience with a wealth of information on sustainability could have posed design challenges.\n\nFUTURE POTENTIAL: SustainaBite could evolve by incorporating a wider range of features, such as community-driven content, partnerships with local food suppliers, and gamification elements to encourage user participation. There is also potential for expansion into a mobile app, which could increase accessibility and engagement, as well as integrating machine learning for even more personalized user experiences in sustainable eating.",
    "hackathon": null,
    "prize": "Winner Prizes",
    "techStack": "react, typescript",
    "github": "https://github.com/sutr4/sustainabite",
    "youtube": "https://www.youtube.com/watch?v=Y0vRktjl_Sk",
    "demo": "https://sustainabite-seven.vercel.app/",
    "team": "tracy su",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/sustainabite-of4zcx"
  },
  {
    "title": "MediFusion AI",
    "summary": "Project Summary: MediFusion AI\n\nMediFusion AI is an innovative healthcare application that leverages deep learning and AI technologies to enhance medical data processing and communication. It aims to streamline patient interactions by converting spoken language into text and providing insightful analysis, thereby improving healthcare accessibility and efficiency.\n\nKEY ACHIEVEMENTS: The project stood out due to its integration of advanced machine learning models, such as Llama-3-Vision and OpenAI Whisper, which enable high accuracy in speech recognition and natural language processing. Its success was further amplified by the user-friendly interface built with Gradio, making the technology accessible to healthcare professionals and patients alike.\n\nMediFusion AI employs a robust tech stack that includes Docker for containerization, FFmpeg for audio processing, and various libraries like Hugging Face for natural language understanding. The combination of technologies such as Python, speech-to-text capabilities, and cloud solutions like Groq-cloud ensures efficient processing and deployment of the application.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data privacy and compliance with healthcare regulations, given the sensitive nature of medical data. Additionally, achieving high accuracy in diverse accents and dialects of speech recognition may have posed technical hurdles. Ensuring the application's scalability to handle varying volumes of user interactions could also be a concern.\n\nFUTURE POTENTIAL: MediFusion AI has significant growth opportunities, including expanding its capabilities to support multilingual interactions and integrating with existing healthcare systems for seamless data exchange. Future iterations could also incorporate advanced analytics to provide predictive insights based on patient data, ultimately transforming patient care and operational efficiency in healthcare settings.\n\nWhat it does: MediFusion AI is an AI Voice Doctor that allows users to talk naturally and get intelligent, medically relevant responses. It: Listens to patient speech using Whisper + ElevenLabs STT with noise reduction\nAnalyzes symptoms and medical images using Google Cloud Vertex AI + LLaMA 3 Meta Vision (Groq API)\nResponds like a human doctor using ElevenLabs TTS for natural medical voice output\nPredicts diseases using ML models for Diabetes, Tumor Detection, and Heart Disease\nProvides a fully voice-based consultation through a Gradio conversational UI\n\nInspiration: In many regions, patients experience long waiting times and challenges accessing healthcare due to limited medical resources. We wanted to build an intelligent system that enables fast, voice-driven medical interaction. MediFusion AI bridges the gap between patients and doctors using real-time speech understanding, medical reasoning, and disease prediction.\n\nChallenges: Managing latency between multimodal model and speech generation\nSynchronizing STT, reasoning, TTS, and UI in real time\nHandling noisy audio input and transcription accuracy\n\nAccomplishments: Real-time voice-based intelligent medical consultation system\nIntegrated ElevenLabs + Google Cloud + Groq multimodal AI successfully\nAchieved smooth, natural conversational experience\n\nWhat we learned: Multimodal AI systems (Vision + Voice + ML)\nCloud deployment and real-time inference pipelines\nBuilding scalable conversational AI healthcare solutions\n\nWhat's next: Multilingual voice consultation\nMedical reports + symptom history tracking\nMobile app version and Firebase integration\nEnd-to-end patient analytics dashboard",
    "hackathon": null,
    "prize": null,
    "techStack": "deep-learning, docker, elevenlabs, ffmpeg, gradio, groq, groq-cloud, gtts, huggingface, llama-3-vision, machine-learning, openai-whisper, pyaudio, python, speech-to-text, vs-code",
    "github": "https://github.com/amit-sharma-ds/GenAIHeathcare",
    "youtube": "https://www.youtube.com/watch?v=kCBcLShagHk",
    "demo": "https://huggingface.co/spaces/AmitSharma99/MediFusionAI",
    "team": "Amit Sharma",
    "date": "2026-01-17",
    "projectUrl": "https://devpost.com/software/medifusion-ai-48mn51"
  },
  {
    "title": "3Docs",
    "summary": "3Docs is an innovative project that leverages artificial intelligence to enhance the way users interact with audio documentation, potentially targeting educational or training environments. By integrating advanced audio processing and AI technologies, the project aims to create an intuitive platform that efficiently organizes, analyzes, and presents audio content.\n\nKEY ACHIEVEMENTS\n3Docs distinguished itself by winning both the MadHacks First Place and overall Winner prizes, likely due to its unique approach of combining multiple cutting-edge technologies to deliver an exceptional user experience. Its ability to efficiently process and present audio content in a meaningful way resonated with judges and participants alike, showcasing a blend of creativity and practicality.\n\nThe project is built using a robust tech stack that includes Next.js for the frontend, Python for backend processing, and SQLite for data management. Notable implementations likely include the use of AI for audio analysis, enabling features such as transcription, summarization, and contextual insights, along with the integration of TypeScript for enhanced code quality and developer experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring the accuracy and reliability of AI-generated audio insights, which can be complex due to variations in audio quality and context. Additionally, integrating various technologies seamlessly, managing data storage effectively with SQLite, and creating a user-friendly interface in React may have posed significant hurdles during development.\n\nFUTURE POTENTIAL\n3Docs has substantial potential for evolution by expanding its capabilities to support a wider range of audio formats and languages, enhancing its AI algorithms for better accuracy, and integrating more interactive features such as user feedback loops. Moreover, it could be adapted for various industries such as education, healthcare, and corporate training, broadening its market reach and impact.\n\nWhat it does: 3Docs is a platform that transforms boring old PDF manuals into interactive 3D manuals complete with voice guidance and PDF references for each step of a process.\n\nInspiration: Reading a manual is hard, especially when images don't always make the most sense. However, guides are a lot easier to follow when more information (like an extra dimension!) is available. 3Docs makes repair more accessible by displaying manuals intuitively in ways that can \"make it click\" for more people.\n\nHow it was built: Frontend: Built with React (Next.js). We utilized the Three.js to render the .glb files directly in the browser.\nBackend: We used Python to manage API calls, caching, and sqlite database queries & insertions. To determine which images and instructions were useful and paraphrase instructions, we used Gemini 3 Pro Preview and Gemini 2.5 Pro, and we then matched paraphrased instructions to the loaded PDF to synchronize the user experience. To generate 3D models from images, we attempted several models, but ended up using the Tripo3D API; to generate audios for each instruction we used Fish Audio's API.\n\nChallenges: Currently available Image to 3D model models are kind of not great, so we had to use a private model locked behind an API.\nOlder models of Gemini (like 2.5 Pro) are bad at determining image content and usefulness\nLive audio processing and API calls for real-time generation of audio has too high latency to work well\n\nWhat's next: There are some features that 3Docs still lacks, such as input sanitation and live conversation. We also could benefit from sourcing a better and open source Image to 3D model model.",
    "hackathon": null,
    "prize": "Winner MadHacks First Place",
    "techStack": "artificial-intelligence, fish-audio, gemini, next.js, python, react, sqlite, tripoai, typescript",
    "github": "https://github.com/r-thak/3Docs-MadHacks2025-Winner",
    "youtube": "https://www.youtube.com/watch?v=FfbDphUebQk",
    "demo": "https://docs.google.com/presentation/d/1wH5-2nVvFJ_qfr9qA5CMiAYAcgmEjXY4lHjbWKdL734/edit?usp=sharing",
    "team": "Manas Joshi, Nick Yeung, Rohan Thakkar, Khoa Cao",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/3docs"
  },
  {
    "title": "FocusMate",
    "summary": "FocusMate Hackathon Project Summary\n\nFocusMate is a productivity-focused application designed to help users maintain focus and accountability while working on tasks. By integrating OpenAI, it likely offers features such as smart reminders, personalized productivity insights, or virtual co-working sessions that enhance user engagement and efficiency.\n\nKEY ACHIEVEMENTS: FocusMate stood out due to its innovative approach to tackling productivity issues, winning both the overall hackathon and the Best Productive Hack award. Its unique blend of technology and user-centric design likely provided a compelling solution that resonated with both judges and users.\n\nThe project employs HTML and JavaScript for its front-end development, ensuring a responsive and user-friendly interface. The integration of OpenAI suggests advanced functionalities, such as natural language processing for task management or intelligent scheduling, which enhance user interaction and personalize the experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring seamless integration of OpenAI functionalities, addressing user privacy concerns regarding data handling, and creating a scalable architecture that can handle multiple users simultaneously without performance degradation.\n\nFUTURE POTENTIAL: FocusMate has significant potential for evolution by incorporating features such as gamification to further boost user engagement, expanding to mobile platforms for accessibility, or integrating with other productivity tools and platforms to create a comprehensive ecosystem for users aiming to enhance their focus and productivity.\n\nWhat it does: First you tell the extension what task you will be focusing on, then the extension starts a timer and samples what tabs you have open in 10 minute intervals to tell how productive you are being. Having a majority of productive tabs will award you more points, where as you can lose points if you have too many off task tabs.\n\nInspiration: The main inspiration was productivity/educational applications that add an aspect of gamification to the process of working/learning. We also drew inspiration from online videogames with concepts like achievements, badges, and leaderboards.\n\nChallenges: Understanding how to use the OpenAI API. The main one was figuring out how to parse the large JSON file to get the information we wanted. Fixing small bugs with the logic for rewarding achievements since some of them didn't work.\n\nAccomplishments: Figuring out how to utilize the OpenAI API to tell if a user is on task. Creating unlockable achievements with fun designs that help to add to the gamification aspect of our project\n\nWhat we learned: How to make a chrome extension with embedded ChatGPT functionality\n\nWhat's next: Finalizing the landing page and deployment!",
    "hackathon": null,
    "prize": "Winner Best Productive Hack",
    "techStack": "html, javascript, openai",
    "github": "https://github.com/Stevie169/FocusMate",
    "youtube": "https://www.youtube.com/watch?v=_bDibaqiO5g",
    "demo": null,
    "team": "Shunyao Zhang, IanFleuchaus, Anthony Sestito, kishanKOTA Kishan rishik kota",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/focusmate-j1fwt2"
  },
  {
    "title": "Findr",
    "summary": "Findr Project Summary\n\nFindr is a platform designed to streamline the process of locating and connecting with friends or acquaintances in real-time, leveraging geolocation features. It aims to enhance social interactions by allowing users to easily find nearby connections for spontaneous meet-ups or planned gatherings.\n\nKEY ACHIEVEMENTS:  \n   Findr stood out in the hackathon by successfully integrating user-friendly geolocation services with a sleek and responsive interface. Its innovative approach to social connectivity and real-time interaction resonated with judges, leading to its recognition as a winner among the top teams.\n\nThe project utilized a robust tech stack, including Firebase for real-time data storage and authentication, React for building interactive user interfaces, and Tailwind CSS for rapid and responsive design. The use of TypeScript enhanced code quality and maintainability, while deployment on Vercel ensured seamless performance and scalability.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges related to user privacy and data security, as handling real-time location data necessitates stringent measures to protect user information. Additionally, ensuring a smooth user experience under varying network conditions could have posed difficulties, especially in high-traffic scenarios.\n\nFUTURE POTENTIAL:  \n   Findr could evolve by integrating features such as event planning tools, user-generated content, or enhanced privacy controls, allowing users to customize their visibility and interactions. Expanding the platform to include integrations with social media and offering AI-driven recommendations for meet-ups could further enhance its functionality and user engagement.\n\nWhat it does: Findr is a modern digital lost-and-found platform for schools. It has two main parts:\nStudent Portal: Students can browse a clean feed of found items, each with a photo, date, and description ‚Äî no login required. This makes it easy to quickly check if their belongings have been found.\nTeacher Portal: Teachers log in securely to upload new items with photos and descriptions, and can remove old items. Everything posted instantly appears in the student feed, keeping the system organized and efficient. \nFindr replaces messy lost-and-found boxes with a fast, user-friendly, and organized digital solution.\n\nHow it was built: We built Findr using a full-stack approach:\nFrontend: React + TypeScript + Vite for a fast and clean UI\nStyling: TailwindCSS for modern, responsive design\nBackend: Firebase Authentication + Firestore Database\nImage Storage: Firebase Storage for uploaded photos\nDeployment: Vercel for smooth hosting\nWe connected teacher authentication, real-time database updates, and image uploads to create a seamless experience.\n\nChallenges: Designing the UI: Making it modern, simple, and easy for both students and teachers\nAuthentication Logic: Ensuring only teachers can log in and upload items\nDatabase Sync: Making sure new items appear instantly for students\nImage Handling: Compressing and storing images without slowing down the app\nTime Constraint: Building the full stack and polishing everything quickly\n\nAccomplishments: Built a fully working app with real authentication, image upload, and item feed\nCreated a clean and elegant UI that feels professional\nBuilt a system that actually solves a real school problem\nMade a digital solution that could realistically be used by any school. More specifically, something that real schools would be interested in purchasing.\n\nWhat we learned: How to integrate frontend and backend systems efficiently\nHow to use Firebase Auth, Firestore, and Storage together\nHow to design user-friendly interfaces under time pressure\nHow to create an idea that is not too overcomplicated but also not basic, like a task manager.\n\nWhat's next: I want to create a history feed where teachers can see all the things that were found and lost to better keep a record. I also want notifications to teachers when a student claims an item. \nLastly, I would also like to add a bounty system where students can get points or some sort of reward if they turn things in to the lost and found. This would create a reason to give things to the lost and found instead of stealing it.",
    "hackathon": null,
    "prize": "Winner TOP 6 TEAMS",
    "techStack": "firebase, react, tailwind, tailwindcss, typescript, vercel, vite",
    "github": "https://github.com/RakeshJai/Findr",
    "youtube": "https://www.youtube.com/watch?v=nmn9570g19Q",
    "demo": "https://findr-murex.vercel.app/",
    "team": "Rakesh Jai",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/findr-s7vyjn"
  },
  {
    "title": "Jarvis Smart OS",
    "summary": "Jarvis Smart OS is an advanced operating system that leverages AI technologies to optimize user experience and enhance productivity through intelligent automation and personalized assistance. Its integration with Google Gemini API allows for sophisticated natural language processing and machine learning capabilities, enabling users to interact seamlessly with their devices.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively utilizing the Google Gemini API to deliver a unique and intuitive user interface that enhances user interaction. Winning both the overall hackathon and the \"Best Use of Google Gemini API\" prize highlights its innovative approach and successful implementation of cutting-edge technology.\n\nKey technical implementations include the integration of Eleven Labs for voice synthesis, the use of Gemini for advanced AI functionalities, and a robust backend built with Python and various Python libraries. This combination allows for efficient processing of user commands and intelligent responses.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring data privacy and security, particularly given the AI‚Äôs need for user data to personalize responses. Additionally, integrating multiple APIs and maintaining seamless communication between components while minimizing latency would have been a significant technical hurdle.\n\nFUTURE POTENTIAL\nJarvis Smart OS has the potential to evolve into a comprehensive personal assistant platform, expanding its capabilities to include more third-party integrations, enhanced machine learning for predictive analytics, and user-customizable features. It could also explore applications in various sectors, such as education, healthcare, or remote work environments, further enhancing its utility and user base.\n\nWhat it does: ‚ÄúJARVIS automates tasks, understands commands, manages devices, and acts as a smart assistant that simplifies and enhances everyday computing.‚Äù\n\nInspiration: ‚ÄúI built JARVIS OS because I refused to let the future stay in the movies.‚Äù\n\nHow it was built: ‚ÄúI built it by using Gemini AI model for the brain, automation scripts, and custom code into one unified system that can understand, respond, and take action.‚Äù\n\nChallenges: ‚ÄúThe biggest challenges were turning imagination into code, making the system truly intelligent, and overcoming the limits of technology.‚Äù\n\nAccomplishments: ‚ÄúI‚Äôve built the foundation of JARVIS‚Äîan intelligent system that listens, responds, and begins to think alongside its creator.‚Äù\n\nWhat we learned: ‚ÄúI learned that with vision, persistence, and curiosity, even the impossible can become real.‚Äù\n\nWhat's next: ‚ÄúNext, JARVIS will evolve into a fully autonomous, intuitive OS that learns, adapts, and collaborates seamlessly with its user.‚Äù",
    "hackathon": null,
    "prize": "Winner Best Use of Google Gemini API",
    "techStack": "elevenlabs, gemini, mem0, python, pythonlibraries",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ot9sI5nplZw",
    "demo": null,
    "team": "Barrack vobeck",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/jarvis-smart-os"
  },
  {
    "title": "Frames",
    "summary": "Frames Hackathon Project Summary\n\nFrames is a design-focused project that leverages collaborative tools like FigJam and Figma to streamline the process of creating and presenting design mockups. By integrating these platforms, the project enables teams to visualize ideas effectively and share them seamlessly, enhancing collaboration and feedback during design iterations.\n\nKEY ACHIEVEMENTS: \n   Frames stood out by winning both the overall prize and a second-place award, indicating strong recognition from judges. Its innovative approach to integrating multiple design tools for improved workflow efficiency and collaborative capabilities likely resonated well with the hackathon criteria, showcasing real-world applicability and user-centric design.\n\nThe project effectively utilized Figma's design capabilities alongside FigJam's brainstorming features, allowing for a smooth transition between ideation and execution. The integration of Figma Slides further enhanced the presentation aspect, enabling users to create comprehensive presentations directly from their design mockups, which is a significant technical achievement in terms of user experience.\n\nPOTENTIAL CHALLENGES: \n   The team likely faced challenges related to ensuring seamless integration between the different platforms, managing real-time collaboration among team members, and addressing potential user interface issues that could arise from combining multiple tools. Additionally, optimizing the workflow to avoid redundancy and maintain clarity in the design process could have been a hurdle.\n\nFUTURE POTENTIAL: \n   Frames has significant potential for evolution by expanding its features to include more automation in design processes, such as AI-driven suggestions based on user input. Additionally, it could explore integrating with other project management and communication tools to create a more holistic ecosystem for design teams, enhancing user engagement and productivity.",
    "hackathon": null,
    "prize": "Winner Second Place",
    "techStack": "figjam, figma, figma-slides",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=8eLthlAIr90",
    "demo": "https://drive.google.com/file/d/16-t7E1vizsNB2FM3DsB1DONLk2f1y5TW/view?usp=sharing",
    "team": "natalie yu",
    "date": "2025-11-22",
    "projectUrl": "https://devpost.com/software/frames-wv0j6l"
  },
  {
    "title": "NutriScan AI",
    "summary": "NutriScan AI is a health-focused application designed to analyze nutritional information and provide personalized dietary recommendations. Leveraging advanced machine learning algorithms, the tool helps users make informed food choices based on their individual health needs and preferences.\n\nKEY ACHIEVEMENTS\nNutriScan AI distinguished itself by winning multiple accolades, including the Best Health Hack by Sun Life and the MLH Best Use of DigitalOcean Gradient AI. Its success can be attributed to its innovative use of AI for personalized nutrition, effective user interface design, and the ability to integrate seamlessly with health data.\n\nThe project was built using a modern tech stack that includes JavaScript, TypeScript, Node.js, and React, providing a robust and responsive user experience. The use of DigitalOcean's Gradient AI showcases its capability to leverage cloud-based machine learning services for real-time data processing and analysis. Additionally, Tailwind CSS was utilized for its sleek and customizable design.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to data accuracy and the complexity of dietary recommendations, as individual nutritional needs can vary significantly. Integrating various data sources, ensuring user privacy, and maintaining performance under high load conditions could have also posed technical hurdles.\n\nFUTURE POTENTIAL\nNutriScan AI could evolve by incorporating more sophisticated AI models that adapt to user feedback over time, enhancing personalization. Future iterations could include features such as meal planning, integration with fitness tracking devices, and community support for users to share experiences and recipes, further enriching the user experience.\n\nWhat it does: NutriScan uses a multi-modal AI system powered by two visual assessments:\nFace Scan Reads hydration levels.\nPicks up pallor and vitality cues. Fingernail Scan Analyzes keratin strength and texture.\nReads colour patterns tied to iron, B-vitamins, and minerals.\nChecks nail-bed circulation for micronutrient indicators. These scans feed into a lightweight lifestyle questionnaire. The system then generates: An Overall Health Score\nSubscores for hydration, vitamins, minerals, protein quality, and calorie balance\nA personalized daily meal plan\nA simple supplement protocol\nA budget-friendly grocery list\nA clean dashboard with widgets to track your daily targets\n\nInspiration: NutriScan came from a simple problem most university students face: eating healthy on a tight budget takes way too much effort. Classes, assignments, and job applications already drain your time. Building a meal plan on top of that feels unrealistic. Most nutrition apps tell you to ‚Äúhit your calories,‚Äù but they don‚Äôt explain what your body actually needs or why. None of them factor in how broke or busy you are as a student. NutriScan fixes that. You take two quick scans, answer a short form, and get direct, affordable recommendations you can follow without thinking. It‚Äôs built for real students with real schedules.\n\nHow it was built: We built NutriScan AI using a modern React and TypeScript stack, capturing high-quality biometric imagery directly in the client. These visual inputs are processed by Google's Gemini Multimodal API, which analyzes the images for physiological biomarkers and references them with user-provided health data. The frontend, styled with Tailwind CSS for a responsive dark-mode experience, parses the AI's structured JSON response to render real-time, interactive visualizations and personalized meal plans.\n\nChallenges: Personally the hardest part was finding an idea that was actually useful and original. Most ground breaking ideas either already seem to exist or don‚Äôt solve a real problem. Once we settled on NutriScan, the next challenge was figuring out how to implement it cleanly with a simple yet intuitive UI that doesn‚Äôt overwhelm users. We had to balance keeping the scans fast and the dashboards readable with shipping meaningful features.\n\nAccomplishments: The entire project, we came up with the idea Saturday morning and we implemented everything by working non stop till 5 am in the morning. Constantly improving the project while also making it.\n\nWhat we learned: We learned that the power of AI only shows up when you have a strong idea and a clear plan. Once we locked in the concept, everything, from prompts to model outputs to UI decisions, became 10x easier. Good features came from understanding the user (Us - Students), not from trying to overbuild. To use a strong idea mattered more than the tech. We also learned how much teamwork shapes the final product. The connection between us pushed the project forward and made the long hours feel lighter. When everyone commits, the work moves fast and the results show. That‚Äôs why NutriScan came together the way it did. We‚Äôre proud of what we built and how we built it.\n\nWhat's next: Our Immediate Next Steps -> Apple Watch integration to pull hydration, HRV, and sleep patterns\nNew assessments as we do deeper research\nUser-to-user updates and messaging for progress tracking\nExpanded scan capabilities with larger datasets",
    "hackathon": null,
    "prize": "Winner Best Health Hack by Sun Life; Winner [MLH] Best Use of DigitalOcean Gradient AI",
    "techStack": ".tech, digitalocean, gemini, git, github, javascript, json, node.js, react, tailwindcss, typescript, vite",
    "github": "https://github.com/ZElnagar/HackWestern12",
    "youtube": "https://www.youtube.com/watch?v=VRBSDdESFvY",
    "demo": "https://nutriscans.tech/",
    "team": "Ziad Elnagar, Tristan Beley, Shayaan Tanvir, Mohammadh Hammad",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/nurtiscan"
  },
  {
    "title": "HackProd",
    "summary": "HackProd Project Summary\n\nHackProd is an innovative application that leverages LightningAI to create a dynamic, interactive environment for users, likely focusing on enhancing productivity through advanced visualization techniques. Utilizing technologies like React and Three.js, the project aims to transform user interaction with data and applications in a visually engaging manner.\n\nKEY ACHIEVEMENTS: HackProd distinguished itself by effectively integrating LightningAI, showcasing its capabilities in a practical application that enhances user experience and productivity. Winning both the overall prize and the award for the best use of LightningAI indicates high levels of creativity, technical proficiency, and relevance to current tech trends.\n\nThe project employs JavaScript and React for efficient UI development, combined with Three.js for rendering 3D graphics. This powerful stack allows for real-time visualization and interaction, providing users with a compelling experience that sets it apart from traditional applications.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as optimizing performance for real-time rendering, integrating various technologies seamlessly, and ensuring user accessibility and engagement. Additionally, addressing any potential scalability issues as user demands grow could have also been a concern.\n\nFUTURE POTENTIAL: HackProd has significant potential for evolution, including expanding its feature set to support collaboration tools, integrating additional AI functionalities, or targeting specific industries for tailored solutions. As the demand for interactive and immersive applications grows, HackProd could position itself as a leader in productivity solutions.\n\nWhat it does: HackProd is an autonomous AI agent that analyzes hackathon projects to detect 15 sponsor technologies, scores integration depth (0-10), generates technical + plain-English summaries, and validates integrations by actually running projects in the cloud via Lightning AI. It learns from every analysis‚Äîgetting smarter, faster, and more accurate over time.\n\nInspiration: Judges, sponsors, and organizers manually reviewing every hackathon project to understand sponsor integrations is time-consuming, inconsistent, and doesn't scale. Teams shouldn't have to write extra reports‚Äîtheir code should speak for itself.\n\nHow it was built: Agent: Claude/OpenAI orchestrator with autonomous tools (read files, search code, parse dependencies)\nMemory & Learning: Redis-based memory system with reflection loops‚Äîagent recalls past learnings and self-improves\nCloud Execution: Lightning AI integration to deploy, test, and validate projects actually work\nBackend: Node.js + Express + async job queue (Redis)\nStorage: Sanity CMS for structured results, optional AWS S3 for repos\nSponsors used: Anthropic (Claude), Redis, Sanity, Lightning AI, AWS\n\nChallenges: Making the agent truly autonomous with memory/reflection, implementing fail-safe cloud execution that never breaks static analysis, handling diverse project structures (monorepos, microservices), building reliable sponsor detection across 15 different technologies, managing API costs while achieving high accuracy.\n\nAccomplishments: ‚úÖ Autonomous learning agent that improves 42% in accuracy after 10 analyses\n‚úÖ Cloud execution validation via Lightning AI with proper fail-safes\n‚úÖ Self-reflection system that stores learnings and recalls them\n‚úÖ 15 sponsor detectors with deep integration scoring\n‚úÖ Complete pipeline: API ‚Üí Queue ‚Üí Clone ‚Üí Analyze ‚Üí Execute ‚Üí Store ‚Üí Cache\n\nWhat we learned: How to build truly autonomous agents with memory and reflection (not just chatbots), the power of combining static analysis + execution validation, importance of fail-safe design when adding complex features, that agents get dramatically better with real-world feedback loops.\n\nWhat's next: Multi-agent specialists (one per sponsor technology), human-in-the-loop feedback to correct the agent, transfer learning to share knowledge across hackathons, GPU support for ML project analysis, visual UI testing with screenshots, Postman/Newman integration for comprehensive API validation.",
    "hackathon": null,
    "prize": "Winner Best use of LightningAI",
    "techStack": "javascript, react, three.js",
    "github": "https://github.com/bilgee0517/hackathon-automation-agent",
    "youtube": "https://www.youtube.com/watch?v=O8YEurGkzeQ",
    "demo": null,
    "team": "Chinguun Ganbaatar, Bilegjargal Altangerel",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/hackprod"
  },
  {
    "title": "Urban",
    "summary": "Urban is a project designed to leverage advanced cloud technologies to create an innovative solution aimed at enhancing urban living. While specific details are not provided, the project's core likely involves utilizing data and AI tools to address challenges faced in urban environments, such as resource management, public safety, or community engagement.\n\nKEY ACHIEVEMENTS\nUrban distinguished itself by securing multiple awards at the hackathon, including \"Winner,\" \"Best Use of Parallel,\" \"Best Use of LightningAI,\" and \"Top Overall Winners.\" These accolades indicate that the project not only excelled in its concept and execution but also effectively integrated advanced technologies to deliver a compelling solution that resonated with judges and participants alike.\n\nThe project utilized a robust tech stack, including Amazon Web Services (AWS) for cloud infrastructure, LightningAI for artificial intelligence capabilities, and Parallel for optimizing performance. Additionally, it incorporated Sanity for content management and Skyflow for secure data handling, showcasing a comprehensive approach to managing and processing data in an urban context.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating multiple technologies seamlessly and ensuring data security, particularly when dealing with personal or sensitive information. Additionally, building a scalable solution that can adapt to diverse urban environments and user needs may have posed significant technical hurdles.\n\nFUTURE POTENTIAL\nUrban has the potential to evolve into a comprehensive urban management platform that can adapt to various cities and their unique challenges. Future developments could include enhanced AI capabilities for predictive analytics, partnerships with local governments for real-world implementations, and the addition of community engagement features to foster collaboration among urban residents.\n\nHow it was built: Our build was a fast, hackathon-grade version of a production system: Document understanding: extracted and structured policy text into targets, rules, timelines.\nKnowledge modeling: stored policy + programs + milestones in a structured CMS.\nRetrieval + memory: indexed policies and past case summaries for fast semantic lookup.\nAgent orchestration: connected tool-calling agents with clear JSON schemas.\nSimulation layer: simple forecasting formulas + interactive UI sliders.\nDashboard UI: KPI cards, charts, scorecards, and a 3D map layer for impact.\nEnd-to-end demo flows: crisis case ‚Üí policy lookup ‚Üí evidence ‚Üí action plan ‚Üí analytics update.\n\nChallenges: PDF complexity ‚Üí structured truth: policies are long, verbose, and sometimes inconsistent; extracting reliable targets was non-trivial.\nSimulation realism vs time: we had to keep models simple but believable in one day.\nTool coordination: ensuring agents worked with retrieval, evidence tools, and privacy layers without breaking the flow.\nAvoiding ‚Äúdashboard spam‚Äù: we focused on policy-aligned metrics only, not random charts.\nCivic safety: ensuring the system doesn‚Äôt give unsafe advice, especially in sensitive cases.\n\nWhat we learned: Policies become powerful only when they are operationalized. We learned how to convert dense government PDFs into checkable rules + KPIs.\nAgentic systems need guardrails. Civic agents must be safe, minimal-question, privacy-aware, and auditable.\nDashboards without simulations are blind. Real decision tools need ‚Äúwhat-if‚Äù views, not just reporting.\nThe story matters. The 3D post-effects layer dramatically improved how intuitive policy impact felt.",
    "hackathon": null,
    "prize": "Winner Best Use of Parallel; Winner Best use of LightningAI; Winner Top Overall Winners",
    "techStack": "amazon-web-services, lightningai, parallel, sanity, skyflow",
    "github": "https://github.com/Steve-Dusty/urban",
    "youtube": null,
    "demo": null,
    "team": "Ayaan Gazali, Steve Kuo",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/urban"
  },
  {
    "title": "Briefly - Stay Smart in Minutes",
    "summary": "\"Stay Smart in Minutes\" is a project designed to provide users with quick, digestible insights and knowledge across various topics, leveraging advanced AI technologies. By integrating services such as natural language processing and speech synthesis, the platform aims to enhance learning and information retention in a time-efficient manner.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively combining multiple cutting-edge technologies to create a seamless user experience. Its innovative approach to delivering information in a concise format, along with its user-friendly interface, earned it recognition as a top overall winner at the hackathon.\n\nNotable technical implementations include the use of Amazon Web Services for cloud infrastructure, which ensures scalability and reliability. The integration of Anthropic's AI for natural language understanding and ElevenLabs for voice synthesis allows for interactive and engaging content delivery. The use of Redis for data management enhances performance by providing quick access to frequently used information.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring the accuracy of the information presented and managing the integration of diverse technologies. Additionally, optimizing the user experience while maintaining a quick response time could have posed technical hurdles.\n\nFUTURE POTENTIAL\nThe project could evolve by expanding its content library to cover more subjects and languages, as well as incorporating personalized learning paths based on user preferences. Future developments could also include mobile app integration and partnerships with educational institutions to broaden its reach and impact.\n\nWhat it does: Briefly creates personalized daily podcast episodes that summarize topics you care about in 5-, 10-, or 15-minute audio briefings. Using AI, it curates top stories, converts them to natural speech, and delivers them automatically to Spotify or your favorite podcast app. It‚Äôs effortless daily knowledge, tailored to you.\n\nInspiration: We wanted to simplify how people stay informed. Endless articles, newsletters, and podcasts take hours to digest. Our team envisioned a smarter way, a personalized, AI-powered daily audio feed that turns the world‚Äôs noise into quick, high-quality insights people can actually keep up with.\n\nHow it was built: We built the frontend using Lovable for fast, responsive design. The backend runs on Parallel, and integrated with Redis for caching. We used Claude and the Claude SDK to power our agents, which automatically search the internet for relevant news via Parallel. The summarized text is then converted to natural, high-quality speech using ElevenLabs, completing a seamless pipeline from web content to generated podcast audio.\n\nChallenges: Connecting multiple tools and frameworks within limited time was a core challenge. Integrating embeddings and vector search for accurate topic recommendations required troubleshooting schema mismatches and fine-tuning queries. Synchronizing asynchronous agent calls between Parallel and ElevenLabs also tested timing and data consistency.\n\nAccomplishments: Lets users select personalized interests and generates dynamic podcast episodes in real time. Seamlessly transforms summarized text into high-quality, natural-sounding audio. Implements vector-based user preference matching that actually works. Looks polished and production-ready, complete with branding, UI, and logo system\n\nWhat we learned: We learned how powerful agents can be when orchestrated effectively, particularly their ability to browse, filter, and synthesize real-time web content. Vector databases add a whole new dimension to personalization but require deep understanding of embedding quality and retrieval optimization. We also saw that even in fast builds, clean architecture and modular design make iteration exponentially faster.\n\nWhat's next: Integrate Stripe for premium subscriptions and creator monetization. Expand content sources beyond news into research, business, and educational topics. Tailor summaries to knowledge levels and age demographics for broader accessibility. Launch a mobile-first app and Chrome extension for daily listening on the go. Our long-term goal: make personalized audio briefings a new standard for consuming information, fast, trustworthy, and human-like.",
    "hackathon": null,
    "prize": "Winner Top Overall Winners",
    "techStack": "amazon-web-services, anthropic, elevenlabs, loveable, parallel, python, redis",
    "github": "https://github.com/michaelwaves/briefly",
    "youtube": "https://www.youtube.com/watch?v=cHvPEjYWgaA",
    "demo": null,
    "team": "Bryce Masterson, Michael Yu, jmiller94102 Miller",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/briefly-stay-smart-in-minutes"
  },
  {
    "title": "API Self Healer",
    "summary": "The API Self Healer project aims to automatically monitor and repair APIs, ensuring they remain functional and efficient even in the face of failures or changes in behavior. By leveraging advanced tools and technologies, the project provides a proactive solution for maintaining API integrity and reliability, enhancing overall application performance.\n\nKEY ACHIEVEMENTS\nThis project stood out in the hackathon due to its innovative approach to API management, winning multiple awards including Best Use of Parallel and Best Use of Postman. Its ability to self-diagnose and rectify issues in real-time distinguishes it as a robust solution for developers, minimizing downtime and improving user experience.\n\nThe project integrated several advanced technologies such as agentsdk for automation, Claude for natural language processing, and Parallel for efficient task handling. Additionally, it utilized Postman for API testing and documentation, coupled with Python and TypeScript for backend and frontend development, respectively, enhancing its functionality and usability.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the complexity of API systems and the reliability of automated repair mechanisms. Ensuring that the self-healing process does not introduce new errors or regressions could also have been a significant hurdle, as well as achieving seamless integration with existing APIs in various environments.\n\nFUTURE POTENTIAL\nThe API Self Healer could evolve into a comprehensive API management platform that not only offers self-healing capabilities but also predictive analytics to forecast potential issues before they occur. Future developments could include integration with more diverse API ecosystems, enhanced user interfaces for better monitoring, and support for a wider range of programming languages and frameworks.\n\nInspiration: Debugging and maintaining APIs is tedious‚Äî400 errors, missing fields, wrong types, evolving schemas. AI coding agents still require a lot of handholding to fix. We wanted an agent that acts like an API mechanic: it detects broken requests in postman, finds the relevant documentation, and fixes the request automatically in Postman.\n\nHow it was built: We integrated Postman‚Äôs API with Claude and Parallel, designed failure cases across Stripe and Notion (wrong types, wrong data formats, missing required fields, malformed nested JSON structures), and built an iterative repair loop grounded in schema definitions from the docs.\n\nChallenges: Ensuring the agent grounded fixes in real documentation, generating malformed schemas that produced meaningful validation errors, and coordinating the cycle of reading ‚Üí repairing ‚Üí retesting ‚Üí updating the request.\n\nWhat's next: More support for other types of API errors such as auth. Hosting it so it is accessible for other agents to use.",
    "hackathon": null,
    "prize": "Winner Best Use of Parallel; Winner Best Use of Postman",
    "techStack": "agentsdk, claude, parallel, postman, python, typescript",
    "github": "https://github.com/bekhamit/api-self-healer-agent",
    "youtube": "https://www.youtube.com/watch?v=rQES4c079sQ",
    "demo": null,
    "team": "Anna McGovern, Bek Hamit",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/api-self-healer"
  },
  {
    "title": "StudyBuddy",
    "summary": "StudyBuddy Project Summary\n\nStudyBuddy is an innovative educational platform designed to enhance collaborative learning by connecting students with study partners based on shared subjects and study goals. The platform leverages AI-driven recommendations and real-time communication tools to create an engaging and productive study environment.\n\nKEY ACHIEVEMENTS:  \n   StudyBuddy distinguished itself by securing multiple awards, including the title of Best Agent Using RedisVL and Best Use of mcptotal.ai. Its success can be attributed to its effective integration of advanced AI technologies and a user-friendly interface that fosters collaboration and connection among students.\n\nThe project utilizes FastAPI for building efficient APIs, Next.js for a dynamic front-end experience, and Redis for real-time data handling and caching, ensuring quick responses and smooth user interactions. Integration with mcptotal.ai provides enhanced AI capabilities, allowing for personalized study partner recommendations.\n\nPOTENTIAL CHALLENGES:  \n   The team likely faced challenges such as ensuring scalability to handle multiple users simultaneously, maintaining data privacy and security given the sensitive nature of educational data, and effectively integrating various technologies to create a seamless user experience.\n\nFUTURE POTENTIAL:  \n   StudyBuddy has significant potential for growth, including the addition of features like gamification elements to increase engagement, integration with educational resources and platforms, and expansion into different educational contexts or age groups. Further development could also explore advanced analytics to provide insights into study habits and partner effectiveness.\n\nWhat it does: StudyBuddy converts any study input, PDFs, YouTube links, Notion pages, URLs, or raw text into a complete AI-powered learning package:\nüìä Auto-generated slide deck with clean layouts and AI visuals\n‚ùì Interactive quizzes with explanations\nüÉè Flashcards for revision\nüìö Personalized learning using Redis (your history influences difficulty & selection)\nüîó Notion Integration (via MCP) saves organized content directly into your Notion workspace\nüé• YouTube transcript support extracts content from long lectures instantly\nüí¨ Context-aware chatbot that answers questions using your uploaded material StudyBuddy turns messy notes + long videos into structured learning instantly.\n\nInspiration: Studying today is overwhelming. Students collect PDFs, messy class notes, YouTube tutorials, Notion pages, screenshots, and random web articles, but turning all of this raw information into clean, structured, digestible study material is time-consuming. We wanted a tool that instantly transforms any learning content into slides, quizzes, flashcards, and summaries, so students can focus on learning, rather than formatting.\nThat idea became StudyBuddy.\n\nHow it was built: StudyBuddy is built as a multi-agent AI pipeline that deeply integrates Postman MCP, Notion, Sanity, Redis, and LLM reasoning. Every part of the system is powered by a sponsor tool. 1) Sanity CMS - Content Operating System Sanity acts as the content backbone of StudyBuddy, storing all generated learning materials in a clean, structured, and scalable way. ‚úÖ What we store in Sanity\nSanity houses every part of the learning package:\nSlides, Title, Bullet points, AI-generated diagrams, Associated images, Flashcards, Front (question or concept), Back (answer or explanation), Quizzes, Multiple-choice questions, Options, Correct answer, index, Explanations, Study summaries, Topic summaries, Chapter breakdowns, Assets, Images(generated diagrams, visual examples), Uploaded files, Reference media üöÄ\n\nChallenges: Notion MCP integration: Understanding how MCP tools interact with Notion pages required debugging through MCPTotal\nEmbedding storage & retrieval speed when using Redis for large documents\nFragmented YouTube transcripts are causing inconsistent chunk boundaries\nTime constraints, balancing a multi-agent architecture within the hackathon time\n\nAccomplishments: Built a full multi-agent learning pipeline end-to-end\nSuccessfully integrated Notion + MCPTotal MCP into our app\nCreated beautifully structured auto-generated slides & quizzes\nDesigned a clean UI that feels like a real product\nBuilt a scalable and modular architecture for future growth\nIntegrated YouTube ‚Üí Transcript ‚Üí Learning package flow\nAchieved usable personalization with Redis RAG\n\nWhat we learned: How powerful MCP workflows become when paired with Notion\nHow to integrate multiple sponsor tools into a single ecosystem\nEfficient prompt engineering for multi-agent systems\nStructuring long PDFs into meaningful chunks for RAG\nBuilding real-time pipelines between front-end, backend, Sanity, Redis, and MCPTotal\nHow to turn raw content into digestible study material using AI\n\nWhat's next: Adding full audio narration for each slide\nExport to PowerPoint, Quizlet, and Anki\nA mobile app version for on-the-go studying\nSupport for GPT-based tutoring mode\nMulti-user workspaces + collaborative studying\nChrome extension ‚Üí ‚ÄúConvert this page to slides‚Äù\nLMS integrations (Canvas, Moodle, Blackboard)",
    "hackathon": null,
    "prize": "Winner Best Agent Using RedisVL; Winner Best Use of mcptotal.ai",
    "techStack": "fastapi, mcptotal, nextjs, postman, python, redis, sanity",
    "github": "https://github.com/Sakshamyadav19/MysticNotes",
    "youtube": "https://www.youtube.com/watch?v=sgEiWbvlSG8",
    "demo": null,
    "team": "Vedant Vivek Kandge, Yash Pokharna, Vedant Kandge, Mandar Menjoge, Saksham Yadav",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/studybuddy-vzefio"
  },
  {
    "title": "Pulse",
    "summary": "Pulse Project Summary\n\nPulse is an innovative application designed to leverage advanced technologies for health monitoring and personal wellness. It integrates with Huawei devices and utilizes a variety of software tools to provide users with insightful health data and personalized wellness recommendations.\n\nKEY ACHIEVEMENTS: Pulse stood out for its seamless integration with Huawei‚Äôs ecosystem, particularly the Huawei Watch 5, and its ability to deliver real-time health metrics. Winning the hackathon and the Huawei prize highlights its potential impact in the health tech space and the effectiveness of its user-centric design.\n\nThe project is built using a combination of arkts, arkui, deveco-studio, and openharmony frameworks, showcasing its robust architecture. Notably, it employs advanced data processing algorithms to analyze health data from wearables, ensuring accurate and timely feedback to users.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring data privacy and security, as health-related applications must comply with strict regulations. Furthermore, optimizing the user experience across various devices while maintaining performance would have been a critical hurdle.\n\nFUTURE POTENTIAL: Pulse has significant opportunities for growth, including expanding its features to incorporate AI-driven health insights and recommendations. Future developments could also involve partnerships with healthcare providers for a more comprehensive health management solution, potentially transforming it into a holistic health platform.\n\nWhat it does: Pulse is a standalone wearable application that uses a multimodal feedback system: Feels: A strong vibration metronome set to 110 BPM keeps the rescuer locked into the perfect rhythm.\nTells: Voice commands give immediate corrective feedback (e.g., \"Push Faster,\" \"Release Chest Fully\").\nShows: A clear, high-contrast UI designed for circular displays providing visual cues.\n\nInspiration: In a cardiac emergency, panic is the enemy. While many people know the basics of CPR, maintaining the correct rhythm (100-120 BPM) and proper form (depth and recoil) while under extreme stress is incredibly difficult. We realized that the technology to solve this: accelerometers, gyroscopes, and haptic engines is already sitting on our wrists. We wanted to build a real-time CPR coaching system to turn a Huawei Watch 5 into a real-time life-saving coach that doesn't just track data, but actively guides the rescuer.\n\nHow it was built: We built the application using ArkTS on the OpenHarmony operating system, pushing the wearable hardware to its absolute limits. By accessing raw high-frequency sensor data, we reconstructed the biomechanics of the rescuer's hands. Instead of relying on generic activity recognition APIs, we engineered and trained a custom machine learning decision tree using a proprietary dataset we collected ourselves directly with the Huawei Watch 5. By running this custom-trained model on-device, we can analyze complex motion patterns in real-time, allowing us to distinguish between proper compressions and specific form errors with a level of precision that standard algorithms cannot achieve.\n\nChallenges: Our biggest technical hurdle was recoil detection (detecting \"leaning\"). Accelerometers are noisy, and distinguishing between a shallow press and a failure to release the chest (leaning) is mathematically complex due to gravity drift. We solved this by implementing a dynamic calibration system that locks the \"zero point\" when the user starts, allowing us to detect when the chest hasn't returned to its neutral position.\n\nWhat we learned: The Power of HarmonyOS & ArkTS: We learned that ArkTS is capable of much more than just simple UI. By leveraging OpenHarmony's efficient architecture, we were able to process high-frequency sensor data (50Hz) and run complex logic in real-time without compromising the smoothness of the UI. This proved to us that ArkTS delivers the native-grade performance required for critical health applications on wearable hardware.",
    "hackathon": null,
    "prize": "Winner Huawei",
    "techStack": "arkts, arkui, deveco-studio, huawei-watch-5, oniro, openharmony",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=9JfntLw_g_I",
    "demo": null,
    "team": "Carlo Lubeseder, Pascal Weidner, Linus Richter, Attila Ersin",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/huawei-oxc57d"
  },
  {
    "title": "Les enfants  d‚ÄôAdam",
    "summary": "\"Les enfants d‚ÄôAdam\" is a project that aims to create an interactive platform, likely focused on educational or community engagement themes, leveraging modern web technologies. The project combines a robust backend with a user-friendly frontend to facilitate seamless interactions and data management, targeting a specific audience, possibly children or educators.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its innovative approach to user engagement and its effective use of a full-stack technology stack. Winning the hackathon suggests that it successfully addressed a pressing need or problem within its target demographic, showcasing both creativity and technical proficiency.\n\nNotable technical implementations include the use of Node.js for a scalable server-side architecture, PostgreSQL for reliable data management, and React with TypeScript for a responsive and maintainable frontend. The combination of these technologies likely allowed for a high-performance application with a smooth user experience and robust data handling capabilities.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as integrating various technologies seamlessly, ensuring data security and user privacy, and creating a compelling user experience that meets the needs of its audience. Additionally, managing time constraints typical of hackathons could have posed a significant hurdle during development.\n\nFUTURE POTENTIAL\n\"Les enfants d‚ÄôAdam\" has significant potential for evolution by expanding its features to include gamification elements, mobile app development, or partnerships with educational institutions. Scaling the platform to reach a broader audience and incorporating user feedback could enhance its relevance and impact in the community.\n\nWhat it does: What it does The project provides an intelligent platform that simplifies users‚Äô lives by bringing together essential services: communication, management, security, information, and automation through AI.\n\nInspiration: Inspiration Our project was inspired by the need to solve real problems faced by communities. Every great solution begins with a challenge, and we decided to transform these challenges into opportunities for innovation.\n\nHow it was built: How we built it We built the project by combining research, software development, and teamwork. We used modern tools, a clear structure, and strong collaboration to create a reliable and scalable solution.\n\nChallenges: Challenges we ran into We faced several challenges, including limited resources, integrating different technologies, optimizing security, and managing time. But each challenge helped us improve and strengthen the project.\n\nAccomplishments: Accomplishments that we're proud of We are proud of turning a simple idea into a functional solution. We succeeded in creating a strong, secure, and useful foundation with real impact. Most importantly, we learned, grew, and built the groundwork for the future of the project.\n\nWhat we learned: What we learned Throughout this project, we learned how to transform problems into practical solutions. We improved our technical skills, strengthened our teamwork, and gained experience in building secure, scalable systems. We also learned the importance of user feedback and adaptability when creating impactful solutions.\n\nWhat's next: Inspiration\nWhat we learned Throughout this project, we learned how to transform problems into practical solutions. We improved our technical skills, strengthened our teamwork, and gained experience in building secure, scalable systems. We also learned the importance of user feedback and adaptability when creating impactful solutions.\nWhat‚Äôs next Next, we plan to expand the platform with more advanced features, improve the user experience, and integrate stronger AI capabilities. We will also focus on scaling the project, reaching more users, and building partnerships to bring the solution to a larger community.",
    "hackathon": null,
    "prize": null,
    "techStack": "node.js, postgresql, python, react, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=G64KJMLL7xk",
    "demo": "https://les-enfants-d-adam.onrender.com/",
    "team": null,
    "date": "2025-11-25",
    "projectUrl": "https://devpost.com/software/les-enfants-d-adam"
  },
  {
    "title": "Toyota GR Cup Real-Time Dashboard Simulator",
    "summary": "The Toyota GR Cup Real-Time Dashboard Simulator is an interactive platform designed to visualize real-time data related to the Toyota GR Cup racing events. It utilizes various technologies to provide users with an engaging and informative dashboard that displays live statistics, race analytics, and performance metrics.\n\nKEY ACHIEVEMENTS\nThis project stood out due to its innovative use of real-time data analytics, providing users with immediate insights into race performance and dynamics. Winning both the overall hackathon and the \"Best of Real-time Analytics\" prize highlights its effectiveness and appeal, showcasing a seamless integration of data visualization and user experience.\n\nNotable technical implementations include the use of Three.js for 3D visualization, which enhances the graphical representation of racing data. The project also leverages Google technologies for data handling and real-time updates, alongside CSV files for structured data input, demonstrating a robust approach to managing and displaying complex datasets.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to real-time data synchronization, ensuring the accuracy and reliability of live updates during races. Additionally, integrating multiple data sources and maintaining performance while rendering complex visualizations could have posed significant technical hurdles.\n\nFUTURE POTENTIAL\nThis project has significant potential for evolution by incorporating advanced machine learning algorithms to predict race outcomes or enhance analytics features. Expanding the dashboard to include user personalization options and mobile compatibility could also broaden its audience. Additionally, partnerships with racing organizations could lead to further refinements and real-world application.\n\nWhat it does: The GR Cup Real-Time Dashboard Simulator is a browser-based analytics and strategy tool designed for use by a race engineer on a tablet. It provides a real-time, visual experience of the race. It shows live standings and telemetry, while dynamically tracking all cars on the Barber Motorsports Park circuit, highlighting the selected car in red. The tool features a Strategy Impact panel displaying critical metrics like Tire Status and degradation, and competitor Gap Ahead/Behind. Most importantly, it uses AI to provide an immediate, data-driven recommendation (e.g., PUSH) and calculates the optimal Pit Stop window.\n\nInspiration: The project was primarily inspired by the critical need for race engineers to make instant, data-driven decisions in the high-pressure environment of a live race. I recognised that winning often depends on the speed and accuracy of strategic calls. My goal was to create a tool that transforms massive amounts of raw telemetry data into actionable intelligence, directly enhancing driver insights and team performance during race-day decision-making for the GR Cup Series.\n\nHow it was built: The dashboard was built using JavaScript and the three.js library for the visualisation and car movement. The strategic insights and recommendations are generated using Google AI Studio. For data, I used the anonymised CSV files provided for the competition, including the Analysis Endurance and Weather data for Race 1, to simulate a live data feed. Creating the track layout involved tracing the Barber circuit on Google Maps, converting the file from KML to TXT, and then rendering it as an SVG to plot the animated car positions.\n\nChallenges: The primary technical challenge involved accurately mapping the circuit and syncing the car movement. I had difficulty extracting the PNG from the supplied files, so I manually traced the circuit and converted the file to an SVG format for web rendering. Furthermore, I had to precisely \"clip\" the car models and program the simulation to ensure cars not only flowed around the track but also realistically slowed down at bends, adding complexity to the data synchronisation based on the start and end times provided in the CSV.\n\nAccomplishments: I am most proud of developing an elegant and highly functional solution. I created a clean, simple interface using a sans-serif font, optimised for rapid readability on a small screen under high pressure. A key accomplishment is the Actionable AI Insight system, which not only displays data but translates it into an immediate, justified action plan for the race engineer. I also successfully achieved a realistic simulation, showing cars moving and decelerating accurately on the custom-rendered track.\n\nWhat we learned: I gained significant experience in the complexities of converting real-world geographic data into functional web visualisations. This project reinforced the need for custom solutions, such as tracing the track, when provided map files are challenging to integrate. I learned how to effectively process time-series race data to generate high-value strategic recommendations based on factors like tyre status and race positioning, emphasising the need for clarity and speed in the user experience.\n\nWhat's next: The next steps involve moving beyond the current simulation to a fully live system. The priority is to link the dashboard to a live racing API so the telemetry data reflects what the car is actually doing in real-time. Future enhancements include refining the AI for advanced predictive modelling, such as forecasting competitor tyre wear, and developing features to allow the engineer to monitor and manage multiple cars simultaneously.",
    "hackathon": null,
    "prize": "Winner Best of Real-time analytics",
    "techStack": "csv, gemini, google, javascript, three.js, txt",
    "github": null,
    "youtube": null,
    "demo": "https://gr86strategy.netlify.app/",
    "team": "Danielle Royer",
    "date": "2025-11-23",
    "projectUrl": "https://devpost.com/software/gr-cup-real-time-dashboard-simulator"
  },
  {
    "title": "Little Critters - Advanced hand tracking update",
    "summary": "Little Critters is an innovative project that leverages advanced hand tracking technology to create an immersive gaming experience. Players interact with adorable, animated creatures in a virtual environment, using their hands to manipulate and engage with the game, enhancing user immersion and interactivity.\n\nKEY ACHIEVEMENTS\nThe project stood out for its seamless integration of advanced hand tracking, providing a unique user experience that sets it apart in the casual gaming category. Its recognition as both the overall winner and the Best Casual Game Runner-up indicates a strong execution of gameplay mechanics and a captivating user interface that resonated with judges and players alike.\n\nLittle Critters utilizes cutting-edge tools such as Meta's tracking technology and the Unity game engine, allowing for precise hand movements and interactions within the game. The implementation of real-time gesture recognition enhances gameplay fluidity, and the use of MRUK (Meta‚Äôs Reality and User Knowledge) frameworks contributed to the project's robust performance and user engagement.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring accurate hand tracking across diverse user environments, as well as optimizing the game's performance for various hardware specifications. Additionally, balancing the complexity of interactions while maintaining an intuitive user experience would have been critical during development.\n\nFUTURE POTENTIAL\nLittle Critters has significant potential for evolution into a broader platform that could incorporate more complex gameplay elements, such as multi-player interactions or additional creature behaviors. Future updates could explore virtual reality social experiences or educational components, allowing it to appeal to a wider audience and fostering community engagement.\n\nWhat it does: This update to Little Critters improves hand tracking support, taking things to the next level: Scene tracked surfaces including walls, floors and ceilings are used for extra context, e.g. players can squash enemies between their hands and a real world surface. Hitting at a grazing angle to the surface will knock enemies off the surface instead of squashing them.\nWe reworked all hand interactions including picking up, grabbing and throwing to be more accurate and responsive.\nAdded gestures and interactions that are only available when playing with hand tracking.\n\nInspiration: Last month we released Little Critters, a casual tower defense game built from the ground up for mixed reality. We designed the game around cutting edge mixed reality features including scene tracking to drive the gameplay and depth occlusion to blend the virtual with the real. Hand tracking is a great tool for immersing players in mixed reality. For the Meta Horizon Start competition we chose to build an advanced hand tracking update that refines and extends the hand tracking support in Little Critters. Our aim was to make it a showcase of what is possible and one of the best examples of hand tracking on the Meta platform.\n\nHow it was built: We developed a custom gesture recognition system for greater accuracy to support the needs of the game. The system is calibrated with multiple different hand poses which then drives the game's general input system. Special rules cater to tracking loss, so that carried items aren't dropped if the player's hands go out of view of the tracking cameras. In addition, we developed different responses to physics collisions between the player's fingers and the enemy critters. The orientation of nearby real world surfaces from scene tracking and the velocity of the player's hand are combined to provide the appropriate response in the critter's behaviour.\n\nChallenges: In playtesting we noticed that when players pick up virtual objects, they use different hand poses, e.g. a thumb + index finger pinch, closing the fingers in a 'crab claw' pose, or a full fist grab. We initially expected played to use a pinch gesture, so we needed to refine our grab system to support these multiple different hand shapes.\n\nAccomplishments: We are especially proud of how we have managed to combine hand tracking with scene tracking to build an extensive range of new interactions that goes beyond the standard pinch and poke gestures.\n\nWhat we learned: Our team learned to look beyond simply creating a drop in replacement to controllers, looking at ways to explore how hand tracking can offer new, unique interactions for the player. We also learned how to adapt to player's needs as they have different expectations for how they should be able to use their hands to interact with virtual objects.\n\nWhat's next: We are continuing to build content updates for Little Critters and will be regularly updating the title on the store!",
    "hackathon": null,
    "prize": "Winner Best Casual Game Runner-up",
    "techStack": "meta, mruk, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=i2qJPLwJRSw",
    "demo": "https://www.meta.com/experiences/little-critters/8692631140830732/",
    "team": null,
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/little-critters"
  },
  {
    "title": "Spell Swiper: Defend the Kingdom!",
    "summary": "Spell Swiper: Defend the Kingdom! is an interactive game designed to engage players in a magical defense scenario where they must use spells to protect their kingdom from invading forces. The project combines elements of strategy and action, allowing players to cast spells dynamically to thwart enemies.\n\nKEY ACHIEVEMENTS\nThe project won the top prize at the hackathon, likely due to its innovative gameplay mechanics and engaging user experience. Its unique blend of spell-casting and defense strategy, along with polished visuals and gameplay, distinguished it from other entries, showcasing both creativity and technical expertise.\n\nSpell Swiper was built using Blender for 3D modeling, Horizon for game development, and Noesis for user interface design. The integration of these tools enabled the team to create high-quality visuals and a smooth user experience, while the gameplay mechanics were enhanced through effective spell-casting algorithms and real-time enemy interactions.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to balancing gameplay mechanics to ensure both fun and difficulty. Additionally, optimizing performance for different devices and ensuring a seamless experience across various platforms may have posed technical hurdles. Collaboration and time management during the hackathon could also have been significant factors.\n\nFUTURE POTENTIAL\nSpell Swiper has the potential to evolve into a more comprehensive gaming experience by expanding its storyline, introducing multiplayer modes, and incorporating additional spell types and enemy variations. Future updates could also include seasonal events or customizable character options to enhance player engagement and retention.\n\nWhat it does: In Spell Swiper: Defend the Kingdom! monsters drift toward your castle on enchanted balloons after being launched in the air by a nearby catapult. Each balloon has a weakness though, a shape that you can cast to pop them! This could be a circle, line, or zigzag shape, for instance. The core loop becomes a quick, satisfying rhythm of gesture ‚Üí feedback ‚Üí explosion as enemies flood in faster and faster. It‚Äôs simple to learn, but as more shapes and enemy variations appear, it transforms into a game of timing, awareness, and mastery. Tip: If you're having issues registering the straight line, vertical or horizontal, make sure that you make a longer line. It doesn't register short gestures very well.\n\nInspiration: Spell Swiper: Defend the Kingdom! started with a simple question: What if spell-casting was as satisfying as slicing fruit? We wanted a game that anyone could instantly understand: draw the shape, cast the spell, save the castle, but with enough hidden depth to reward skill and mastery. Mixing the kinetic fun of Fruit Ninja with the nostalgia of old-school rune-drawing games, we focused on creating a one-handed mobile experience that still makes you feel powerful. Examples of our iteration progress:\n\nHow it was built: The game was developed entirely using Horizon Worlds‚Äô World Editor, with Noesis UI powering the title screen and leaderboard. Our earliest prototype was hilariously forgiving, any scribble counted as a ‚Äúspell,‚Äù which was fun for a minute but not a real game. So we built a custom gesture recognizer from scratch and iterated over it dozens of times until it felt fair, readable, and reliable. Once the core mechanic clicked, we layered in particle effects, floating animations, polished hit reactions, and a difficulty curve that moves from relaxing to chaotic. Light, optimized scripting kept things smooth on mobile, and we designed the entire experience around portrait mode for a true mobile-first feel.\n\nChallenges: Gesture recognition was our biggest boss battle. Tiny differences in line curvature or stroke order could make the game feel too strict or too easy. Balancing that element took a lot of playtesting. We also had to rethink traditional VR-friendly Horizon Worlds layouts because portrait mode compresses everything vertically. Reading action and incoming threats in a tall, narrow frame required major adjustments to camera placement, enemy spacing, and timing. Keeping the game visually exciting without tanking mobile performance is another ongoing challenge.\n\nAccomplishments: We created a spell casting system that feels magical. Shapes read cleanly, spells fire instantly, and enemies burst with satisfying feedback. New players can \"swipe-mash\" and survive for a while, but skilled players can hit a flowing rhythm that feels almost like drawing calligraphy. We‚Äôre also proud of pushing Horizon Worlds into a more mobile-native, portrait-optimized experience. It‚Äôs a platform where this type of game isn‚Äôt common, and seeing it run smoothly feels like opening a new door. We're eager to continue to explore the possibilities there.\n\nWhat we learned: We learned that gesture-based gameplay is incredibly sensitive; changing a circle‚Äôs tolerance by even a few pixels changes how ‚Äúfair‚Äù the game feels. We discovered how critical animation timing is, especially when players only have a small window of time to take in the input and react to it. Our biggest lesson? Simple ideas become great when you cut away the noise and sharpen the core fun, and hackathons should keep a focused scope in mind. Our previous submissions were larger in scope, but this time we focused on a tight, instantly enjoyable mechanic and that shift made a huge difference.\n\nWhat's next: We have a lot of ideas queued up: new enemy types, advanced shapes, and boss encounters that require chained gestures or multi-step spell combos. A progression system with unlockable spells and cosmetic upgrades is high on our list, as well as daily challenges and score competitions to keep players returning. Long term, we‚Äôd love to explore co-op spell slinging or competitive swipe battles. Spell Swiper: Defend the Kingdom! was intentionally built small and clean, which makes it the perfect foundation for future expansions.",
    "hackathon": null,
    "prize": "Winner Top Prize",
    "techStack": "blender, horizon, noesis",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EayyOqGQxKM",
    "demo": "https://horizon.meta.com/world/10102098126185425",
    "team": "Scripting and moral support, 3D assets, QA",
    "date": "2025-12-08",
    "projectUrl": "https://devpost.com/software/spell-swiper"
  },
  {
    "title": "R3KON GPT",
    "summary": "R3KON GPT Project Summary\n\nR3KON GPT is an innovative application that leverages artificial intelligence and machine learning to enhance user interaction with data. By integrating advanced natural language processing capabilities, the project aims to provide insightful analysis and recommendations based on user queries, making complex data more accessible and actionable.\n\nKEY ACHIEVEMENTS: The project stood out due to its seamless integration of various AI components, showcasing an impressive ability to generate accurate responses and insights from complex datasets. Its user-friendly interface developed with Tkinter further contributed to its success, making it approachable for users with varying levels of technical expertise, which ultimately led to it winning the hackathon.\n\nNotable technical implementations include the use of Qwen for natural language understanding, allowing the system to accurately process and respond to user input. The deployment of Python for the backend logic ensures scalability and flexibility, while the Tkinter framework provides an intuitive graphical user interface that enhances user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data accuracy and model training, as ensuring the AI provides reliable and contextually relevant information can be complex. Additionally, user interface design challenges may have arisen, requiring careful consideration to balance functionality with usability.\n\nFUTURE POTENTIAL: R3KON GPT has significant potential for evolution, including expanding its capabilities to support multi-language processing or integrating with other data sources for broader applicability. Enhancements in machine learning algorithms could also lead to improved accuracy and faster response times, while potential collaborations with industry partners could facilitate its deployment in various sectors, such as education, healthcare, or business analytics.\n\nWhat it does: R3KON GPT is a cybersecurity-focused offline AI assistant designed for low-connectivity regions.\nIt can:\nGive cybersecurity tips and safe-practice guidance Help detect suspicious behavior and give threat-awareness advice Function fully without Wi-Fi or stable internet Run as a standalone Windows .exe so anyone can use it instantly Offer general AI responses while prioritizing digital safety It‚Äôs built for users who need protection, not data drain.\n\nInspiration: In many parts of Africa, internet access is expensive, unstable, or completely unavailable , but cybersecurity risks are still growing. I wanted to build an AI tool that doesn‚Äôt depend on constant connectivity, doesn‚Äôt expose user data to the cloud, and can provide real, practical guidance offline. That was the spark behind R3KON GPT: an AI designed for areas where cybersecurity support is needed most, but connectivity is weakest.\n\nHow it was built: Python for core logic\nAn offline-capable model packaged directly into the app\nMultiple cybersecurity-focused prompt layers and logic modules\nA Windows build system that bundles everything into one .exe\nLocal processing so users don‚Äôt need cloud access or API keys\nThe large file size comes from embedding all required AI and security components directly inside the executable ‚Äî making it fully functional offline.\n\nChallenges: Managing the huge file size caused by offline AI models Packaging everything into a single .exe without dependency errors Ensuring the app runs on multiple Windows setups without missing DLLs Designing the AI to be helpful, safe, and security-aware offline Sharing the release safely through Google Drive without exposing personal files\n\nAccomplishments: Creating an AI tool that works fully offline, something rare for cybersecurity assistants Building and shipping a functional 1.3GB AI executable at 19 Designing a system that can help users in areas with no Wi-Fi Adding another major product under Aethar Tech Proving that African-first AI tools can be powerful and independent\n\nWhat we learned: Offline AI is powerful, but packaging it is challenging Cybersecurity tools need careful design and clear, safe guidance Building for low-connectivity environments requires a different mindset Managing large builds and dependencies is its own skill Releasing software safely matters as much as building it\n\nWhat's next: A lighter version with modular downloads Stronger offline security features (local threat scanning, phishing detection) A cleaner UI and more stable packaging Mobile and web versions Integration with Nyx Browser for secure browsing + AI support Partnerships and pilots in schools, cyber clubs, and rural areas Turning R3KON GPT into a complete offline cybersecurity toolkit",
    "hackathon": null,
    "prize": null,
    "techStack": "artificial-intelligence, machine-learning, python, qwen, tkinter",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=CSFLJeiS6yk",
    "demo": "https://aethartech.itch.io/r3kon-gpt",
    "team": null,
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/r3kon-gpt"
  },
  {
    "title": "The Twin Earth",
    "summary": "The Twin Earth Hackathon Project Summary\n\nThe Twin Earth is an innovative multimedia project that likely explores the intersection of visual storytelling and audio-visual technology, leveraging AI-driven tools to create immersive experiences. Although specific details are not provided, the combination of film and sound elements suggests a narrative-driven approach to engaging users in a creative format.\n\nKEY ACHIEVEMENTS: The project stood out by securing multiple prizes, including 1st Place in the Film category and 3rd Place as a Gold Sponsor winner. Its ability to resonate with judges through originality and execution likely contributed to its recognition and success among competing projects.\n\nThe project utilizes several advanced tools, including DaVinci Resolve for video editing, ElevenLabs for audio generation, Epidemic Sound for music licensing, Higgsfield for data handling, Klingai for design, and Topaz Studio for image enhancement. This diverse tech stack showcases a sophisticated approach to content creation, integrating AI and advanced editing software to elevate the final product.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges such as coordinating complex multimedia elements, ensuring seamless integration between different tools, and managing time constraints typical in hackathon environments. Additionally, achieving a coherent narrative that effectively utilized the diverse technologies could have posed a significant hurdle.\n\nFUTURE POTENTIAL: The Twin Earth has the potential to evolve into a full-fledged multimedia platform or series, expanding its narrative universe and engaging a wider audience. Future developments could include interactive experiences, partnerships with content creators, or leveraging emerging technologies like AR/VR to enhance user engagement and storytelling depth.",
    "hackathon": null,
    "prize": "Winner Film ‚Äì 1st Place; Winner Gold Sponsor ‚Äì 3rd Place",
    "techStack": "davinciresolve, elevenlabs, epidemicsound, higgsfield, klingai, topazstudio",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=rxEnjG6GjfM",
    "demo": null,
    "team": null,
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/the-twin-earth"
  },
  {
    "title": "Vyx‚Äî AI-Powered Universal Content Repurposing Engine",
    "summary": "Vyx is an AI-powered engine designed to streamline the process of content repurposing across various formats and platforms. By leveraging advanced algorithms, it automates the transformation of existing content into new formats, making it easier for creators and marketers to reach diverse audiences efficiently.\n\nKEY ACHIEVEMENTS\nVyx stood out by winning both the Nexus Champion (Grand Prize) and the overall winner title at the hackathon, showcasing its innovative approach to content repurposing. Its ability to significantly reduce the time and effort required to adapt content across multiple channels likely impressed judges, as it addresses a common pain point for content creators and marketers.\n\nThe project integrates several cutting-edge technologies: \n- AI for intelligent content analysis and transformation.\n- FFmpeg for efficient media processing and format conversion.\n- n8n as a workflow automation tool, enabling seamless integration of various services.\n- Next.js for building a responsive and user-friendly front-end.\n- Pollination for content sourcing and enrichment.\n- yt-dlp for video downloading and repurposing capabilities.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring the quality and relevance of the repurposed content, particularly in maintaining engagement across different formats. Additionally, integrating multiple technologies smoothly and handling various media types and formats may have posed technical hurdles.\n\nFUTURE POTENTIAL\nVyx has significant potential for evolution by expanding its capabilities to include more advanced AI features, such as sentiment analysis and targeted content suggestions. Future developments could also focus on creating a user-friendly interface that allows non-technical users to harness its power, as well as integrating with more platforms for seamless content distribution. Additionally, incorporating analytics to measure the performance of repurposed content could further enhance its value to users.\n\nWhat it does: Vyx transforms any YouTube video into a complete multi-platform content package in under 10 minutes:\n\nChallenges: Dual Pipeline Coordination - Users triggering both pipelines simultaneously caused resource conflicts. We implemented separate webhook endpoints with dedicated node paths and health check monitoring. FFmpeg Subtitle Timing - Clips from middle of videos had incorrect timestamps. AI now re-generates subtitles starting from 00:00:00 for each clip, ensuring perfect sync. Cloudinary Rate Limiting - Uploading 5 clips simultaneously triggered limits. We implemented sequential processing‚Äîclips process one at a time through upload stage. Temporary File Cleanup - Failed workflows left gigabytes of video files. Added mandatory cleanup nodes that execute regardless of workflow success. CORS Preflight Handling - Browser OPTIONS requests were blocked. Created dedicated OPTIONS webhook handlers with prop\n\nWhat we learned: Multi-Pipeline Architecture is Powerful - Building two distinct pipelines within one system taught us that modular design allows users to choose their workflow while sharing common infrastructure. AI Quality Depends on Multi-Step Processing - Moving from a single AI call to a 5-step pipeline increased content quality by 95%. Each step builds on the previous, creating compound improvements. Speed Matters - Switching to Pollinations.ai (instant) reduced total pipeline time by 40%. Users value speed over marginal quality differences‚Äî5-7 minutes vs 10-15 minutes dramatically improves UX. Video Processing is Resource-Intensive - Temporary file management is critical, batch processing needs rate limiting, and 9:16 vertical format requires precise cropping calculations. N8N Scales for Production\n\nWhat's next: A/B Testing Recommendations using historical performance data\nScheduled Content Calendar for entire month\nLive Video Processing for ongoing livestreams\nAnalytics Dashboard tracking AI-generated content performance\nPodcast Support (Spotify, Apple Podcasts)\nTeam Collaboration with role-based access",
    "hackathon": null,
    "prize": "Winner Nexus Champion (Grand Prize)",
    "techStack": "ai, ffmpeg, n8n, next, pollination, yt-dlp",
    "github": "https://github.com/NabilThange/omni",
    "youtube": "https://www.youtube.com/watch?v=WMODqeRTyLM",
    "demo": "https://vyx.vercel.app/",
    "team": "Yojith Rao",
    "date": "2025-11-30",
    "projectUrl": "https://devpost.com/software/omni-ai-powered-universal-content-repurposing-engine"
  },
  {
    "title": "AI enterprise platform",
    "summary": "The AI enterprise platform leverages advanced machine learning and AI technologies to streamline business operations, enhance decision-making, and improve overall efficiency within organizations. By integrating various tools and frameworks, the platform provides customizable solutions tailored to specific enterprise needs.\n\nKEY ACHIEVEMENTS\nThis project stood out due to its innovative integration of AI capabilities with user-friendly interfaces, allowing for seamless interaction between users and complex data systems. Winning both the main prize and 2nd place indicates exceptional execution and a strong presentation of its potential impact on enterprise solutions.\n\nThe platform effectively utilizes FastAPI for building high-performance APIs, enabling rapid development and deployment. By incorporating React, it offers a dynamic and responsive user interface, while agentcore and bedrock likely provide robust AI functionalities, enhancing the platform's adaptability and intelligence in processing enterprise data.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring data security and privacy, especially when handling sensitive enterprise information. Additionally, integrating diverse technologies (like Python with FastAPI and React) could lead to compatibility issues, requiring careful management and testing throughout the development process.\n\nFUTURE POTENTIAL\nThe project has significant potential for evolution, including the expansion of its AI capabilities to incorporate predictive analytics and automated decision-making tools. Future iterations could also explore integration with other enterprise software systems and expand its user base by creating tailored solutions for different industries, driving further innovation in the enterprise sector.",
    "hackathon": null,
    "prize": "Winner 2nd Place",
    "techStack": "agentcore, bedrock, fastapi, python, react",
    "github": "https://github.com/pramodthe/enterprise-ai-platform/tree/main",
    "youtube": "https://www.youtube.com/watch?v=d6G1MZlqGzI",
    "demo": "https://enterprise-ai-landing-page.netlify.app/",
    "team": "pramod thebe",
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/ai-enterprise-platform"
  },
  {
    "title": "LeetCourt",
    "summary": "LeetCourt Project Summary\n\nLeetCourt is a platform designed to enhance the coding interview process by providing a collaborative environment for candidates and interviewers. It likely integrates real-time coding challenges with feedback mechanisms, leveraging AI to simulate coding interviews and assess candidates' skills.\n\nKEY ACHIEVEMENTS: LeetCourt stood out by winning multiple awards, including the overall winner and the best use of ElevenLabs, indicating its innovative use of AI technologies and robust user experience. Its unique approach to coding interviews likely impressed judges, showcasing its potential to streamline and improve hiring processes.\n\nThe project employs a sophisticated tech stack, including Node.js for backend services, React for a dynamic frontend, and TypeScript for type safety. Additionally, the integration of ElevenLabs suggests advanced AI capabilities, possibly for code analysis or real-time feedback, while Tailwind CSS ensures a modern and responsive design.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in creating a seamless user experience that balances complexity and usability. Additionally, ensuring the AI components provide accurate and relevant feedback while maintaining engagement in the interview process may have required extensive testing and refinement.\n\nFUTURE POTENTIAL: LeetCourt could evolve into a comprehensive platform for tech recruitment, incorporating features like personalized interview coaching, analytics for hiring trends, and partnerships with educational institutions to prepare candidates. Expanding its capabilities to offer mock interviews and skill assessments could further enhance its utility in the job market.\n\nWhat it does: LeetCourt is an AI-driven, voice-interactive courtroom simulator that allows users to: Conduct realistic Opening, Direct, Cross, and Closing phases.\nFace live objections such as Relevance, Hearsay, Speculation, and Leading.\nUpload any PDF case file and have an AI extract facts, evidence, and precedents.\nReceive performance analysis at the end of the game.\nTrack clarity, logic, persuasiveness, and precedent usage.\nInteract with a fully voice-enabled multi-agent AI judge, Lawyer, and Orchestrator built using ElevenLabs Conversational AI.\nAccess an integrated tools panel with evidence, notes, and case summaries.\nCreate secure, isolated development sandboxes for AI-powered legal case analysis using Daytona.io \nReceive automated code reviews and logic checks for scoring engine and agent scripts\n\nInspiration: Legal argumentation is a skill that traditionally requires in-person training, expert feedback, and structured courtroom environments, none of which are easily accessible to most students or professionals. Despite the rise of AI tools for writing and research, there is still no platform that allows users to practice real-time courtroom reasoning, face objections, or learn procedural flow in a fun and engaging way. We wanted to build something that feels like LeetCode for legal reasoning: a place where anyone can practice arguments, improve rhetorical clarity, and simulate real trial pressure with an AI judge that behaves like the real thing. This project was inspired by a gap we repeatedly saw among law students, debaters, and public speakers: they have theory, but no way to practice execu\n\nChallenges: Latency management: synchronizing real-time speech with LLM responses required heavy optimization and caching.\nMaintaining consistent judge behavior: the judge must be strict, procedural, and predictable‚Äîrequiring extensive prompt engineering and reinforcement rules.\nHandling messy PDFs: scanned or irregular files often broke extraction, forcing us to implement fallbacks and preprocessing.\nConversation drift: without a strict FSM, courtroom phases would blur and confuse the model; designing the conversation structure was non-trivial.\nIntegrating multiple models: coordinating judge logic, scoring, extraction, and agent workflows without conflicting context required careful orchestration.\n\nAccomplishments: Built a fully voice-interactive courtroom simulator with realistic objections.\nCreated a dynamic PDF-to-case extraction pipeline that works on nearly any legal document.\nSuccessfully implemented a real-time scoring engine that analyzes user arguments every 3 seconds.\nDeveloped a clean, intuitive UI that mirrors professional trial environments.\nDesigned a flexible case library that supports search, filtering, uploading, deletion, and metadata.\nCreated a scalable architecture that can support new agents, new case types, and multiplayer scenarios.\nLeveraged Daytona.io to create secure, isolated development sandboxes for AI-powered legal case analysis.\nUsed CodeRabbit to automate code quality checks and ensure scoring logic correctness.\nTigris Storage for storing legal case documents for pract\n\nWhat we learned: Real-time voice interactions require more than LLM prompts‚Äîthey need architectural discipline, buffering, and predictable flow control.\nLegal reasoning benefits from structure: an FSM model greatly improves accuracy and realism.\nLLMs perform best when each is assigned a specific, narrow function (judge, analyzer, extractor).\nUser experience matters: trial simulations need pacing, clarity, and visual anchors to feel authentic.\nHigh-quality prompts aren‚Äôt enough‚Äîenvironment design determines how well agents behave.\n\nWhat's next: Multiplayer Mode: human vs. AI opposing counsel or human vs. human with an AI judge.\nMobile App for on-the-go argument practice.\nCommunity Case Marketplace for user-submitted scenarios.\nInstitutional versions for law schools, debate clubs, and training programs.\nAdvanced scoring models that learn from user patterns.\nAnalytics dashboard showing improvement over time.\nScenario-based learning tracks (Criminal Law, Torts, Contracts, Evidence, Constitutional Law). LeetCourt aims to become the first platform that makes courtroom reasoning truly accessible, interactive, and measurable.",
    "hackathon": null,
    "prize": "Winner 3rd Prize; Winner Best Use of ElevenLabs",
    "techStack": "coderabbit, daytona, elevenlabs, node.js, openrouter, react, tailwind, typescript",
    "github": "https://github.com/ayushs2k1/leetcourt/tree/leetcourt-daytona-sprint-hack-nyc",
    "youtube": "https://www.youtube.com/watch?v=VFsn7VtvEpU",
    "demo": "https://luminous-fox-3d91e8.netlify.app/",
    "team": "Ayush Sharma",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/leetcourt-unej6r"
  },
  {
    "title": "AUTO-OPS: AI DevOps Agent",
    "summary": "AUTO-OPS: AI DevOps Agent is an innovative solution designed to automate DevOps processes using advanced AI technologies. It streamlines software development and deployment workflows, enhancing efficiency and reducing the need for manual intervention by leveraging integrations with various APIs and development tools.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively demonstrating the seamless integration of multiple tools and APIs, showcasing its capability to optimize DevOps tasks. Winning the \"Best Use of CodeRabbit\" prize highlights its innovative use of the platform to enhance code quality and deployment efficiency, contributing to its overall recognition as a winner in the hackathon.\n\nNotable technical implementations include the use of the Anthropic Claude API for AI-driven insights, integration with GitHub for version control, and the deployment of a responsive frontend built with Next.js and React. The project also utilized Tailwind CSS for styling, ensuring a modern and user-friendly interface, while leveraging TypeScript and Node.js for robust backend development.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring compatibility among the diverse set of technologies used, managing API rate limits, and achieving high performance in real-time data processing. Additionally, integrating AI effectively into existing DevOps workflows without disrupting established processes could have posed significant hurdles.\n\nFUTURE POTENTIAL\nAUTO-OPS has significant potential for evolution, including expanding its capabilities to support more DevOps tools and platforms, enhancing AI algorithms for predictive analytics, and incorporating user feedback for continuous improvement. Furthermore, it could evolve into a comprehensive platform that not only automates tasks but also provides insights and recommendations for optimizing DevOps practices.\n\nWhat it does: AUTO-OPS automatically detects runtime errors from any Python project, analyzes the code, generates a patch, tests it safely in a Daytona sandbox, and opens a pull request ‚Äî all without human intervention.\nIt logs traces in Galileo, generates code and reasoning using Anthropic‚Äôs Claude, tests fixes inside Daytona environments, stores everything in Tigris, gets CodeRabbit reviews automatically, and even speaks a summary of what it fixed using ElevenLabs. In short, it‚Äôs an autonomous software engineer that handles your crashes end-to-end.\n\nInspiration: We wanted to build something that eliminates the ‚Äúdebug fatigue‚Äù developers face ‚Äî when your code breaks and you spend hours tracing logs, fixing one bug just to introduce another. With AI tools evolving fast, we saw an opportunity to let an autonomous agent not only understand code errors but actually fix them safely in real time. That vision became AUTO-OPS, an AI DevOps agent that acts like your tireless coding teammate.\n\nHow it was built: Frontend: Next.js + Tailwind dashboard showing incoming errors and a live ‚Äúagent console‚Äù timeline.\nBackend: Node/TypeScript API routes handling error intake, Claude prompts, Daytona sandbox orchestration, and GitHub PR creation.\nPython client: A lightweight wrapper that catches exceptions and POSTs traceback + source code to AUTO-OPS.\nAPIs integrated:\n\nüß† Anthropic Claude ‚Üí analyzes stack traces & generates code patches\nüß∞ Daytona ‚Üí spins up isolated test environments and runs patched code\nüêá CodeRabbit ‚Üí automatically reviews created PRs\nüìà Galileo ‚Üí logs every crash trace and agent decision\nüíæ Tigris Data ‚Üí stores incidents, logs, and patches for long-term memory\nüó£Ô∏è ElevenLabs ‚Üí turns the AI‚Äôs summary into a spoken ‚Äúfix report‚Äù Everything runs through a coordinated agent loop managed\n\nChallenges: Coordinating six separate APIs under one asynchronous workflow.\nKeeping Claude‚Äôs reasoning ‚Äúsafe‚Äù so it never writes destructive patches.\nManaging Daytona environment creation and teardown efficiently under time limits.\nGetting real-time error feeds to sync smoothly between Python and the dashboard.\nHandling API rate limits and secret management without breaking demo reliability.\n\nAccomplishments: Built a fully autonomous debugging pipeline that touches every partner API.\nAUTO-OPS can realistically detect, fix, test, and commit a live Python error.\nThe project runs end-to-end in minutes with clear transparency via the console UI.\nCreated a voice-enabled demo where the AI literally talks you through the fix.\nClean architecture that other developers can extend to JavaScript or Go next.\n\nWhat we learned: How to combine multiple AI systems ‚Äî Claude, CodeRabbit, ElevenLabs ‚Äî into a cohesive workflow that doesn‚Äôt collapse under API latency.\nThe importance of sandboxed testing before applying AI-generated code changes.\nThat strong observability (Galileo + Tigris) is essential when you give autonomy to code-writing agents.\nBuilding fast is easy; building safe and explainable AI systems is the real challenge.\n\nWhat's next: Introduce real-time VS Code integration so the agent appears as an in-editor assistant.\nBuild team dashboards for organizations to monitor agent performance across repos.\nIntegrate LLM fine-tuning using historical Tigris logs to make AUTO-OPS learn your project‚Äôs coding style.\nExpand sandbox orchestration to CI/CD pipelines for true self-healing infrastructure. AUTO-OPS isn‚Äôt just a hackathon project ‚Äî it‚Äôs a prototype for the future of hands-free software maintenance.",
    "hackathon": null,
    "prize": "Winner Best Use of CodeRabbit",
    "techStack": "anthropic-claude-api, coderabbit, daytona, elevenlabs-api, github, intel-galileo, next.js, node.js, python, react, rest, tailwind-css, tigris-data, typescript",
    "github": "https://github.com/Soulemane12/AUTO-OPS-AI-DevOps-Agent",
    "youtube": "https://www.youtube.com/watch?v=iV6OdexdXec",
    "demo": "https://auto-ops.vercel.app/",
    "team": "Soulemane Sow",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/auto-ops-ai-devops-agents"
  },
  {
    "title": "Virgo's Whisper AI (VWAI)",
    "summary": "Virgo's Whisper AI (VWAI) is an advanced voice recognition and transcription tool that leverages artificial intelligence to convert spoken language into written text accurately and efficiently. The project aims to enhance communication accessibility and facilitate seamless interaction between users and technology, making it particularly beneficial for individuals with hearing impairments or those in noisy environments.\n\nKEY ACHIEVEMENTS\nVWAI stood out in the hackathon for its innovative integration of multiple AI technologies and its user-friendly interface. The project won the competition due to its high accuracy in transcription, responsive design, and the ability to handle various accents and dialects, thereby providing a versatile solution for a diverse user base.\n\nNotable technical implementations include the use of assemblyai for real-time voice recognition, integration with Eleven Labs for natural-sounding voice synthesis, and deployment via Flask for a robust backend. The project also utilizes Firebase for real-time database capabilities and authentication, ensuring smooth and secure user interactions.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing the transcription accuracy across different languages and accents. Ensuring low latency during real-time processing and managing server load during peak usage times may have also posed significant hurdles. Additionally, addressing privacy concerns surrounding voice data and implementing effective security measures would have been critical.\n\nFUTURE POTENTIAL\nVWAI could evolve into a multi-language transcription tool that serves a broader global audience, expanding its capabilities to include more languages and dialects. Future enhancements could also involve integrating more advanced machine learning algorithms for improved context understanding and sentiment analysis. Additionally, potential partnerships with educational institutions and businesses could lead to its adoption as a standard tool for accessibility in various sectors.",
    "hackathon": null,
    "prize": null,
    "techStack": "ai, assemblyai, cerebras, cerebras-cloud-sdk, css3, elevenlabs, es6+), firebase, firebase-admin, firestore, flask-cors, generation:, javascript, python-3-flask-html5, python-dotenv, pythonanywhere, transcription, voice",
    "github": "https://github.com/Valhallanxxx/Virgo-s-Whisper-AI.git",
    "youtube": "https://www.youtube.com/watch?v=kPvWO0YECD0",
    "demo": null,
    "team": "Shashwat Balodhi",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/virgo-s-whisper-ai-vwai-5isedc"
  },
  {
    "title": "AILien",
    "summary": "AILien Project Summary\n\nAILien is an immersive entertainment experience that combines artificial intelligence and virtual reality to create interactive narratives. It engages users by allowing them to interact with AI-driven characters and environments, enhancing storytelling through real-time user input and feedback.\n\nKEY ACHIEVEMENTS: AILien stood out by winning the \"Best Immersive Entertainment Experience\" award by Meta, highlighting its innovative approach to blending AI with VR. Its unique user engagement and the ability to create personalized experiences likely contributed to its recognition and success in the hackathon.\n\nThe project was built using C# and integrated with Convai for natural language processing, enabling dynamic conversations with AI characters. Additionally, it utilized Meta's SDK and VR Optimiser by Valem to optimize the virtual reality experience, ensuring smooth and immersive interactions.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to ensuring real-time responsiveness of AI interactions, maintaining a seamless VR experience without latency, and balancing user engagement with narrative coherence. Furthermore, integrating multiple technologies effectively could have posed technical hurdles.\n\nFUTURE POTENTIAL: AILien has significant potential for evolution by expanding its narrative scope and character depth, allowing for more complex storylines and user choices. Future iterations could incorporate multiplayer interactions or adaptive learning algorithms to personalize experiences even further, making it a versatile platform for various entertainment applications, including gaming and education.\n\nHow it was built: We used the Convai NPC AI Engine to create a voice character that had the personality and responses of a cosmic oracle. We used MetaSDK incorporate this into a Unity scene. We also used the VR Optimiser by Valem to speed up the final stages of development. The assets include 3D scans of the actual Knyahinya Meteor, environmental design assets using hand drawn sketches that were converted to 3D models using AI and an original score and sound design.\n\nChallenges: The first challenge we encountered was controlling the scope of the project. The ideas for narrative and interactive elements kept expanding and so we had to focus and make sure that we were aligned on the core features of the experience. We had to adapt to the time limits and experience of our team and judge what was realistic within the timeframe without losing the heart of the project. Another key challenge was working with the ConvAI NPC Engine, it doesn‚Äôt immediately integrate with the MetaSDK so with the support of our mentors we had to go into the code and start manually changing elements.\n\nAccomplishments: We‚Äôre really pleased with the level of optimisation we were able to achieve in the amount of time. The NPC AI Engine slowed the experience considerably, creating a loading screen every time the microphone was turned on. Through editing this API and extensive visual optimisation we were able to reduce the delays. This was our first time using the Convai Engine, this took considerable amount time to come to grips with but we‚Äôre really excited to explore this tool further.\n\nWhat's next: We have many ideas for expanding AILien., One step would be to expand the narrative elements of the experience including the descent of the asteroid. This would include more scenes and more complex interactions. We‚Äôre also interested in using spatial anchors to have the meteor align with physical objects in the room e.g the meteor aligns with a beanbag. This would also include using passthrough and reskinning the tracked objects in the room to match the environment e.g pillars become trees. For wider expansion, we would be interested in creating more characters with different personalities that can increase the interaction. We also feel multiplayer could add an interesting layer of interaction.",
    "hackathon": null,
    "prize": "Winner Best Immersive Entertainment Experience by Meta 1st Place",
    "techStack": "c#, convai, metasdk, vroptimiserbyvalem",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=OJZghDpvQsQ",
    "demo": "https://drive.google.com/drive/folders/1uydJcDCLZ0Yim0uzyOA5yEzf2W4CGdLi?usp=share_link",
    "team": "Beth Lewis, Letta Shtohr, Jacqueline Montenegro",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/ailien"
  },
  {
    "title": "[B04] Hander Attack",
    "summary": "Hander Attack is an immersive mixed and virtual reality experience that combines interactive storytelling with advanced graphics. The project leverages state-of-the-art technologies to create a compelling environment where users can engage in unique challenges, enhancing their overall gaming experience.\n\nKEY ACHIEVEMENTS\nThis project garnered recognition as the winner in the Mixed & Virtual Reality Category by Meta, showcasing its innovative approach and high-quality execution. Its standout elements likely include engaging user experience design and technical sophistication, which set it apart from other entries.\n\nHander Attack utilizes a robust tech stack, incorporating tools such as Unity for game development, Blender for 3D modeling, and ElevenLabs for realistic audio generation. The integration of Photon for real-time multiplayer capabilities and MetaSDK for VR functionalities demonstrates advanced technical prowess, enhancing both interactivity and visual fidelity.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance across different VR platforms, ensuring seamless user experiences without latency. Additionally, balancing the complexity of the graphics with the accessibility for users who may be new to VR gaming could have posed design considerations.\n\nFUTURE POTENTIAL\nHander Attack has significant potential for evolution, including the expansion of its narrative elements and the introduction of new gameplay mechanics. Future updates could incorporate user-generated content or collaborate with other platforms for cross-play capabilities, further enhancing engagement and community building.\n\nWhat it does: Hander Attack is a mixed reality, hand-tracking multiplayer game for Meta Quest where your own room becomes the arena. First, the app scans your room and generates a minimap of it. Then, both minimaps are swapped, giving each player a miniature representation of the opponent‚Äôs room. Small interactions on the minimap have immediate consequences in the mixed-reality space (and vice versa), turning it into a real-time chase where one player has to hide while the other tries to catch them. The VFX and audio react to events in the room, reinforcing the immersion, social presence, and the fun of the game! The result is a playful bird's-eye view experience where you try to find and catch the other player hiding in their room using their minimap.\n\nInspiration: The original idea of Hander Attack comes from the concept of the digital twin. A digital twin creates a live, data-driven mirror of a physical space, allowing you to monitor, analyse, or even control what happens there. We translated this idea into gameplay through a simplified version of a digital twin: an interactable minimap of a room. Traditionally, minimaps only help you navigate the world, not influence it. In Hander Attack, the minimap becomes the controller, exploring how interacting with a representation of a space can affect the real world.\n\nHow it was built: We built the project using Unity 6, C#, and Universal Render Pipeline for the rendering, gameplay logic and all interaction systems. For room scanning, we relied on the Meta MR Utility Kit, which provided us with a JSON representation of the environment. This representation was then used to reconstruct the minimap and collision geometry. Additional core features such as basic hand tracking and pose recognition were implemented using the Meta Core SDK, and Photon was used to explore multiplayer capabilities. For 3D models and animations, we used Blender along with Unity‚Äôs timeline and Director tools to handle the full asset production pipeline. Hand tracking was introduced using the Meta MR Utility Kit and we implemented pose recognition. For optimized visual effects that collide with Unity\n\nChallenges: Making it feel like it‚Äôs ‚Äúhappening in front of you‚Äù Because the experience is fully based on hand tracking, we couldn‚Äôt rely on controllers or haptics to sell impact. We had to design VFX, audio, and timing very carefully so that every interaction looks and sounds punchy enough to feel physical, even without vibration. Synchronizing room scans between players On the multiplayer side, the most difficult part was synchronizing the scanned minimaps between players. After a room scan, the device generates a JSON file, so we rebuilt the room layout from that JSON and used it as the shared reference. We also needed a reliable way to send controller positions: we first converted the controller‚Äôs world position into the minimap‚Äôs local space, transmitted that data, and then reconstructed the corr\n\nAccomplishments: We are proud of implementing a functioning, fun, and aesthetic game that integrates an end-to-end pipeline from conception to the delivery of a polished prototype. We are proud of how clearly Hander Attack communicates the idea of interacting with a digital twin through a minimap while still being entertaining and engaging.\n\nWhat we learned: Through this project, we learned a lot about how powerful a minimap can be as an interaction tool in mixed reality. It was especially interesting to see how small actions on the minimap could have an immediate impact in another player‚Äôs room in real time, creating a strong sense of shared presence. We also gained a deeper understanding of the design constraints of hand-tracking‚Äìonly experiences. Without haptics, every piece of feedback has to be conveyed through visuals, sound, and timing. This forced us to think more carefully about how to communicate impact, intention, and responsiveness through VFX and audio alone. Finally, working with room scanning, JSON-based reconstruction, and synchronizing local and world spaces across devices taught us a lot about the technical and UX challenges\n\nWhat's next: Looking ahead, the idea of interacting with distant spaces through a minimap opens up a wide range of possibilities. A small virtual model of a remote environment can act as a powerful control interface, allowing users to manipulate objects, guide other people, or coordinate shared tasks without physically being there. This approach could be extended far beyond simple room interactions: remote assistance, collaborative design, education, and even large-scale simulations all become more intuitive when the minimap itself becomes the main tool for shaping the world. By refining the accuracy of room reconstruction and improving cross-device synchronization, we imagine a future where users can seamlessly interact with completely different locations as if they were miniature worlds resting in th",
    "hackathon": null,
    "prize": "Winner Mixed & Virtual Reality Category by Meta Runner-Up",
    "techStack": "adobe, aftereffects, blender, c-sharp, elevenlabs, metasdk, photon, unity, vegaspro",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=moVZ8fKZOOc",
    "demo": null,
    "team": "Roger Montserrat, Alex Fuentes Ravent√≥s, Reiya Itatani, Georgios Tsampounaris, Lisa Izzouzi",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b04-hander-attack"
  },
  {
    "title": "Kitchen Futures ‚Äì Interior Design Showcase Platform",
    "summary": "Kitchen Futures is an innovative platform designed to showcase interior design concepts specifically for kitchen spaces. It enables users to explore various design styles, layouts, and functionalities, allowing them to visualize and personalize their kitchen environments through an interactive interface.\n\nKEY ACHIEVEMENTS\nThe project stands out for its user-centric design and smooth interactive experience, facilitated by the use of framer-motion for animations. Its ability to allow users to easily navigate and visualize kitchen designs contributed to its recognition as the hackathon winner, showcasing both creativity and practical application.\n\nKey technical implementations include the use of React.js for building dynamic user interfaces, Tailwind CSS for responsive and modern design aesthetics, and Vite for a fast development and build process. The integration of ES6+ features enhances code quality and maintainability, while framer-motion provides sophisticated animations that improve user engagement.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring cross-browser compatibility and optimizing performance for a seamless user experience. Additionally, managing the complexity of interactive design features and integrating real-time updates may have posed difficulties during development.\n\nFUTURE POTENTIAL\nKitchen Futures has significant potential for evolution, including the addition of user-generated content, such as design submissions and reviews. Future enhancements could involve augmented reality (AR) features for virtual kitchen tours, partnerships with furniture and appliance brands for product placements, and integration with e-commerce platforms to facilitate direct purchases of showcased items.\n\nWhat it does: The platform professionally showcases: Kitchen makeovers\nInterior decoration concepts\nServices & offerings\nA high-quality image gallery Everything is structured to highlight visuals, build trust, and give users an effortless browsing experience.\n\nInspiration: Interior designers in my region rely heavily on WhatsApp images, outdated layouts, and slow websites that don‚Äôt reflect their craftsmanship. I wanted to build something that feels like 2025: clean, responsive, visually immersive, and conversion-focused.\n\nHow it was built: I built the platform from scratch and focused on: Clean UI/UX architecture\nReusable React components\nSmooth animations and transitions\nPerformance optimization\nFully responsive layouts for all screen sizes I ensured consistent branding, fast loading, and a polished viewing experience for both mobile and desktop users.\n\nChallenges: Maintaining a premium design without sacrificing load time\nHandling large, high-resolution gallery images\nEnsuring visual consistency across all sections\nMaking ultra-wide layouts and mobile designs both look equally strong\n\nWhat we learned: This project strengthened my skills in: Real-world UI/UX decision making\nComponent-driven architecture\nPerformance tuning\nResponsive & accessible design\nManaging client requirements and revisions\n\nWhat's next: Planned upgrades include: 3D kitchen visualizer\nBefore/after interactive slider\nAuto-generated design styles\nAdmin dashboard for easy content updates",
    "hackathon": null,
    "prize": null,
    "techStack": "es6+), framer-motion, javascript, react.js, tailwindcss, vite",
    "github": null,
    "youtube": null,
    "demo": "https://thekitchen-futures.vercel.app/",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/kitchen-futures-interior-design-showcase-platform"
  },
  {
    "title": "Relaxing Garden",
    "summary": "Relaxing Garden Project Summary\n\nRelaxing Garden is an immersive virtual experience designed to provide users with a tranquil environment for relaxation and mindfulness. Leveraging advanced AI and camera access, it creates personalized gardens that adapt to users' preferences, enhancing their overall well-being.\n\nKEY ACHIEVEMENTS: The project stood out by integrating real-time AI-driven customization and visual elements that resonate with users‚Äô emotional states. Winning the AI with Camera Access prize highlights its innovative use of technology to foster relaxation in a unique and interactive manner.\n\nBuilt using Meta‚Äôs technologies alongside Python, Unity, and Quest, the project features seamless integration of AI algorithms for personalized garden creation, real-time camera integration for user interaction, and visually appealing graphics that enhance the immersive experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring smooth AI integration with camera functionalities, optimizing performance for different hardware specifications, and creating a truly engaging user experience that effectively promotes relaxation.\n\nFUTURE POTENTIAL: Relaxing Garden could evolve into a comprehensive mental health tool by incorporating features like guided meditation sessions, social sharing capabilities, and expanded garden customization options. Additionally, it could integrate wearable technology to adapt to users‚Äô stress levels in real-time, fostering an even more personalized experience.\n\nWhat it does: Relaxing Garden translates your heart rate and heart rate variability into a dynamic, mixed-reality environment with a personal touch. Users can draw their own leaf shape on paper, which the system detects and incorporates into the plants that flourish in their AR garden. As you relax, your custom leaves unfurl, new plants sprout, and the digital landscape grows increasingly lush and vibrant. When your arousal levels rise, growth slows, providing intuitive feedback on your body‚Äôs stress response. This experience feels playful, not clinical, making relaxation practices approachable for beginners and engaging for experienced mindfulness users alike.\n\nInspiration: We were inspired by the idea that biofeedback doesn‚Äôt need to feel clinical or intimidating. Augmented reality provides a uniquely soothing space where physiological signals become gentle, visual experiences. We realised that by combining AR with heart rate data, we could transform stress-management sessions into something beautiful, immersive, and truly relaxing. Instead of charts and numbers, users see a garden that reflects their inner state. A calming environment that promotes slow breathing and emotional balance.\n\nHow it was built: We connected a heart rate monitor and used both HR and HRV measurements to calculate a continuous relaxation score. This score reflects moment-to-moment changes in physiological arousal. We developed a lightweight Flask API to stream this data into a Unity-based AR environment, where custom growth shaders and procedural generation logic respond to the user‚Äôs relaxation state in real time. As a result, the garden changes fluidly as the user breathes, focuses, and relaxes.\n\nChallenges: One of the challenges was accurately detecting and rendering the shapes of the leaves. Ensuring the organic outlines stayed crisp, responsive, and natural required fine-tuning both the procedural geometry and the smoothing of sensor input. We also needed to avoid sudden growth jumps, calibrate HRV thresholds for different users, and maintain a tranquil atmosphere, even when data fluctuated rapidly.\n\nAccomplishments: We‚Äôre proud that Relaxing Garden feels peaceful from the moment it loads. It‚Äôs a space where technology truly supports wellbeing. The garden grows in a way that is both vibrant and unobtrusive, and the biofeedback loop functions reliably for all users. We‚Äôre especially pleased with how seamlessly physiological data integrates into the AR visuals, resulting in a calming experience that feels both magical and scientifically grounded.\n\nWhat we learned: We learned how sensitive HR and HRV signals are, and how much careful smoothing and interpretation they need. We also found that users respond strongly to subtle design choices: soft motion, gentle colours, and gradual growth patterns significantly enhance feelings of calm. Most importantly, we discovered that biofeedback is much more engaging when presented as a soothing interaction rather than as a performance metric.\n\nWhat's next: Next, we plan to expand the garden‚Äôs ecosystem by incorporating a wider variety of plant species, introducing seasonal environments, and enabling user-customizable growth patterns. We are also exploring deeper personalisation through baseline HRV profiles and adding support for more sensors, such as respiration belts and skin conductance monitors. Additionally, we hope to introduce shared ‚Äúrelaxation sessions,‚Äù allowing multiple users to cultivate a peaceful space together. Our goal is to continue transforming physiological data into visual, motivating, and genuinely helpful tools for wellbeing.",
    "hackathon": null,
    "prize": "Winner AI with Camera Access by Meta Runner-Up",
    "techStack": "meta, python, quest, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=NaNxa7DFRBo",
    "demo": "https://drive.google.com/drive/folders/1B9n-WyV1XdSxKscd6zOjkj7jhNLr8vMd?usp=share_link",
    "team": "Tobias Furtlehner, Aruaci, Sofiia-Khrystyna Borysiuk, Georg Becker",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/relaxing-garden"
  },
  {
    "title": "NOOX AI hardware device",
    "summary": "NOOX AI is a hardware device that leverages the capabilities of Arduino and ESP32 to integrate artificial intelligence functionalities in a compact and efficient manner. It aims to provide intelligent solutions for various applications, potentially enhancing user interaction and automating processes in everyday tasks.\n\nKEY ACHIEVEMENTS\nThe project won the hackathon due to its innovative integration of AI with robust hardware components, showcasing a unique user experience. Its ability to perform real-time data processing and decision-making set it apart from other entries, demonstrating both technical proficiency and practical application.\n\nNotable technical implementations include the use of FreeRTOS for efficient multitasking, allowing the device to handle multiple processes simultaneously. The integration of PlatformIO facilitated streamlined development and deployment, ensuring a smooth workflow. The combination of Arduino and ESP32 provided a powerful platform for wireless connectivity and sensor integration.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to hardware limitations, such as power consumption and processing capacity, especially when implementing AI algorithms. Additionally, ensuring seamless integration between the software and hardware components may have posed difficulties, particularly in terms of real-time performance and reliability.\n\nFUTURE POTENTIAL\nNOOX AI could evolve into a versatile platform for various applications, such as smart home devices, health monitoring systems, or educational tools. Future development could explore enhanced AI capabilities, improved user interfaces, and broader connectivity options, potentially expanding its market reach and application scope.",
    "hackathon": null,
    "prize": null,
    "techStack": "arduino, esp32, freertos, platformio",
    "github": "https://github.com/hurricane-0/NOOX",
    "youtube": "https://www.youtube.com/watch?v=BV11TCvB2EbS",
    "demo": "https://oshwhub.com/hgyzqxt/noox",
    "team": "h hric",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/noox-ai-hardware-device"
  },
  {
    "title": "cAIne",
    "summary": "cAIne Hackathon Project Summary\n\ncAIne is an innovative project leveraging augmented and virtual reality to enhance accessibility for users. By integrating advanced machine learning techniques and real-time object recognition, it aims to provide immersive experiences that assist individuals with disabilities in navigating their environments more effectively.\n\nKEY ACHIEVEMENTS: The project stood out for its dual recognition as a winner in both the overall competition and the accessibility category. Its unique approach to combining immersive technology with accessibility solutions not only showcased creativity but also addressed a significant need in enhancing the quality of life for users with disabilities.\n\ncAIne employs a sophisticated tech stack, including C# for development, Unity for creating immersive environments, and YOLO (You Only Look Once) for real-time object detection. The integration of these technologies enables seamless interaction and responsiveness within the augmented reality framework, making it a standout technical accomplishment.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to ensuring compatibility across various devices and platforms, optimizing performance for real-time processing of visual data, and addressing the diverse needs of users with different disabilities. Additionally, creating intuitive user interfaces that are accessible and user-friendly would have been a significant hurdle.\n\nFUTURE POTENTIAL: cAIne could evolve into a comprehensive accessibility platform that expands beyond navigation to include various functionalities such as communication aids, social interaction tools, and educational resources. Future iterations could also leverage advancements in AI and machine learning to further personalize user experiences, thereby broadening its impact on the accessibility community.\n\nInspiration: If you have someone next to you, look at them. Are they wearing glasses, or are you? Chances are that's the case. Approximately sixty-five percent of the world has some form of vision correction, and half of those individuals have very low-vision, and half of that population has no vision. \nXR provides a unique opportunity of an evolved form of vision correction, mixing Context-Aware AI and haptic feedback to create a new cane - a cAIne. Using the Quest's passthrough and camera access, we can scan an environment in real time, as well as with the Meta RayBans. This calculates distances and providing instant responses for the end user, and provides an intuitive gesture based UI for hand tracking, or vibrational feedback for controller use, allowing a user to decide which feels more comfortab\n\nChallenges: Figuring out the right AIs to work with our project and getting them to mesh in the way we wanted was difficult. This was made all the more difficult as not all AIs that we were going to utilize are available at this time, and we had to use substitutions. The time constraints made us limit our scope and cut features we would have loved to include. Another challenge was finding audio distinct enough and yet not too sharp for the cAIne's feedback.\n\nAccomplishments: Wrangling together various image and spatial recognition AIs was an exhilarating challenge. We've managed to run a local Yolo model directly on the Quest 3, providing very fast response. With a small team, succeeding in making the cAIne register these features and objects was a rewarding endeavor.\n\nWhat's next: As this software's goal is to be assisting low-vision/Blind individuals, our next stage goal is to implement image-to-speech for real time navigation. We will also fine-tune the way that distance is represented by the cAIne and improve our audiography. Another thing will be to add more pre-set objects that our AI can identify, increasing from 80 immediate recognition objects to possibly hundreds. A major future goal would be to implement more wearable technology, such as glove controllers with a larger array of feedback for both mobility, ease of access, and combining the current trade off that the controller vs hand tracking has, thus negating the issue.",
    "hackathon": null,
    "prize": "Winner Accessibility",
    "techStack": "c#, meta, mr, mruk, passthrough, unity, xr, yolo",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=iq5YVgHmPSs",
    "demo": "https://drive.google.com/file/d/1yX8inXNNnou6Cp58h6CeG2t_REBDox4J/view?usp=drive_link",
    "team": "Sophie Zw√∂lfer",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/caine"
  },
  {
    "title": "Remember Me",
    "summary": "Remember Me - Project Summary\n\nRemember Me is a project designed to enhance personal memories through innovative storytelling and multimedia experiences. By leveraging advanced AI and creative tools, it aims to create personalized narratives that allow users to relive and share significant moments in their lives.\n\nKEY ACHIEVEMENTS: The project stood out due to its unique approach to memory preservation and storytelling, integrating various multimedia elements to provide an immersive experience. Winning first place in the Film category signifies its creative and impactful execution, likely appealing to both judges and audiences alike.\n\nRemember Me utilized a range of cutting-edge technologies such as AI voice synthesis from Eleven Labs, image generation from Midjourney, and collaboration tools from Google and others. This combination allowed for seamless integration of voice, visuals, and narrative, enhancing user engagement and creativity.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the quality and coherence of generated content across different media formats. Additionally, navigating the ethical considerations surrounding AI-generated narratives and handling user data responsibly would have been critical.\n\nFUTURE POTENTIAL: Remember Me has significant potential for evolution into a comprehensive platform for memory sharing and storytelling. Future iterations could incorporate social features, collaborative storytelling, or even augmented reality experiences, further enriching the way users connect with their memories and with each other.\n\nWhat it does: It starts off a story of betrayal, family and resilience. And some fun future meets past meets dynasties at war.\n\nInspiration: I got inspired by the idea of people forgetting who you are. Then having to risk everything to make your loved ones remember you. The entire world is against you, what will you be ready to do so they don't forget forever? I also wanted the challenge of telling a story in a very short amount of time , so I chose the micro-drama category. Made we watch quite a few of them... what a world.\n\nHow it was built: For the first time, I used a node workflow for 95% of the content. With Freepik's new Spaces. It's such an interesting way of creating and visualising AI content, so much better than just regular prompts. My entire story, the bloopers, the evolution of my characters, all are in a Freepik Space. I also heavily leveraged Claude. Much more as a second brain assistant than usual. The script was a fantastic back and forth where I asked it to be brutally honest with me every step of the way. The process was: script in Claude, where I got a small description for every single shot required, then used an assistant within Spaces to kickstart some of my image prompts, the rest was a combination of several nodes and tricks. Finally, with all shots numbered, I took a huge amount of time to generate all\n\nChallenges: The inconsistency of the models made spreading out the shots between the video models not practical at all. Some shot images also just wouldn't work, so I had to rethink entire sequences. After creating all images, my main character got denied by the models' obscure censorship algorithm, I therefore had to do a last minute \"recast\". Nothing too complicated, but quite the surprise after all the work done.\n\nAccomplishments: Discovering ingredients in Veo 3.1 could create incredible sequences with multiple characters. Creating the music in Suno  at the 11th hour was so pleasing. It's an absolute gem of a tool. Finding a storyline that fits with different beats, within 120 sec. Matching my cool songs to those beats. The characters feel like they could be living somewhere, in some galaxy far far away.\n\nWhat we learned: Don't overestimate the number of shots in a short movie. It makes you create more than you can fit in the time frame or that you need for the story. A lot of sequences got cut, and yet I had spent time crafting the starting frames. For a quick last minute participation to the competition though, I'm satisfied. All in all, what fun it is to create stories. I'll never stop.\n\nWhat's next: Episode 2... perhaps a full micro-drama season? I've got the full story after all. Thanks Claude you're fun.",
    "hackathon": null,
    "prize": "Winner Film ‚Äì 1st Place",
    "techStack": "elevenlabs, freepik, google, hailuo, higgsfield, kling, magnific, midjourney, minimax, suno, veo",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=EvS0424JWP8",
    "demo": null,
    "team": "Tell Me World",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/remember-me-nfhbic"
  },
  {
    "title": "KRAR",
    "summary": "KRAR Project Summary\n\nKRAR is an innovative project leveraging artificial intelligence to enhance film production processes, likely focusing on automating tasks, improving storytelling, or optimizing production workflows. Although specific details are not provided, the project's goal appears to center around revolutionizing the film industry through AI integration.\n\nKEY ACHIEVEMENTS: KRAR stood out by winning multiple awards, including 2nd Place in the Film category and an overall winner designation. This recognition suggests that the project effectively addressed a significant need or gap within the film industry, showcasing creativity, technical prowess, and practical application.\n\nWhile specific technical implementations are not detailed, the use of AI implies advanced algorithms for tasks such as script analysis, video editing, or audience engagement prediction. The project likely involved machine learning models trained on extensive film datasets to generate insights or automate complex processes.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges such as data quality and availability, ensuring the AI's outputs align with creative intentions, and overcoming resistance from traditional industry stakeholders. Additionally, integrating AI tools within existing production environments can present logistical and technical hurdles.\n\nFUTURE POTENTIAL: KRAR could evolve by expanding its AI capabilities, perhaps incorporating features like real-time audience feedback analysis or personalized content generation. Collaborations with film studios and tech companies could lead to broader adoption and refinement of the technology, ultimately transforming how films are produced and consumed.",
    "hackathon": null,
    "prize": "Winner Film ‚Äì 2nd Place",
    "techStack": "ai",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=Aen8x9JQOAs",
    "demo": null,
    "team": "Juli√°n Santoro",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/krar"
  },
  {
    "title": "[B24] BearHammer Games",
    "summary": "BearHammer Games is a gaming project developed within the Horizon Worlds platform, possibly focusing on immersive virtual experiences that leverage 3D modeling and interactive design. While specific gameplay details are not provided, the project likely emphasizes community engagement and creativity in game development.\n\nKEY ACHIEVEMENTS\nThe project distinguished itself by securing the Winner title in the Horizon Worlds Category by Meta and also being recognized as a Runner-Up. This indicates a strong execution of creative concepts, user experience, and technical proficiency that resonated with judges and participants alike.\n\nBearHammer Games utilized Blender for 3D modeling and Visual Studio Code for coding, showcasing a blend of artistic design and programming skills. The integration within Horizon Worlds suggests advanced use of its tools to create engaging environments and interactive gameplay elements.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance within Horizon Worlds, ensuring compatibility across devices, and developing engaging gameplay mechanics that effectively utilize the platform‚Äôs capabilities. Additionally, balancing creativity with technical constraints may have posed difficulties.\n\nFUTURE POTENTIAL\nBearHammer Games has the potential to evolve into a more comprehensive gaming platform by expanding its game offerings, enhancing user-generated content features, and incorporating multiplayer functionalities. Future iterations could also explore partnerships for cross-platform play or additional immersive experiences, further engaging the community.",
    "hackathon": null,
    "prize": "Winner Horizon Worlds Category by Meta - Runner-Up",
    "techStack": "blender, horizonworlds, visualstudiocode",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=DJRpJZ-IQ1A",
    "demo": "https://drive.google.com/drive/folders/145txkRowDuAcPSu3ndU7iyztENAXekvP",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b24-bearhammer-games"
  },
  {
    "title": "2043 Teaser Trailer",
    "summary": "The \"2043 Teaser Trailer\" is an innovative multimedia project that likely combines various digital tools to create an engaging and futuristic teaser trailer. By utilizing advanced AI technologies and multimedia editing software, the project aims to captivate audiences with a compelling narrative that hints at a future scenario or storyline.\n\nKEY ACHIEVEMENTS\nThis project stands out due to its creative use of diverse technologies and its ability to seamlessly integrate them into a cohesive trailer. Winning the title sponsor's 2nd place award indicates that the judges recognized its originality, execution quality, and potential impact in engaging viewers with a unique vision of the future.\n\nThe project incorporates cutting-edge tools such as Midjourney for AI-generated visuals, ElevenLabs for advanced audio synthesis, and CapCut for video editing. The combination of these tools allows for high-quality graphics and sound design, enhancing the overall viewer experience. Additionally, the use of platforms like Astra and Magnific demonstrates a strong focus on user experience and interface design.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in coordinating the various technologies to create a seamless final product, especially in achieving a consistent aesthetic and narrative flow. Additionally, integrating AI-generated elements with traditional media could lead to discrepancies in quality and coherence. Ensuring that the final trailer resonates with the intended audience while maintaining originality may also have posed a significant challenge.\n\nFUTURE POTENTIAL\nThe \"2043 Teaser Trailer\" has the potential to evolve into a full-fledged multimedia project, possibly expanding into a series of trailers or an interactive narrative experience. Leveraging audience feedback and exploring additional storytelling formats could enhance engagement. Furthermore, partnerships with other creators or platforms could broaden its reach and influence in the digital content space.\n\nInspiration: This project started as a comic book series, created using AI, inside and not community. The movie was presented as worldwide premiere at the AI Film Festival Japan in Tokyo on November 3rd.",
    "hackathon": null,
    "prize": "Winner Title Sponsor ‚Äì 2nd Place",
    "techStack": "astra, capcut, elevenlabs, kling, magnific, midjourney, suno",
    "github": null,
    "youtube": "https://youtu.be/dJx-HMR7eR4?si=oo83lvi2c169sBss",
    "demo": "https://youtu.be/dJx-HMR7eR4?si=oo83lvi2c169sBss",
    "team": "Simone Sighinolfi",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/2043-teaser-trailer"
  },
  {
    "title": "NEO",
    "summary": "PROJECT SUMMARY: NEO\n\nNEO is a multimedia project designed to create engaging content by leveraging various digital tools for video editing, audio generation, and graphic design. It aims to streamline the content creation process, making it accessible and efficient for creators.\n\nKEY ACHIEVEMENTS: NEO stood out by winning the Hackathon and securing 2nd place in the Film category, showcasing its innovative approach to multimedia storytelling and its potential to enhance user engagement. The combination of tools used likely contributed to a polished final product that resonated with judges and participants alike.\n\nThe project utilized a diverse suite of technologies, including CapCut for video editing, ElevenLabs for audio generation, and Freepik for graphic resources. This integration of tools reflects a sophisticated understanding of content creation workflows, allowing for high-quality outputs in both audio-visual formats.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to the seamless integration of multiple platforms and technologies, ensuring compatibility and smooth user experience. Additionally, balancing creative vision with technical constraints and managing time effectively during the hackathon would have been significant hurdles.\n\nFUTURE POTENTIAL: NEO has the potential to evolve into a comprehensive content creation platform that incorporates advanced AI features for personalized content recommendations, collaborative tools for team projects, and monetization options for creators. There is also an opportunity to expand its reach by targeting specific niches within the content creation community.\n\nWhat's next: This short film, entirely produced with AI, explores, on a deeper level, our relationship with emerging technologies. It reflects on how these tools can empower us to tackle challenges‚Äîif we embrace them responsibly and move beyond the fear of the unfamiliar. Neophobia. It was created and developed in less than a month using AI tools from FREEPIK and FAL. The challenges were TIME and maintaining CHARACTER CONSISTENCY; without a doubt, I learned a tremendous amount from this project. I feel proud of the outcome and of the message it aims to convey. What I‚Äôve learned is invaluable; my skills have been pushed to their limits, and I‚Äôm extremely satisfied with the outcome. The next step is hoping the audience understands that AI and emerging technologies are not a threat. No tool can function w",
    "hackathon": null,
    "prize": "Winner Film ‚Äì 2nd Place",
    "techStack": "capcut, elevenlabs, fal, freepik, hailuo, kling, minimax",
    "github": null,
    "youtube": null,
    "demo": null,
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/neo-z6c4ur"
  },
  {
    "title": "FixItXR",
    "summary": "FixItXR Project Summary\n\nFixItXR is an innovative augmented reality (AR) application that leverages hand tracking and camera access to provide users with real-time assistance in fixing common household issues. By utilizing advanced AI and computer vision technologies, the application guides users step-by-step through repair processes, enhancing their DIY skills and confidence.\n\nKEY ACHIEVEMENTS: FixItXR stood out in the hackathon by winning the 1st Place prize for its integration of AI with camera access, which creates a seamless user experience. Its ability to effectively combine multiple technologies to deliver practical, hands-on solutions demonstrated its potential impact and user engagement.\n\nThe project features notable implementations such as hand tracking for intuitive interaction, the MetaSDK for augmented reality capabilities, and OpenAI for intelligent assistance. Additionally, the use of computer vision techniques from Roboflow allows the application to recognize tools and parts, making the repair process more efficient and user-friendly.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring accurate hand tracking and object recognition in diverse environments. Additionally, optimizing the app for various lighting conditions and user scenarios may have posed significant technical hurdles. Balancing user experience with the complexity of AR functionalities could also have been a concern.\n\nFUTURE POTENTIAL: FixItXR has the potential to evolve into a comprehensive platform for various DIY projects beyond household repairs, such as automotive or electronics maintenance. Future developments could include more advanced AI capabilities for personalized recommendations, integration with smart home devices, and expansion to multilingual support to reach a broader audience.\n\nWhat it does: FixItXR automatically labels the components inside the engine bay and overlays clear, labels on each part. It then displays step-by-step tutorials showing the user exactly how to perform maintenance tasks or repairs. Using the Meta Quest 3‚Äôs cameras and spatial tracking, the system aligns instructions with real components, making each repair easy to follow.\n\nInspiration: FixItXR was inspired by the idea that car repairs should feel intuitive rather than confusing. We wanted to replace paper manuals and guesswork with clear, mixed-reality guidance. Using the Meta Quest 3‚Äôs cameras and spatial tracking, we envisioned an assistant that overlays labels and instructions directly onto the real engine bay, helping anyone perform repairs with confidence and clarity.\n\nHow it was built: We built FixItXR using a combination of mixed reality tools, AI models, and spatial computing. Unity, together with the Meta Quest 3 passthrough and MR SDK, handled real-time spatial tracking, scene understanding, and anchoring virtual labels onto real engine components. Using 169 photos, we trained a Roboflow computer vision model capable of detecting key parts inside the engine bay. OpenAI powered the adaptive step-by-step guidance, reasoning, and TTS instructions that respond to the user‚Äôs actions. By combining these systems, we created a seamless workflow that scans the environment, identifies components, and delivers clear, interactive repair tutorials.\n\nChallenges: One of the biggest challenges was getting object detection to work reliably in 3D space. Existing pre-trained models worked in 2D images but failed when used in mixed reality, so we trained our own Roboflow model from 169 photos to recognize engine components more accurately. Another challenge was integrating multiple systems‚Äîcomputer vision, Unity MR interactions, UI flow, and AI-driven tutorials‚Äîinto one smooth pipeline. We also had to carefully coordinate timing between detection events, spatial anchoring, and step-by-step instructions to ensure the experience felt stable and responsive inside the headset.\n\nAccomplishments: We're proud that our custom-trained model consistently labels engine components correctly with no noticeable misidentification, even in a real mixed-reality environment. We successfully built clear, structured tutorials that guide users through each step, supported by smooth text-to-speech instructions that make the experience accessible and hands-free. Bringing all these elements together‚Äîaccurate detection, stable MR anchoring, and an intuitive AI-driven tutorial system‚Äîfelt like a major milestone for our team.\n\nWhat we learned: We learned a lot about training and refining computer vision models, especially how much data and iteration it takes to get reliable results. We also gained experience debugging complex MR interactions and solving technical challenges quickly as a team. Most importantly, we built strong friendships and learned how to collaborate effectively under pressure, bringing together different skills to create something meaningful in a very short time.\n\nWhat's next: We plan to expand FixItXR to cover more car systems beyond the engine bay‚Äîincluding brakes, suspension, electrical components, and interior repairs. We also plan to add real-time error tracking to detect and correct user mistakes before they cause damage, preventing issues like loosening the wrong bolt or skipping safety steps.\nAdditionally, we want to add multilingual support to make automotive repair accessible globally, delivering instructions in users' native languages. Our vision is to build a comprehensive platform that empowers anyone to confidently maintain and repair their vehicle, regardless of experience level.",
    "hackathon": null,
    "prize": "Winner AI with Camera Access by Meta 1st Place",
    "techStack": "handtracking, metasdk, openai, passthrough, pca, roboflow, unity",
    "github": "https://github.com/sarahimdad/MetaPCARoboflow/tree/add-tutorial-1",
    "youtube": "https://www.youtube.com/watch?v=8-F2Ac9t3I0",
    "demo": null,
    "team": "Jesus Hernandez, Chris KAMAL YOUSSEF",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/fixitxr"
  },
  {
    "title": "Morning buddy",
    "summary": "Morning Buddy Project Summary\n\nMorning Buddy is an innovative application designed to enhance user interaction through hand tracking technology. By leveraging advanced hand tracking capabilities, the project aims to create an immersive and intuitive experience for users, potentially serving as a personal assistant or companion in daily routines.\n\nKEY ACHIEVEMENTS: The project stood out by successfully integrating hand tracking technology in a seamless manner, which impressed judges and led to its recognition as the winner of the hackathon. Its innovative implementation of user interaction and practical application of cutting-edge technology contributed to its accolades, including the Best Implementation of Hand Tracking by Meta.\n\nMorning Buddy was built using C#, Unity, and NVIDIA technologies, showcasing strong performance in real-time hand tracking. The use of Meta's hand tracking solutions allowed for precise gesture recognition and interaction, enhancing the user experience and setting a high bar for implementation standards.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to the accuracy and responsiveness of hand tracking, particularly in varying lighting conditions or user environments. Additionally, optimizing the application for different hardware specifications and ensuring a smooth user interface could have presented significant hurdles.\n\nFUTURE POTENTIAL: Morning Buddy has the potential to evolve into a comprehensive personal assistant application that could integrate with smart home devices, health monitoring systems, or educational tools. Future iterations could focus on expanding functionality, improving machine learning algorithms for better gesture recognition, and incorporating voice commands for a more holistic user experience.\n\nWhat it does: In this mixed reality experience, you begin by settling into your usual morning routine. You place your coffee cup in front of you, and wherever you set it on the table, a small Buddy emerges beside it, eager for a sip, but only once he reaches the top of the mug. Your goal is to complete a sequence of gestural tasks. They may look simple but performing them accurately and consistently within a set time limit is surprisingly challenging. When you succeed, the little Buddy finally earns his sip of coffee, and you get your well-deserved first sip too.\n\nInspiration: One part of what inspired this group of Hackers, who have known each other for a while, was a topic that popped up quite often: Forgetfulness.\nThere's a common subtle sense of fear of the brain aging, maybe sooner than it should. One of the team members had a genuine freak-out about early memory issues the long-term cognitive health, becoming hyper-aware of it. They came across this video while doom scrolling on Instagram which sparked the research from there: 3 Brain Activation Exercises for Beginners\nAnother part of the inspiration came from noticing how easy it is to rely on AI in daily life for things that used to challenge our minds. Stopping to push our own thinking stops the support of neuroplasticity, what our brain needs to stay sharp. \n\"Morning Buddy\" came from this reminder of w\n\nHow it was built: For this application's logic, we utilized: Unity + Meta XR SDK for hand tracking, and scene understanding +Hand gesture classifiers for sequences like peace sign, palm open, rock on, and index up. For this application's narrative virtual layer: We designed the character \"Buddy\", he's brainy fella.\nModeled \"Buddy\" using hyper3d\n-Rigged and animated \"Buddy\" using Mixamo\nWe modeled on Maya the stairs that \"Buddy\" uses to climb us the coffee cup.\nWe made timeline sequences in unity for the intro/outro. \nWe made voiceovers using elevenlabs\n\nChallenges: During the hackathon, we faced the challenge of implementing cutting-edge technology with components still in beta (Meta AIO SDK), with the incompatibility issues between packages and versions that this entails. For some of us it was the first time doing a hackathon so time was running by so quickly, we had to keep the scope of the project within clear limits and not get carried away by the chaos of this type of environment, analyzing and prioritizing all the situations we encountered, such as wasting too much time implementing or improving components that do not have a primary impact on the experience, or building stable checkpoints to which we could return in case of a major problem.\n\nAccomplishments: The members of this team for years have been primarily focused on building VR experiences with a specific SDK XROAM so we‚Äôre quite proud to create a mixed reality experience in just two days using building blocks we hadn‚Äôt worked with extensively and not as familiar with the recent updates.\nThis was the first hackathon for 4 people out of 5 in this team, so being extra agile was a great exercise. A lot of creative solutions came from this while keeping focus on the main goal, the core mechanic.\n\nWhat's next: Include context-aware behavior:\nTea/Coffee/Milk/Juice, etc..... + Text-To-Speech for the character (elevenlabs)\nBigger/ wider library of gestural tasks. \nPersonalization: Adaptive to your progress with time / your weak points.\nAdd difficulty levels: Play with time frame",
    "hackathon": null,
    "prize": "Winner Best Implementation of Hand Tracking by Meta 1st Place",
    "techStack": "c#, meta, nvidia, unity",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ZnqLUPS5Eo4",
    "demo": null,
    "team": "Hoda Rawas, Alex Gavalda, David Sillero, Katrina Gutierrez",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/morning-buddy-3y2uwt"
  },
  {
    "title": "GR Cup Analytics",
    "summary": "GR Cup Analytics is a data-driven platform designed to analyze and visualize performance metrics related to the GR Cup, likely focusing on sports analytics. The project aims to provide insights into player and team statistics, trends, and overall performance, enabling fans and analysts to better understand the competition.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its comprehensive analytical capabilities and user-friendly interface, which effectively transformed complex data into easily digestible insights. Winning both the overall hackathon and the Best of Wildcard prize indicates exceptional innovation and execution, showcasing strong design and analytical prowess.\n\nNotable technical implementations include the use of HTML5 for an interactive web interface, Python for backend data processing and analysis, and Streamlit for creating a dynamic, real-time user experience. These technologies combined enable robust data visualizations and a responsive design, enhancing user engagement.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to data acquisition and integration, ensuring data accuracy, and developing algorithms for meaningful analysis. Additionally, creating an intuitive user interface that effectively communicates complex data could have posed design challenges.\n\nFUTURE POTENTIAL\nThe project could evolve by expanding its data sources to include real-time statistics, integrating machine learning for predictive analytics, or adding features like user personalization and social sharing capabilities. Collaboration with sports organizations or the inclusion of fan engagement tools could further enhance its value and reach.\n\nWhat it does: The app analyzes race 1 data for Barber Motorsports Park,Circuit of the Americas,Indianapolis, Sonoma race tracks.  It provides driver insights, pre race predictions and post event analysis.  The user can select a race and get the insights.  It also provides the ability for the user to select a driver and see how well they performed compared to the average.\n\nInspiration: The goal was to analyze GR Cup Data using race data that was provided.  There were multiple races with different races data information\n\nHow it was built: It was built using python and streamlit.  Python data analysis libraries were used (python, dumpy, scikit-learn, plotly).  Streamlit was used to build a data web app which showcases the insights in a simple to understand format.\n\nChallenges: The main challenge was understanding the data as there was no main data dictionary.  Another challenge was inconsistent naming and columns information across races.  The file names were inconsistent as well as some columns were not present in some of the race data.  To tackle this the information was narrowed to Race 1 information for Barber Motorsports Park,Circuit of the Americas,Indianapolis, Sonoma race tracks.\n\nWhat we learned: For me it was my first time working with race data, so it was a fun learning experience.\n\nWhat's next: In terms of next steps it is to improve Improved Data: Work to ensure data is consistent across GR Cup races\nExperiment with more models: Use different modelling approaches\n-Multi-Race Comparison: Improve the application to provide the ability to analyze results and metrics across races",
    "hackathon": null,
    "prize": "Winner Best of Wildcard",
    "techStack": "html5, python, streamlit",
    "github": "https://github.com/edimaudo/grcup-analytics",
    "youtube": "https://www.youtube.com/watch?v=K3ZWtno3Vfs",
    "demo": "https://blank-app-qd7rpm31xpn.streamlit.app/",
    "team": "Ed U",
    "date": "2025-11-19",
    "projectUrl": "https://devpost.com/software/gr-cup-analytics"
  },
  {
    "title": "Polkadot Analytics hub",
    "summary": "Polkadot Analytics Hub is a comprehensive platform designed to provide users with in-depth insights and analytics related to the Polkadot ecosystem. By leveraging advanced data processing and visualization techniques, the project aims to facilitate better decision-making for developers and investors involved in the Polkadot network.\n\nKEY ACHIEVEMENTS\nThe project distinguished itself by winning both the main prize and the community vote, indicating strong support and recognition from stakeholders. Its ability to seamlessly integrate various data sources and present analytics in an intuitive manner likely contributed to its appeal, showcasing innovation and user-centric design.\n\nNotable technical implementations include the use of Python libraries such as NumPy and Pandas for data manipulation and analysis, as well as Git for version control, ensuring collaborative development. The integration of APIs and DevOps practices likely enhanced the deployment and scalability of the analytics hub, allowing for real-time data processing and updates.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges including data integration from multiple sources within the Polkadot ecosystem, ensuring data accuracy and reliability. Additionally, scaling the platform to handle increasing user demands and maintaining performance during high traffic periods could have posed significant hurdles.\n\nFUTURE POTENTIAL\nThe Polkadot Analytics Hub could evolve by incorporating machine learning algorithms for predictive analytics, enhancing its value proposition for users. Expanding its functionality to include additional blockchain ecosystems or integrating with other analytics platforms could broaden its user base and applicability, making it a vital tool for blockchain analysis.\n\nWhat it does: Polkadot Analytics Hub provides a unified dashboard with:\nReal-time parachain metrics (activity, TVL, transactions)\nCross-chain insights and comparisons\nHistorical charts (7‚Äì30 day trends)\nInteractive visualizations and clean UI\nWallet connection with Polkadot.js\nOptional AI features: forecasting + anomaly detection\nIt helps developers, analysts, and users understand network health in one place.\n\nInspiration: The Polkadot ecosystem is growing fast, with many parachains and cross-chain activities happening at the same time. But there is no simple way to see everything in one place. Most tools focus on a single chain or provide only raw metrics. We wanted to build a central analytics hub that shows clear, real-time insights for the entire Polkadot network.\n\nHow it was built: Backend: Node.js + Express for API endpoints, caching, validation, and data structuring\nFrontend: Next.js + Tailwind + Chart.js for visual charts and smooth UI\nData: Polkadot RPC / Substrate endpoints\nAI Layer (optional): Python + FastAPI for prediction models and anomaly detection\nTools: GitHub, and Postman for testing\nThe system is modular so features can be added or replaced easily.\n\nChallenges: Handling RPC limits and inconsistent chain responses\nDesigning endpoints that stay fast even with large data\nChart performance with high-frequency updates\nCORS issues during dev and deployment\nKeeping backend and frontend synchronized while developing fast\n\nAccomplishments: A working, full-stack analytics system from scratch\nClean UI with responsive and interactive charts\nA structured backend ready for real Polkadot metrics\nA working AI prediction module\nStable architecture that can scale\nClear documentation and easy setup\n\nWhat we learned: How to structure production-style APIs\nHandling live blockchain data and caching\nAdvanced React + Next.js concepts\nHow to optimize charts for performance\nHow to integrate Python/AI services into a JS system\nTeam coordination, testing, and deployment workflows\n\nWhat's next: Full real-time data syncing with all major parachains\nUser accounts + personal saved dashboards\nAlerts and notifications for unusual network activity\nCross-chain flow diagrams (visualizing asset movement)\nMobile and tablet versions\nIntegration with more ecosystem tools (XCMP, bridges)\nLaunching a public version of the platform for community use",
    "hackathon": null,
    "prize": "Winner Community Vote",
    "techStack": "ai:, api, control:, devops, environment:, gemini, git, google, manager:, npm, numpy, package, pandas, version",
    "github": "https://github.com/Mr-mpange/polkadotblockchain.git",
    "youtube": "https://www.youtube.com/watch?v=ttCsqy98onA",
    "demo": null,
    "team": "I worked as a project manager",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/polkadot-analytics-hub"
  },
  {
    "title": "[B25] Archer Defense",
    "summary": "Archer Defense is an engaging interactive experience built in Horizon Worlds, where players assume the role of archers tasked with defending against waves of enemies. Utilizing immersive gameplay mechanics, the project combines strategy and action to create a captivating environment that encourages teamwork and skillful archery.\n\nKEY ACHIEVEMENTS\nThis project stood out by winning the First Prize in the Horizon Worlds Category by Meta, showcasing its exceptional design and user engagement. Its creative integration of gameplay mechanics and community interaction likely resonated with judges, highlighting both innovation and entertainment value.\n\nBuilt with Horizon and TypeScript, Archer Defense features real-time multiplayer capabilities, allowing users to collaborate in defending their territory. The use of TypeScript enhances code maintainability and scalability, while Horizon's platform enables rich, immersive environments and seamless user experiences.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to optimizing performance for multiplayer interactions, ensuring smooth gameplay despite potential lag or connectivity issues. Balancing game mechanics to maintain player engagement and preventing exploits or imbalances in the gameplay would also have been significant hurdles.\n\nFUTURE POTENTIAL\nArcher Defense has the potential to evolve into a more expansive game with additional features such as customizable characters, new enemy types, and seasonal events. Integrating virtual reality elements or expanding the narrative could enhance user engagement further, along with opportunities for community-driven content updates and tournaments.",
    "hackathon": null,
    "prize": "Winner Horizon Worlds Category by Meta - First Prize",
    "techStack": "horizon, typescript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=X6Id_wBj8sg",
    "demo": "https://horizon.meta.com/world/803648796175840/",
    "team": null,
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/archer-defense"
  },
  {
    "title": "xgotop: Realtime Go Runtime Visualizer",
    "summary": "xgotop is a real-time visualizer for Go runtimes that leverages eBPF (Extended Berkeley Packet Filter) technology to monitor and analyze the performance of Go applications. By providing in-depth insights into runtime behavior, it enables developers to optimize their applications more effectively.\n\nKEY ACHIEVEMENTS\nThis project won a hackathon prize due to its innovative use of eBPF for performance monitoring, which is relatively underutilized in Go environments. Its real-time visualization capabilities offer a unique perspective on runtime metrics, allowing developers to gain immediate feedback on their applications' performance.\n\n- eBPF Integration: The project utilizes eBPF to hook into kernel-level events, enabling detailed performance monitoring without significant overhead.\n- Cross-Language Implementation: Built using C, Go, and Python, the project showcases an adept integration of multiple programming languages to achieve its goals.\n- Real-Time Data Visualization: It employs advanced visualization techniques to present runtime data, making it accessible and actionable for developers.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to the complexity of integrating eBPF with Go runtimes, especially in ensuring compatibility and performance. Additionally, visualizing real-time data effectively requires careful handling of data flow and rendering, which can be technically demanding.\n\nFUTURE POTENTIAL\nxgotop could evolve into a comprehensive monitoring tool for Go applications, potentially expanding its capabilities to include support for other programming languages. Future iterations could incorporate machine learning algorithms for predictive analytics, alerting developers to potential performance issues before they arise. Furthermore, enhancing user interface features could improve usability for a broader audience.\n\nWhat it does: xgotop attaches several uprobe's to the specified program which is written in Go, and traces the goroutine state changes and memory allocations in detail with sub-microsecond delay.\n\nInspiration: I've been learning about, experimenting with, and writing about eBPF for a while. While working on a new project on tracing HTTP requests in HTTP servers written in Go using eBPF and bpftrace, I've needed to trace specific goroutines in realtime. That's when I've noticed that I should develop a realtime Go runtime visualizer with a CLI and a web UI to trace goroutine lifecycle and memory allocation events.\n\nHow it was built: I've built this project by first writing a bpftrace script to prototype the idea to see if it's doable, and then started writing the real implementation using C for the eBPF program, Go with the cilium/ebpf package for the userspace program, Go again for storage/API layer, Python for runtime metrics plotting and test validations, and React for web UI. You can see the overall system design in the illustration below.\n\nChallenges: The initial challenge was that extracting goroutine information from a running Go program is not a trivial task. Memory layout of internal runtime structures depends on the architecture of the host machine and the Go version of the target program. I've read the Go ABI to find out that I needed to use the r28 register to get the g runtime struct of the current goroutine for example. I've also read the Go source code for Go 1.25 to learn the memory offsets of g struct's fields that I need, enumerated values of native Go runtime types, internal values of goroutine states, runtime functions that I can hook, and so on.\nAfter reading the source code, I've bumped into the issue of not being able to use uretprobes on Go runtime functions, as the return probe was overriding the return PC of the run\n\nAccomplishments: I'm proud of being able to build a low-level project from start to finish by first doing the research by reading the Go source code and docs, then prototyping the idea with bpftrace, implementing the production version using C and Go, developing a testing suite around it, and finally building a web UI (with the help of AI) for the project. At the end of the day, I have a tool that I needed and also might be useful for the community.\n\nWhat we learned: I've learned that building tooling products which support multiple versions of the programming language and OS architectures is not a trivial task. That requires a lot of research and testing to validate the correctness of the implementation. I've also learned more about how goroutines work and allocate memory in Go runtime.\n\nWhat's next: There are still many things to fix and optimize and introduce as new features to the project. Here is an incomplete list of my ideas about the next steps for xgotop: Add support for amd64 architecture\nStore events in ClickHouse, which will be storage-efficient and fast as a columnar metrics DB is the perfect fit for this kind of data we generate\nOffline replay of sessions\nAdd support for more Go lifecycle events\nAdd support to be able to see which function is being executed in the goroutine (parsing the function name in runtime)\nAdd support for listing the stack trace of the goroutine at the time of the event\nOptimize the way that it's reading Go runtime objects (g struct has lots of padding now, which is a bad practice)",
    "hackathon": null,
    "prize": "Winner Using eBPF",
    "techStack": "c, ebpf, go, python",
    "github": "https://github.com/ozansz/xgotop",
    "youtube": "https://www.youtube.com/watch?v=Xr3l-eGaY7c",
    "demo": null,
    "team": "Ozan Sazak",
    "date": "2025-11-28",
    "projectUrl": "https://devpost.com/software/xgotop-go-runtime-observer"
  },
  {
    "title": "[B03] XR Latte Art: Pour decisions? Never again.",
    "summary": "XR Latte Art is an immersive mixed reality application designed to enhance the art of coffee-making by allowing users to create intricate latte designs through virtual guidance. By leveraging augmented and virtual reality, it provides interactive tutorials and real-time feedback, making latte art accessible to both beginners and seasoned baristas.\n\nKEY ACHIEVEMENTS\nThe project stood out for its innovative use of mixed and virtual reality to transform a traditional skill into an engaging digital experience. Winning 1st Place in the Mixed & Virtual Reality Category by Meta highlights its exceptional creativity, user interaction, and potential for educational impact in the culinary arts.\n\nKey technical implementations include the integration of ChatGPT for conversational guidance, Eleven Labs for realistic audio feedback, and Meta's SDK for seamless mixed-reality experiences. The use of Unity as the development platform allowed for sophisticated 3D modeling and interactive elements that enhance user engagement.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in developing a user-friendly interface that balances complexity with accessibility. Ensuring precise real-time tracking and responsiveness in a dynamic environment could have posed technical difficulties, particularly in achieving realistic simulations of latte art techniques. Additionally, optimizing the application for various hardware setups would have been essential.\n\nFUTURE POTENTIAL\nThe project could evolve by expanding its tutorial library to include various coffee-making techniques and styles, integrating social features for sharing creations, and incorporating gamification elements to enhance user engagement. Collaborations with coffee shops or barista training programs could also open opportunities for real-world applications and partnerships.\n\nWhat it does: XR Latte Art leverages mixed reality hand tracking to solve a surprisingly complex motor learning problem: latte art. Traditional learning requires expensive repetition and immediate feedback is impossible‚Äîyou only see results after the pour is complete. Our solution overlays 3D pour patterns directly into the user's physical space, tracks pitcher movement in real-time using hand tracking, and provides instant biomechanical feedback. Users practice the precise wrist rotations, pour heights, and movement speeds required for specific patterns (like heart, rosetta, tulip, swan, etc.), with scoring based on trajectory matching and timing accuracy. Once you are done with practicing, you can do the real deal with real milk and coffee. The result: accelerated skill acquisition through deliberate\n\nInspiration: Since I drink 2 coffees per day I only had limited chance to learn latte art and with that my first perfect latte art took 4 months of daily 1-2 pours. I want to learn quicker other latte arts and I was not willing to waste milk.\n\nHow it was built: We began with a focused ideation phase, prioritizing a strict MVP to avoid scope creep and ensure we could deliver a complete, high-quality experience within the hackathon timeframe. Our team split into two parallel workstreams:\n-Three team members developed the hand-tracking movement recording system along with the replay and evaluation engine.\n-Two team members focused on the user-facing experience, UI/UX flow, and overall ambience.\nWe leveraged Meta‚Äôs Hand Tracking SDK to capture high-fidelity hand joint data. To record a real barista‚Äôs movements, we generated a JSON file containing the full frame-by-frame positions and rotations of both hands throughout the demonstration. This JSON serves as the ‚Äúgold standard‚Äù animation sequence: during training sessions, we replay this movement for t\n\nChallenges: -Being able to find the best technique to follow the movement of the accessories in hand to be used.\n-Handtracking of the MetaSDK is not accurate enough to describe the precise movement of the barista with the current technology (fallback we used: use controllers with virtual objects overlayed for recording of the training movements)\n-Some errors with Meta SDK (since we were using the newest version of Meta SDK and Unity there were some incompatibilites and errors)\n\nAccomplishments: -Being able to to iterate from the initial idea through extraordinary individual improvement points\n-Being able to record, track, measure and playback hand movements with accuracy\n-Being able to provide pleasant experience for endusers in this short timeframe with ambience\n-Being able to test the concept in real environment with actual professionals on the field (2 baristas in coffee shops) and apply the feedback to the product.\n-Being able to do the movement recording for use from professional to ensure that the data is high quality.\n\nWhat we learned: -How to mix real objects with the MR space and interact with them in a way that we can also interact with the application at the same time.\n-Good UI designer is key and we were really blissful with ours\n-You can calculate scores based on fitness for data analysis\n-Macbook is faster at building\n-Collaborating with multiple expertise\n\nWhat's next: Phase 1: Expand the Pattern Library\nWe'll add advanced latte art patterns including swan, phoenix, dragon, and competitive-level designs. Users can unlock progressively harder patterns as their precision scores improve, creating a skill tree from beginner hearts to championship-level pours.\nPhase 2: Community Pattern Sharing\nEnable users to record their own signature patterns and share them. A professional barista in Tokyo can upload their award-winning rosetta technique, and enthusiasts worldwide can learn it exactly as performed.\nPhase 3: The Skill Marketplace - Beyond Coffee\nOur core technology‚Äîhand motion recording with precision scoring‚Äîapplies to any craft requiring muscle memory. We're building a marketplace where experts teach and learners master:\nCalligraphy & Hand Lettering - Bru",
    "hackathon": null,
    "prize": "Winner Mixed & Virtual Reality Category by Meta 1st Place",
    "techStack": "chatgpt, elevenlabs, metasdk, unity",
    "github": "https://github.com/danieloquelis/LatteArtXR",
    "youtube": "https://www.youtube.com/watch?v=mo_t3zV5lHk",
    "demo": null,
    "team": "Ignacio Arqueros, Juan Hernandez Martin, Daniel Oquelis",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/b03-latte-art-pour-decisions-never-again"
  },
  {
    "title": "Context Cards",
    "summary": "Context Cards is an innovative project that leverages face recognition technology to create personalized context-aware information cards. By identifying individuals through facial recognition, the application delivers relevant data and insights tailored to the user‚Äôs context, enhancing interaction experiences in various settings.\n\nKEY ACHIEVEMENTS\nThe project stood out by seamlessly integrating advanced face recognition capabilities with user-friendly interfaces, allowing for real-time contextual information delivery. Its recognition as a winner in both the general category and the \"Innovative Minds\" category highlights its creativity and practical application, demonstrating a unique approach to enhancing user engagement through AI.\n\nThe project utilizes a robust tech stack that includes Flask for backend development, SQLite for database management, and advanced libraries like NumPy for data processing. The face recognition functionality showcases the use of machine learning techniques, while the frontend is built using HTML5, CSS3, and JavaScript, ensuring a responsive and interactive user experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring the accuracy and reliability of the face recognition system, particularly in diverse lighting and environmental conditions. Additionally, concerns surrounding user privacy and data security would have needed to be addressed, alongside the integration of real-time data retrieval with the backend infrastructure.\n\nFUTURE POTENTIAL\nContext Cards has significant potential for evolution into various sectors, such as retail, education, and healthcare, where personalized user experiences can enhance engagement and service delivery. Future iterations could incorporate more sophisticated AI algorithms for improved accuracy, expand its functionality to support multiple languages, and develop partnerships with industries that could leverage contextual information for better customer interactions.\n\nWhat it does: We're developing a digital memory assistant to help you remember people in social situations without being creepy or invasive. This is how it works in human terms: 1) Our Simple, Ethical Solution Instead of recording conversations in secret, like some apps, ContextCards uses a mutual consent approach: both parties have to actively agree to connect, by scanning each other's QR codes, in a sort of digital handshake. And Most Important of all, No data collection on the sly; no surreptitious recording. 2) How It Works Day-to-Day *You meet a new person at a conference or networking event *You both open ContextCards and scan each other's QR codes *You jot down a quick note about them: \"Sarah - works on AI safety, loves hiking\" *Next time you see Sarah, your phone's camera recognizes her and flas\n\nInspiration: Last time when I was on event, I made some fantastic connections but next day, I couldn't recall them. This was embarrassing but I asked again and when they told me about context of our last meeting, I remembered everything! this brought the spark for this project.\nAnd I don't think its problem of only mine, I feel like most of us have went through this issue and it is reported that \"85% of professionals admit to forgetting critical details about people they've met.\"\n\nChallenges: 1) Face recognition model: We ran into the typical problems of hackathon about model training but for the current MVP, we decided not to get involved into training one instead, we decided on using face_recognition library from python for this prototype. 2) No IoT: This project is actually meant to be IoT project with camera integrated in glasses but due to limitation, we decided on using web version for demo and if we get enough reach, we could scale this project.\n\nAccomplishments: At the end of the day, we were able to set our skills together to create something that could bring a change, that change for us is solving networking and social connection problems.\n\nWhat we learned: I personally learned many things and as a team, we learned about project designing and other basics as its our first online hackathon which is really different from offline ones we attended.\n\nWhat's next: For future, we can add IoT in this project to 100x the concept. Additionally we can train our own model with custom dataset and all for even better accuracy.",
    "hackathon": null,
    "prize": "Winner Innovative minds",
    "techStack": "css3, face-recognition, flask, html5, javascript, numpy, python, sqlite, werkzeug",
    "github": "https://github.com/sakshyamadhikari01/meoww",
    "youtube": null,
    "demo": null,
    "team": "AKITO009",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/polypharma"
  },
  {
    "title": "NeuroSpring ‚Äì Real-Time Cognitive Load Optimizer",
    "summary": "NeuroSpring is a real-time cognitive load optimizer that leverages EEG (electroencephalography) data to assess and adjust cognitive load during tasks. By analyzing brain activity, the system aims to enhance productivity and well-being by dynamically optimizing work conditions based on individual cognitive states.\n\nKEY ACHIEVEMENTS\nThe project stood out for its innovative integration of real-time brainwave monitoring with adaptive workload management, showcasing a practical application of neuroscience in enhancing productivity. Winning the hackathon highlights its potential impact on personal and professional environments, as well as its technical sophistication.\n\nNeuroSpring utilized a range of advanced technologies including:\n- CNN (Convolutional Neural Networks) for processing EEG data.\n- FastAPI for building a high-performance web API.\n- PostgreSQL for robust data storage and management.\n- WebSocket for real-time communication between the server and clients.\n- ONNX for optimizing machine learning models for deployment.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in accurately interpreting EEG signals, ensuring low latency in real-time processing, and creating a user-friendly interface. Additionally, integrating various technologies (like Docker and Redis) while maintaining system performance would have required significant coordination and testing.\n\nFUTURE POTENTIAL\nNeuroSpring could evolve into a comprehensive productivity tool by integrating features like personalized task recommendations, in-depth analytics on cognitive performance trends, and compatibility with various work environments. Future iterations could explore broader applications in educational settings, mental health monitoring, or even workplace ergonomics to further enhance cognitive efficiency.\n\nWhat it does: NeuroSpring is an AI-powered platform that monitors cognitive load in real-time using EEG and behavioral data. It detects fatigue, stress, and focus lapses, and provides personalized interventions such as micro-break suggestions, mindfulness exercises, and environmental adjustments. It helps optimize productivity, prevent mental fatigue, and improve performance in everyday tasks and professional work.\n\nInspiration: Ever felt completely exhausted after hours of work, yet unable to focus, and wondered why your brain just won‚Äôt cooperate? I noticed that people in high-pressure environments‚Äîwhether at work, studying, or even during daily tasks‚Äîstruggle with mental fatigue, lapses in focus, and stress, leading to mistakes, lost time, and frustration. There was no easy, real-time way to understand what the brain was experiencing or to intervene before it was too late. NeuroSpring was born to solve this: a system that listens to your brain in real-time and helps you stay sharp, focused, and mentally energized throughout the day.\n\nHow it was built: EEG Signal Acquisition: 14-channel EEG for real-time brainwave data\nAI Model: Hybrid CNN-Transformer for cognitive state classification\nMulti-modal Integration: EEG + eye tracking + heart rate + keystroke dynamics + environment sensors\nExplainable AI: SHAP and LRP for real-time transparency of model decisions\nInterventions: Automated, personalized suggestions triggered by cognitive state\nDashboard & API: Real-time metrics visualization, logging, and analytics with React, FastAPI, WebSockets\nDeployment: Docker-compose ready, privacy-compliant, scalable to enterprise level\n\nChallenges: Real-time EEG processing with low latency while maintaining accuracy\nIntegrating multiple modalities (behavioral, physiological, environmental) in a unified model\nDesigning interventions that are effective, personalized, and non-intrusive\nEnsuring data privacy and security while collecting sensitive neurophysiological data\nMaking a production-ready dashboard with live AI explanations\n\nAccomplishments: Successfully implemented real-time EEG cognitive load monitoring with hybrid deep learning\nDeveloped a multi-modal integration system combining EEG, heart rate, eye tracking, and behavioral data\nBuilt Explainable AI features so users understand why interventions are suggested\nDesigned personalized interventions that increase focus duration by 80% and reduce mental fatigue by 60%\nCreated a production-ready, scalable platform ready for hackathon demo or real-world deployment\n\nWhat we learned: Combining neuroscience research with AI enables tangible improvements in cognitive performance\nReal-time, explainable interventions are critical for user trust and effectiveness\nMulti-modal data fusion significantly improves prediction accuracy over EEG alone\nScalable system design, privacy, and real-time processing are achievable in neurotechnology applications\nUser experience and UI design are as important as technical performance\n\nWhat's next: Add team-based and institutional analytics for workplaces\nIntegrate sleep and circadian rhythm optimization for 24/7 cognitive enhancement\nExplore AR/VR immersive environments to further improve focus\nDevelop predictive career guidance using cognitive load trends\nExpand cross-platform mobile apps for wider accessibility",
    "hackathon": null,
    "prize": null,
    "techStack": "cnn, docker, eeg, fastapi, onnx, postgresql, python, react, redis, transformer, websocket",
    "github": "https://github.com/senushidinara/NeuroSpring",
    "youtube": "https://www.youtube.com/watch?v=GaQwzHrI2dE",
    "demo": "https://691b246996bab2377e6c740b--incandescent-squirrel-caf8e0.netlify.app/#metrics",
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/neurospring-real-time-cognitive-load-optimizer"
  },
  {
    "title": "PathLearn",
    "summary": "PathLearn Project Summary\n\nPathLearn is an innovative educational platform designed to facilitate personalized learning pathways for users. By leveraging advanced natural language processing and machine learning techniques, it curates tailored learning experiences based on individual preferences and knowledge levels.\n\nKEY ACHIEVEMENTS: PathLearn distinguished itself by winning the hackathon and securing the 2nd overall position, indicating its strong competitive edge. The project‚Äôs unique approach to personalized education, combined with smooth user experience and effective implementation of technology, likely contributed to its recognition as a standout solution.\n\nThe project employs a sophisticated stack including JavaScript, Node.js, and React for its frontend, while utilizing Python, PyTorch, and sentence-transformers for backend machine learning processes. This combination enables robust data processing and user interaction, making it both functional and responsive.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges such as integrating diverse technologies seamlessly, ensuring scalability of the platform, and dealing with the complexities of natural language processing to provide accurate personalized recommendations. Additionally, user engagement and retention in a competitive educational technology market could pose difficulties.\n\nFUTURE POTENTIAL: PathLearn has significant potential for evolution, including the integration of more advanced AI algorithms to enhance learning pathways, expanding its content library, and incorporating user feedback mechanisms for continuous improvement. Future iterations could also explore partnerships with educational institutions to broaden its reach and impact.\n\nWhat it does: PathLearn is an adaptive AI tutor that builds a completely personalized, career-aligned curriculum for every student. It models each student's thinking using an RNN-based memory system, tracks mastery over time, and learns the student‚Äôs learning style through a Deep Reinforcement Learning teacher agent that continuously adjusts difficulty, pacing, and concept order. All explanations, examples, and practice problems are generated through a RAG pipeline powered by katanemo/Arch-Router-1.5B, ensuring every step is grounded, accurate, and tailored.\nStudents start by choosing both a course (Physics Honors, Algebra 2, Biology, Chemistry, etc.) and a future career (doctor, astrophysicist, biomedical researcher, engineer‚Ä¶). PathLearn fuses these two choices to build a dynamic curriculum that evolv\n\nInspiration: I‚Äôve always noticed how students struggle with learning independently‚Äîespecially at home. Time management, stress, gaps in understanding, and the lack of a truly personalized tutor make studying harder than it needs to be. Existing tools like Khan Academy and AI chatbots help, but they don‚Äôt adapt to you, your pace, or your goals. I wanted to build something that understands each student as an individual learner and shapes a learning path around who they are and who they want to become. That idea became PathLearn.\n\nHow it was built: I combined three core machine learning systems: RNN Learning Memory Model Models the student‚Äôs pace, retention, misconceptions, and improvement over time. DRL Curriculum Orchestrator A reinforcement-learning agent selects the next topic, difficulty, explanation type, and career-aligned examples based on ongoing performance. RAG + katanemo/Arch-Router-1.5B Retrieves curated textbook content, prior attempts, and example templates to generate personalized explanations, steps, and practice problems. A backend pipeline integrates these components, while the frontend adapts to each student with mastery maps, learning diagnostics, and personalized practice flows.\n\nChallenges: Designing a reward function that balances difficulty, mastery, and frustration.\nGetting the RNN to meaningfully predict retention curves based on sparse data.\nIntegrating RAG with Arch-Router-1.5B in a way that consistently produced career-aligned explanations.\nEnsuring the DRL agent didn‚Äôt overfit to a single learning pattern early on.\nMaking the system feel ‚Äúhuman‚Äù instead of just algorithmic.\n\nAccomplishments: Built a genuinely adaptive learning system that goes far beyond Q&A chatbots.\nGot the DRL agent to meaningfully personalize topic sequencing.\nCreated a career-driven learning experience that feels truly unique to each student.\nIntegrated Arch-Router-1.5B into a full RAG pipeline for high-quality explanations.\nDeveloped a model that adjusts to student weaknesses in real time.\n\nWhat we learned: I learned how powerful multi-model AI systems can be when each component plays a specific role. DRL and RNNs together can create deeply personalized learning dynamics that LLMs alone simply cannot achieve. I also gained experience building RAG pipelines, designing reward signals, and engineering scalable, adaptive education tools.\n\nWhat's next: Add more courses and specialized career tracks.\nTrain a larger student-behavior model using anonymized interaction data.\nIntegrate more modalities like drawings, graphs, and math handwriting.\nDeploy PathLearn as a full online learning platform.\nBuild student communities with AI-guided peer collaboration.\nExpand to college-level and professional certification tracks.",
    "hackathon": null,
    "prize": "Winner 2nd Overall",
    "techStack": "javascript, katanemo, node.js, python, pytorch, react, sentence-transformers, tailwindcss",
    "github": "https://github.com/aaaravM/PathLearn",
    "youtube": "https://www.youtube.com/watch?v=M4-9kVrcVt8",
    "demo": null,
    "team": "Aarav Muttanapalli",
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/pathlearn"
  },
  {
    "title": "Launchpad",
    "summary": "Launchpad Project Summary\n\nLaunchpad is a web application designed to facilitate the launch of new projects, products, or services by providing users with resources, tools, and a platform for collaboration. It aims to streamline the process from ideation to execution, making it easier for entrepreneurs and teams to bring their concepts to life.\n\nKEY ACHIEVEMENTS: Launchpad stood out in the hackathon due to its innovative approach to project facilitation, integration of user-friendly design, and effective resource allocation. Its ability to connect users with mentors and essential tools likely contributed to its recognition as a winner.\n\nBuilt with Next.js, Launchpad leverages server-side rendering for improved performance and SEO. Notable technical implementations may include dynamic routing for seamless navigation, API integration for real-time data access, and responsive design to ensure usability across devices.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to user engagement, ensuring the platform is intuitive, and managing scalability as more users join. Additionally, integrating various resources and maintaining a seamless user experience could have posed technical difficulties.\n\nFUTURE POTENTIAL: Launchpad could evolve into a comprehensive ecosystem that includes advanced analytics, AI-driven recommendations for project development, and a marketplace for service providers. Expanding features to include community forums and networking events could further enhance user collaboration and engagement.\n\nWhat it does: Launchpad uses your profile data (user entered) to score your profile out of 10 based on your essays, academics, awards and extracurriculars. It then uses gpt-4o-mini to assess your strengths and weaknesses to present you with a tailor made strategy for college applications.\nLaunchpad also has a (currently small) database of universities/colleges as well as upcoming opportunities (competitions, scholarships). Clicking on a college will presented you with AI generated insights about that college and how your 'fit' matches with what they are looking for and whether you are a competitive applicant or not by rating your fit and chances of admission out of 10 based on SEVERAL FACTORS. This is achieved using a REALLY LONG prompt that goes into in depth detail about admission requirements and nua\n\nInspiration: What Inspired me to build launchpad was that there were like a bajillion application portals for different universities and colleges and as a busy high-schooler, it took me 2 years to wrap my head around most of them. Thats why I  built Launchpad! it consolidates all that you need to know about college applications around the globe and how to make your application better.\n\nHow it was built: It was built using Next.js and Next API routes along with Mongo DB and the open AI API (using gpt 4o mini and Tailwind-CSS for styling and AOS for animations. \nAI disclosure: Github copilot was used to assist in development and testing, and the Open AI API was used for generating the analysis and insights for user profiles. I mentioned prompts for the gpt API before which serve as the backbone for Launchpad so heres the entire prompt (its only a bit long...): const prompt = `You are a college admissions expert. Analyze this student's fit for ${college.name}.\n\nCRITICAL INSTRUCTIONS: \n\n1. ADMISSION CRITERIA - Identify what ${college.name} actually uses:\n   - Canadian universities (Alberta, UofT, UBC, McGill): HIGH SCHOOL GRADES only, NOT SAT/ACT\n   - UK universities: A-Levels, IB, or equival\n\nChallenges: Gathering info on colleges, ensuring the model doesn't hallucinate new info , persisting user profile insights by adjusting the models temperature parameter (to ensure consistent outputs)\n\nWhat's next: We'll continue expanding our database to ensure that it can truly evolve into an all in one platform for college applications.",
    "hackathon": null,
    "prize": null,
    "techStack": "next.js",
    "github": "https://github.com/ARTariqDev/Launchpad",
    "youtube": "https://www.youtube.com/watch?v=oQ47-gXgV2U",
    "demo": "https://launchpad-bice-seven.vercel.app/",
    "team": "Abdul Wasiq Khan, Ahmad Hassan, Muhammad Abubakar, Abdur Rehman Tariq",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/launchpad-7xkjwu"
  },
  {
    "title": "wARkshop",
    "summary": "wARkshop is an interactive workshop platform that utilizes hand tracking technology to create immersive experiences for users. By leveraging Meta's hand tracking capabilities within a Unity environment, it allows participants to engage in activities and learn through intuitive, gesture-based interactions.\n\nKEY ACHIEVEMENTS\nThe project stood out for its innovative implementation of hand tracking, which provided a unique and engaging user experience. Winning the \"Best Implementation of Hand Tracking\" by Meta demonstrates the project's technical excellence and effective use of cutting-edge technology, showcasing the team's ability to push boundaries in virtual interaction.\n\nNotable technical implementations include the seamless integration of Meta's hand tracking SDK with Unity, enabling real-time gesture recognition and interaction. The project also likely utilized C# for scripting, optimizing performance and responsiveness during user interactions, which contributed to the overall immersive experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to accurately capturing and processing hand movements in real-time, ensuring minimal latency and high precision. Additionally, developing user-friendly interfaces and ensuring accessibility for various skill levels may have been hurdles in the design process.\n\nFUTURE POTENTIAL\nwARkshop has significant potential for evolution into a comprehensive learning platform that could cater to a variety of subjects and skills. Future developments could include expanding the range of activities, integrating multiplayer capabilities, or enhancing AI-driven tutorials, making it a versatile tool for education and training in various fields.",
    "hackathon": null,
    "prize": "Winner Best Implementation of Hand Tracking by Meta Runner-Up",
    "techStack": "c#, meta, unity",
    "github": "https://github.com/jordialfonsop/wARkshop",
    "youtube": "https://www.youtube.com/watch?v=KZwpt0Wb5mM",
    "demo": null,
    "team": "I Worked in AI, ComfyUI,, Roberto Villar Vega",
    "date": "2025-11-18",
    "projectUrl": "https://devpost.com/software/warkshop"
  },
  {
    "title": "StudySync AI",
    "summary": "StudySync AI is an intelligent study assistant designed to enhance learning experiences through personalized content delivery and emotional analysis. By leveraging advanced AI technologies, the platform adapts lectures and study materials to meet individual student needs while assessing emotional engagement during learning sessions.\n\nKEY ACHIEVEMENTS\nThe project stood out by effectively integrating multiple AI tools to create a responsive and adaptive learning environment. Its innovative use of emotion detection and lecture processing allowed for real-time adjustments to study materials, significantly improving user engagement and educational outcomes. Winning multiple prizes indicates a strong recognition of its impact and potential.\n\nStudySync AI utilizes a combination of cutting-edge technologies, including:\n- Emotion Detection: Implementing face-api.js to analyze student emotions and tailor content accordingly.\n- Lecture Processing: Using Google Gemini API for processing and enhancing lecture materials.\n- Frontend Technologies: Built with React and TypeScript for a responsive user interface, coupled with Tailwind CSS for streamlined design and Recharts for data visualization.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in integrating various AI technologies smoothly to ensure real-time performance and reliability. Additionally, addressing data privacy concerns, especially regarding emotion detection, and ensuring the system's adaptability to diverse learning styles could have posed significant hurdles.\n\nFUTURE POTENTIAL\nStudySync AI has the potential to evolve into a comprehensive educational platform that not only personalizes content but also includes features like collaborative learning, gamification elements, and broader integration with various educational resources. By expanding its capabilities, it could serve a wider range of educational institutions and individual learners, enhancing overall educational effectiveness.\n\nWhat it does: StudySync AI reads your emotions, tracks your focus, and predicts when you'll forget‚Äîadapting in real-time to help you learn smarter. LectureGPT processes uploaded lectures (video/audio/PDF) and generates Cornell notes, flashcards, and quizzes instantly. Focus Sessions track tab-switching in real-time‚Äîwhen you get distracted, positive messages appear (\"Stay focused! You've got this! üéØ\"), achieving 88% average focus scores. MindMirror uses webcam emotion detection to identify confusion and automatically simplifies content with visual aids. Memory Retention Engine predicts exactly when you'll forget using the Ebbinghaus forgetting curve, scheduling reviews at optimal intervals. AI Tutor provides on-demand explanations during study sessions. Students improve quiz scores from 75% to 95%, main\n\nInspiration: You're studying quantum mechanics. You switch to check Instagram‚Äîyour focus score drops and a gentle message appears: \"Stay focused! You've got this! üéØ\" You furrow your brow at a complex equation‚Äîyour webcam detects confusion and simplifies the explanation. Three days later, just as you're about to forget, a review notification arrives. That's StudySync AI: the platform that knows when you're distracted, confused, or forgetting‚Äîand helps in real-time.\n\nHow it was built: Tech Stack: React + TypeScript, Tailwind CSS, Google Gemini API (lecture processing), Face-API.js (emotion detection), Recharts (analytics), and Ebbinghaus algorithms (spaced repetition). Development: We built in 4 phases over 48 hours‚Äîinfrastructure setup, AI feature integration, behavioral tracking implementation, and analytics dashboard polish. Used client-side processing for speed, LocalStorage for persistence, and prompt engineering to generate structured Cornell notes from Gemini.\n\nChallenges: Emotion detection accuracy: Face-API.js misclassified expressions. We added confidence thresholds (75%+) and 3-second smoothing windows, achieving 85% accuracy. API rate limiting: Hit Gemini limits during testing. Implemented exponential backoff, caching, and switched to faster gemini-2.0-flash-exp model. Real-time performance: Face detection + timer + tracking caused lag. Reduced detection from 60fps to 10fps and optimized React renders. Time constraints: At hour 30, we had 5 half-built features. Prioritized ruthlessly‚Äîfinished 2 core features perfectly rather than 5 poorly.\n\nAccomplishments: Successfully integrated 3 complex APIs into a cohesive experience. Built real-time emotion detection that actually works (85% accuracy). Created a complete ecosystem‚Äînot just one feature. Implemented scientific algorithms students can understand. Achieved professional UI that looks like a real SaaS product. Most importantly: we built empathy into code. We don't shame students for distractions‚Äîwe encourage them. We don't wait for confusion‚Äîwe detect and help. We don't send random reminders‚Äîwe predict scientifically.\n\nWhat we learned: Technical: Emotion detection is accessible with pre-trained models. Prompt engineering is an art requiring 20+ iterations. Performance optimization matters more than perfect features. Domain: Ebbinghaus curve actually works (60% better retention). Positive reinforcement beats punishment. Students don't know when they're confused until too late. Hackathon: Build for the demo. Solve real problems judges relate to. Differentiation is key‚Äîemotion detection is our unfair advantage.\n\nWhat's next: Immediate: Beta program with 50 students, move to secure backend, add offline mode. V2.0 (Q1 2025): Mobile apps, group study features, live lecture recording Chrome extension, LMS integration (Canvas/Blackboard). V3.0 (Q3 2025): Enhanced emotion model trained on 10K+ expressions, voice-based AI tutor, predictive performance analytics, AR/VR study environments. Enterprise: University licensing at $5/student/year, API for third-party integrations, academic validation through research partnerships. Vision: Transform learning for 100M+ students worldwide by making education adapt to them‚Äînot the other way around.",
    "hackathon": null,
    "prize": "Winner Participation Prize; Winner Second Place",
    "techStack": "claude, ebbinghaus, face-api.js-(emotion-detection), google-ai-studio, google-gemini-api-(lecture-processing), html5, javascript, npm, react-+-typescript, recharts-(analytics), tailwind-css",
    "github": "https://github.com/srikarkarri/StudySync-AI/tree/main",
    "youtube": "https://www.youtube.com/watch?v=YRp7QXfoaOs",
    "demo": null,
    "team": "Srikar Karri",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/studysync-ai-v86jts"
  },
  {
    "title": "ChainLink Portfolio - Polkadot Cross-Chain Activity Tracker",
    "summary": "ChainLink Portfolio is a cross-chain activity tracker designed for the Polkadot ecosystem, allowing users to monitor and manage their assets across multiple blockchain networks. By integrating various data sources and visualization tools, it aims to provide a comprehensive overview of users' portfolio performance and activity in real-time.\n\nKEY ACHIEVEMENTS\nThis project stood out by effectively leveraging Polkadot‚Äôs unique cross-chain capabilities, facilitating seamless asset tracking across different networks. Its innovative approach to data visualization and user-friendly interface contributed to its recognition as a winner in the hackathon, showcasing both technical proficiency and market relevance.\n\nThe project is built using a robust technology stack that includes Next.js for the frontend, TypeScript for type safety, and Polkadot.js API for blockchain interactions. It utilizes various APIs such as CoinGecko for asset pricing and incorporates advanced data visualization libraries like Recharts. The integration of cross-chain messaging (XCM) further enhances its functionality, allowing for efficient communication between different blockchains.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to interoperability between diverse blockchain ecosystems, ensuring real-time data accuracy, and managing the complexities of cross-chain transactions. Additionally, navigating the evolving regulatory landscape for cryptocurrencies and maintaining user security could have posed significant hurdles.\n\nFUTURE POTENTIAL\nChainLink Portfolio has the potential to evolve into a comprehensive asset management tool that could incorporate features such as automated trading strategies, enhanced analytics, and integration with decentralized finance (DeFi) protocols. As blockchain technology continues to grow, the platform could expand to support additional networks and functionalities, solidifying its position as a leading portfolio management solution in the crypto space.\n\nWhat it does: ChainLink Portfolio is an AI-enhanced, privacy-first dashboard that unifies your Polkadot ecosystem presence: Multi-Chain Aggregation: Connects to 4 parachains (Polkadot, Astar, Moonbeam, Paseo Asset Hub) and displays all balances in real-time with USD values\nAI Portfolio Advisor: Powered by Groq's Llama 3.1, it analyzes your holdings and provides personalized recommendations on staking, allocation, and cross-chain opportunities\nPortfolio Health Score: Calculates a 0-100 score based on diversification, risk balance, chain activity, and portfolio size\nReal XCM Transfers: Execute actual cross-chain asset transfers using Polkadot's XCM protocol‚Äîmove tokens between parachains with one click\nBeautiful Visualizations: Interactive pie charts, chain cards, and responsive design that works on any d\n\nInspiration: As a Web3 learner exploring Polkadot, I faced a frustrating problem: my crypto identity was scattered across multiple parachains. Checking my Astar balance meant opening one explorer, Moonbeam another, Polkadot yet another. I realized this fragmented experience was preventing mainstream adoption‚Äîusers shouldn't need to be blockchain experts to see their assets. Polkadot's unique multi-chain architecture is powerful, but the user experience needed to catch up. I wanted to build something that made cross-chain interaction as simple as checking your bank account‚Äîone dashboard, complete visibility, and actual cross-chain actions.\n\nHow it was built: Tech Stack: Next.js 14, TypeScript, Tailwind CSS, Polkadot.js API, XCM Protocol, Groq AI, Recharts\n\nChallenges: SSR vs Browser APIs: Next.js server-side rendering conflicted with Polkadot.js extension (browser-only). Solved with dynamic imports and typeof window checks.\nXCM Address Encoding: Spent hours debugging \"Expected 32 bytes, found 48 bytes\" error. Learned SS58 addresses need decodeAddress() to convert to raw 32-byte account IDs for XCM messages.\nTestnet Token Availability: Getting testnet tokens was harder than expected. Faucets were deprecated, rate-limited, or broken. Had to find alternative faucets and coordinate multi-chain testing.\nAI Data Formatting: Initially, AI recommendations were generic. Had to refine prompts extensively to ensure the AI understood users already own these tokens and should focus on allocation/staking strategies, not purchase recommendations.\nZero Balance Visualiz\n\nAccomplishments: Real XCM Transfers: Not a mock or demo‚Äîactual on-chain cross-chain asset transfers using Polkadot's XCM protocol\nAI That Understands Polkadot: Created an AI advisor that provides genuinely useful, ecosystem-specific recommendations about staking, XCM opportunities, and parachain strategies\nComplete Portfolio Analytics: Built a comprehensive health scoring system that goes beyond simple balance display\nWeb2 UX in Web3: Achieved a polished, responsive interface that feels modern and accessible to non-technical users\nPrivacy-First Architecture: No backend, no data collection‚Äîeverything client-side while still being feature-rich\nPerformance: Parallel queries load 4 chains simultaneously in ~2-3 seconds with graceful error handling\nFully Responsive: Works beautifully on desktop, tablet, and mob",
    "hackathon": null,
    "prize": "Winner Certified Polkadot Tinkerer",
    "techStack": "3.1, 8b, ai:, api, apis:, asset, astar, chain, chains:, coingecko, data), extension, frontend:-next.js-14, groq, instant, llama, moonbeam, network, paseo, polkadot, polkadot.js, pricing, protocol, recharts, relay, tailwind-css-blockchain:-polkadot.js-api, target, testnet), typescript, visualization:, wallet:, with, xcm",
    "github": "https://github.com/satishtamilan/polkadot-portfolio-ai",
    "youtube": "https://www.youtube.com/watch?v=aFCO6nQfPT4",
    "demo": null,
    "team": "Satish Kumar Anandhan",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/chainlink-portfolio-polkadot-cross-chain-activity-tracker"
  },
  {
    "title": "ConsilAI",
    "summary": "ConsilAI Project Summary\n\nConsilAI is an innovative project that leverages AI technologies to enhance decision-making processes. It integrates various web technologies to create a seamless user experience, helping users to efficiently gather and analyze information from diverse sources.\n\nKEY ACHIEVEMENTS: The project stood out by winning multiple prizes, including a tour of Microsoft Reston and a Techno Basket award. Its recognition likely stems from its unique approach to combining AI with user-friendly interfaces, making complex data more accessible and actionable for users.\n\nNotable technical implementations include the use of Azure for cloud services, BeautifulSoup4 for web scraping, and Playwright for automated testing. The project also employs a modern stack with Next.js and React for the frontend, ensuring a responsive and dynamic user experience, while leveraging Supabase for backend development and database management.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data integration from multiple sources, ensuring the accuracy and reliability of scraped information. Additionally, maintaining a user-friendly interface while handling complex AI computations could have posed significant design and technical hurdles.\n\nFUTURE POTENTIAL: ConsilAI has the potential to evolve into a comprehensive decision-support tool by incorporating more advanced AI features, such as predictive analytics and machine learning models. Future iterations could expand its scope by integrating more data sources and enhancing collaboration features, making it a go-to solution for both individuals and organizations in need of data-driven insights.\n\nWhat it does: ConsilAI is an intelligent classroom management platform that combines student profiling, automated educational research, and AI-powered plan generation to help teachers create personalized learning strategies for every student. Core Features: Comprehensive Student Profiles:  Teachers create detailed profiles for each student including their issues (behavioral or academic challenges), strengths, goals, and contextual notes\nInteractive Drag-and-Drop Seating Chart:  Visual classroom layout simulator where teachers can arrange students and reason about optimal seating arrangements\nAutomated Research Scraping:  The system automatically searches and scrapes real educational research from across the web using Playwright and BeautifulSoup\nAI-Powered Learning Plans:  Azure Phi generates structured\n\nInspiration: The inspiration for ConsilAI came from recognizing the overwhelming challenge teachers face in managing diverse classrooms where every student has unique needs, learning styles, and behavioral patterns. We've seen teachers struggle to provide individualized attention when they're responsible for 25+ students, each requiring different approaches. Furthermore, special education teachers and those working with neurodiverse students face even greater challenges in developing personalized intervention strategies quickly enough to make a real difference fast. We realized that while teachers have incredible expertise and dedication, they often lack the time to research and implement evidence-based strategies for each student's specific situation. ConsilAI was born from the vision of giving teache\n\nHow it was built: Frontend Architecture: Built with Next.js 14 using the modern App Router for optimal performance and routing\nReact 18 for component-based UI with server and client components\nTailwind CSS for rapid, responsive styling with utility-first approach\nImplemented drag-and-drop seating chart for intuitive classroom layout management\nCreated clean, accessible interfaces for student profile creation and viewing\nDesigned the Plans page UI ready for AI integration\nDeveloped a light and dark mode for the user's personal choice Backend & Database: Supabase for authentication, database, and real-time capabilities\nStructured data models for students, profiles, and generated plans\nSecure environment variable management for API keys AI Pipeline (packages/ai): TypeScript for type-safe AI integration code\nAz\n\nChallenges: Integrating Azure Phi: The AI doesn't always return perfectly formatted JSON. We spent a lot of time tweaking prompts and adding error handling (type validation, fallback mechanisms) to parse responses reliably. Web Scraping Development: Every educational website is structured differently. We had to make our scraper smart enough to extract actual content while filtering out menus, ads, and irrelevant content. Timing was tricky too since some pages load content dynamically, meaning we needed Playwright to wait for the right elements. Monorepo Coordination: We had TypeScript handling the frontend and AI calls, while Python did the scraping. Making sure they talked to each other smoothly took some planning, especially around passing data back and forth. Furthermore, there was the coordination\n\nAccomplishments: End-to-End AI Pipeline:  Successfully built a complete pipeline from: student profile ‚Üí keyword extraction ‚Üí web scraping ‚Üí AI generation ‚Üí structured learning plan, all working together seamlessly and in real-time\nReal Research Integration:  Unlike generic chatbot AIs and their responses, our system pulls from actual educational research and search results on the web, making recommendations based on the best practices currently used\nMonorepo Architecture:  Our monorepo structure kept things organized even while rushing. The AI package, scraper, and frontend all have clear boundaries, making it way easier to debug and add features\nIntelligent Web Scraping:  We got web scraping working reliably across different educational sites, which is notoriously finicky. The fact that we can load pages\n\nWhat we learned: Technical Skills: Prompt engineering is an art, since getting Azure Phi to consistently return usable JSON took way more tweaking than expected\nNext.js 14's App Router is powerful but has a learning curve (less if coming from older React patterns, but still quite some)\nPlaywright is amazing for scraping modern websites that load content dynamically\nManaging a project with TypeScript and Python packages requires clear interfaces and good documentation\nSupabase makes auth and database stuff way easier than using your own About the Problem Space: Teachers deal with way more privacy regulations (like FERPA) than we initially thought\nActionable, specific advice matters more than research summaries\nTeachers want to see where information came from since trust is huge (especially in this subject m\n\nWhat's next: Make It More Useful: Let teachers track which plans they've tried and how they're working\nLet teachers edit the AI-generated plans before saving them\nMake it work extremely well on tablets since teachers use those in class Improve the AI: Pull from more specialized education databases, not just general web searches\nAdd a feedback system so teachers can rate plan effectiveness and improve future suggestions\nBuild different strategies for different needs (ADHD, autism, gifted students, etc.) Make It Faster: Scrape multiple pages at once instead of one-by-one\nCache research we've already scraped so similar students don't trigger repeated searches\nShow AI responses as they're being generated instead of waiting for the whole thing Miscellaneous: Add better error handling when scraping fails or",
    "hackathon": null,
    "prize": "Winner Tour of Microsoft Reston; Winner Techno Basket",
    "techStack": "azure, beautifulsoup4, chromium, css, html, javascript, next.js, node.js, phi-4, playwright, python, react, supabase, tailwind, typescript",
    "github": "https://github.com/cchamb26/consilai",
    "youtube": "https://www.youtube.com/watch?v=jGOfytp5zUI",
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/consilai"
  },
  {
    "title": "TaskTamer",
    "summary": "TaskTamer Project Summary\n\nTaskTamer is a productivity application designed to streamline task management and enhance user efficiency. By integrating intuitive features for task organization and prioritization, it aims to help users manage their daily activities more effectively.\n\nKEY ACHIEVEMENTS: TaskTamer stood out by delivering a user-friendly interface and robust functionality that resonated with users' needs for organization. Its innovative features and seamless integration of task management tools contributed to its recognition as a winner at the hackathon.\n\nThe project utilized a combination of base44 for backend architecture, Python for server-side logic, and React for creating a dynamic front-end experience. This stack allowed for a responsive user experience and efficient data handling.\n\nPOTENTIAL CHALLENGES: The team likely faced hurdles related to integrating various technologies seamlessly, ensuring data synchronization across tasks, and maintaining performance under high user load. Additionally, they may have encountered challenges in user experience design to meet diverse user preferences.\n\nFUTURE POTENTIAL: TaskTamer could evolve by incorporating AI-driven features for smarter task suggestions and prioritization. Future developments may include collaboration tools for teams, mobile app versions, and integrations with other productivity platforms to enhance its utility in various workflows.\n\nWhat it does: TaskTamer is a world alive with possibilities. It automatically creates a daily calendar for you based on the onboarding questions, integrating all your tasks, commitments, and personal preferences. It features duel mode, where milestones in your tasks translate into exciting real-time zombie battles; a thrilling way to stay accountable. Your city map grows and evolves as you complete tasks, turning everyday productivity into a personal adventure. Each zombie companion and mutation card represents your progress and health: tasks left unfinished become zombies that challenge your focus, while curing and advancing them reflects tangible growth. Analytics provide clear feedback on your progress, showing statistics and patterns in a playful, motivating way. Different themes let you personalise\n\nInspiration: I built TaskTamer out of a very personal frustration. Like many students, I have a to-do list that grows longer by the day, yet the apps I‚Äôve tried are dull, static, and uninspiring. They never change, never adapt, and quickly become forgotten. I wanted something different. I wanted an app that breathes life into productivity, that moves with me, that actually understands how students like me think and feel. TaskTamer is born from the Eisenhower Matrix, but taken a step further: it updates in real time based on deadlines and importance, nudging the most pressing tasks to the forefront. I know what it is like to struggle with motivation and stress, so I designed the app to create a genuine reward and gratification cycle. Each task completed earns you XP, powers up your zombie companion, and\n\nHow it was built: We built TaskTamer using Base44, with all prompts and AI behaviours engineered by myself. The development process involved extensive market research, where 86% of students expressed excitement for an app like this. We debated multiple themes; basic trees, pets, zombies and landed on zombies because it is unique, fun, and appeals directly to teens‚Äô love of playful challenge. I poured thought into every detail, from calendar logic to duel mechanics, ensuring the app isn‚Äôt just functional, but immersive and enjoyable.\n\nChallenges: Conceptualising TaskTamer in its entirety was a challenge. There were so many ambitious ideas I wanted to include, from duels to city maps to dynamic calendars. Making duel mode feel like a real-time, live battle with milestones was particularly tricky. Animations and graphics for the zombie theme were also a challenge; I needed them to feel professional, engaging, and non-tacky, which required iteration and refinement.\n\nAccomplishments: I am incredibly proud of the variety of themes, which allow the app to feel fresh and personal for any student. The automatic calendar is one of my favourite achievements, as planning my day is something I personally struggle with. I adore the zombie theme, which is quirky, fun, and entirely unique, as well as the random health alerts that gently remind students to breathe, stretch, or reflect; they are small, delightful touches that make the app feel alive. I also love how the Base Camp background changes by time of day, and the overall concept is something I have thought about for years. This hackathon gave me the motivation to finally bring it to life, and I am thrilled with how it turned out.\n\nWhat we learned: Through building TaskTamer, I learned how to turn a concrete vision into reality. I discovered how to expand ideas thoughtfully, balancing ambition with usability, and how to craft a product that truly delivers what students need. I now understand the value of iterative design, research, and making something both functional and magical.\n\nWhat's next: Looking ahead, there are a few exciting areas I want to expand. I hope to enhance gamification in duels, adding more levels, animations, and city map interactions. There are also ideas to introduce new rewards, skills, and evolving challenges. Overall, I am so impressed with what the app has become; it is playful, supportive, motivating, and uniquely designed for students like me, and I cannot wait to see how it continues to grow.",
    "hackathon": null,
    "prize": null,
    "techStack": "base44, python, react",
    "github": "https://github.com/superiorlati/TaskTamer",
    "youtube": "https://www.youtube.com/watch?v=A_nyFxvKhiQ",
    "demo": "https://app-3f3fb448-3a43-49df-b3c0-5abf8ba35258.base44.app/",
    "team": "Ananya Gulati",
    "date": "2025-11-21",
    "projectUrl": "https://devpost.com/software/tasktamer-hetu2y"
  },
  {
    "title": "SafeWalk+",
    "summary": "SafeWalk+ Project Summary\n\nSafeWalk+ is a mobile and web application designed to enhance personal safety during walks, particularly in urban environments. The platform connects users with friends or volunteers who can provide real-time support or monitoring, ensuring that individuals feel secure while traveling alone, especially at night or in unfamiliar areas.\n\nKEY ACHIEVEMENTS: SafeWalk+ stood out in the hackathon due to its focus on social good, addressing a critical issue of personal safety in community settings. Its innovative approach to connecting users with support networks garnered recognition, leading to its dual win as both the overall hackathon champion and the \"Best Hack for Social Good.\"\n\nThe project was built using a robust technology stack that includes HTML, CSS, JavaScript, and React, allowing for a responsive and user-friendly interface. Key technical implementations may have included real-time geolocation tracking, user authentication systems, and integration with messaging services to facilitate communication between users and their safety networks.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring user privacy and data security, particularly when handling sensitive location information. Additionally, achieving seamless real-time communication and maintaining app performance under varying network conditions could have posed significant technical hurdles.\n\nFUTURE POTENTIAL: SafeWalk+ could evolve into a comprehensive personal safety platform by incorporating features such as AI-driven alerts for potential hazards, partnerships with local law enforcement for emergency response, and expanded community engagement tools. Future iterations could also explore additional functionalities like group safety walks or integration with wearables for enhanced monitoring.\n\nWhat it does: SafeWalk+ calculates routes for walking or cycling by balancing safety and efficiency. It uses a weighted safety score that accounts for infrastructure (sidewalks, bike lanes), street lighting, nearby amenities, crime levels, and disruptions like construction. Users can choose between the safest route or the fastest route, select a departure time, and view an interactive map with turn-by-turn guidance, highlighting streets according to their safety.\n\nInspiration: SafeWalk+ was inspired by the need for safer urban navigation for pedestrians and cyclists. In many cities, traditional navigation apps prioritize speed over safety, which can put users at risk in areas with poor lighting, high traffic, or elevated crime. We wanted to create an app that empowers people to confidently navigate their city by prioritizing safety without sacrificing convenience.\n\nHow it was built: We built SafeWalk+ using a React frontend with Leaflet for interactive maps and a Node.js/Express backend for route calculations. Street and safety data are loaded from CSV files and supplemented with mock data for demonstration purposes. The backend constructs a graph of intersections and street segments, calculates weighted safety scores, and uses Dijkstra‚Äôs algorithm to determine the safest and fastest paths. The frontend displays routes with color-coding, allows users to input start/end locations, select departure times, and view upcoming turns and safety alerts.\n\nChallenges: One major challenge was ensuring the safest and fastest routes were distinct‚Äîinitially they were identical because we hadn‚Äôt separated distance and safety scoring in the algorithm. Integrating CSV data with mock data for streets also required careful handling to prevent missing or incomplete segments. Lastly, designing a clean, intuitive UI that could display multiple routes, turn directions, and alerts without clutter was tricky, especially for a hackathon timeframe.\n\nAccomplishments: We are proud of building a fully functional MVP that can calculate and display safe and fast routes on a map, even with limited data. The app dynamically adjusts route selection based on time of day, and our safety scoring formula accounts for multiple real-world factors. We also implemented an Apple-like interface for journey mode with turn-by-turn guidance and safety notifications, providing a professional, polished feel in a short time.\n\nWhat we learned: We learned how to integrate geospatial data from CSVs into a graph structure for routing, and how to apply weighted scoring to balance multiple factors like safety and distance. We also gained experience using React Leaflet for interactive map visualization and handling frontend-backend communication in real time. Finally, we improved our understanding of UI design principles for mobile-friendly, intuitive, and aesthetically pleasing applications.\n\nWhat's next: Next, we plan to refine the backend algorithm to include real-time traffic data, dynamic crime alerts, and better time-of-day adjustments. We also want to enhance the frontend by allowing users to save favorite routes, integrate notifications for upcoming hazards, and expand to additional cities using live data feeds. Our ultimate goal is to make SafeWalk+ a go-to app for safe, confident urban navigation for pedestrians and cyclists everywhere.",
    "hackathon": null,
    "prize": "Winner Best Hack for Social Good",
    "techStack": "css, html, javascript, react",
    "github": "https://github.com/jashanbains-24/HackCampsProject",
    "youtube": "https://www.youtube.com/watch?v=WUHdUjCq7TA",
    "demo": null,
    "team": "Jashan Bains, Priyansh Bahri, Navi Sharma",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/safewalk-bneql3"
  },
  {
    "title": "Mela Pay",
    "summary": "Mela Pay Project Summary\n\nMela Pay is a digital payment solution designed to facilitate seamless transactions in various contexts, leveraging blockchain technology for secure and efficient processing. It aims to provide users with an intuitive interface for managing payments, potentially targeting underserved markets or niche communities in need of modern financial tools.\n\nKEY ACHIEVEMENTS: Mela Pay stood out by winning both the main hackathon prize and the Roots Community Meetup Prize, indicating its strong community engagement and potential impact. The project's innovative approach to secure payments likely resonated with judges, showcasing a unique solution that addresses real-world financial challenges.\n\nThe project utilizes a robust tech stack consisting of express.js, JavaScript, MongoDB, Next.js, Node.js, and Polkadot.js. This combination allows for a scalable and efficient architecture, providing real-time transaction capabilities and a user-friendly experience. The integration with Polkadot.js indicates a commitment to leveraging blockchain interoperability for enhanced security and functionality.\n\nPOTENTIAL CHALLENGES: The team may have faced challenges related to ensuring user security and data privacy, particularly in the context of blockchain technology. Additionally, navigating regulatory compliance for financial applications and achieving user adoption in a competitive market could present significant hurdles.\n\nFUTURE POTENTIAL: Mela Pay has the potential to evolve into a comprehensive financial ecosystem, incorporating features such as loyalty rewards, peer-to-peer transactions, and integration with various financial services. By expanding its reach and building partnerships within the community, it could significantly enhance financial inclusivity and user engagement in the digital payment landscape.",
    "hackathon": null,
    "prize": "Winner Roots Community Meetup Prize",
    "techStack": "express.js, javascript, mongodb, next.js, node.js, polkadot.js",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=MhBIjCCSxVc",
    "demo": "https://mela-polkadot-project-gzok.vercel.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/mela-pay"
  },
  {
    "title": "RENEW'D",
    "summary": "RENEW'D Project Summary\n\nRENEW'D is a web application designed to promote sustainability by helping users track and reduce their carbon footprint. The platform likely offers tools for users to monitor their daily activities, access eco-friendly tips, and engage with a community focused on environmental stewardship.\n\nKEY ACHIEVEMENTS: This project stood out due to its innovative approach to sustainability and user engagement. Winning both the overall prize and finalist recognition indicates it effectively combines functionality with user experience, resonating well with judges and participants alike.\n\nRENEW'D was developed using JavaScript and React, ensuring a responsive and dynamic user interface. The use of SQLite for data management suggests efficient handling of user data, while Tailwind CSS likely provided a modern and visually appealing design.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring accurate carbon footprint calculations, maintaining user engagement, and scaling the application to accommodate growth. Additionally, integrating sustainability education into a user-friendly format may have posed design and content challenges.\n\nFUTURE POTENTIAL: RENEW'D could evolve by incorporating machine learning to personalize user suggestions based on behavior patterns. Expanding the platform to include partnerships with local businesses for eco-friendly products and services could further enhance user engagement and community impact.\n\nWhat it does: Renew'd reimagines thrifting as an effortless, personalized experience. Users swipe through items matched to their style, sizing, and location‚Äîno cash, no shipping costs, just direct trades. Our matching algorithm pairs people based on what they're looking for and what they have to offer, keeping fashion circular while reducing waste and expanding wardrobes. Like something? Match with the owner, chat in-app, and arrange a trade. A world of preloved items now at your fingertips. What makes us unique: Matching algorithm: dating-app meets marketplace\nCash free: item-for-item trading, no shipping costs\nSmart results: pairing users based on sizing, style and location\n\nInspiration: Fast fashion is killing the planet. Millions of tons of textiles hit landfills every year while perfectly good clothes sit unworn in closets. Buying new is expensive, and existing thrift platforms are overwhelming catalogs with zero personalization. We saw an opportunity to turn your closet into someone else's favorite piece of clothing, and make sustainable fashion actually feel good. Why should finding your next favorite piece be harder than finding your next date?\n\nHow it was built: We started by mapping complete user flows from sign-up through listing creation, swiping, matching, and trade completion. The database architecture uses relational structures connecting users, listings, likes, matches, and messages with optimized queries for real-time feed generation. The frontend centers on gesture-based swipe mechanics with smooth card animations and an intuitive, accessible UI with readable layouts and size-inclusive tags. The backend implements a weighted scoring system that normalizes location distance, size compatibility, and style preferences‚Äîboth explicit (tags, requirements) and implicit (swipe history, interaction patterns). We designed the system around mutual matching rather than buyer-seller dynamics, which fundamentally changed how we approached transaction f\n\nChallenges: Balancing algorithm feature weights across heterogeneous data types was complex‚Äîlocation needs distance-based scoring, style requires categorical matching, sizing demands precision. We had to optimize database queries joining multiple tables while filtering already-seen items and maintaining performance. The biggest challenge was rethinking marketplace dynamics for trades instead of purchases, which meant designing for mutual interest and equal exchange rather than one-directional transactions. Time constraints forced us to prioritize the core matching pipeline over features like trade histories, reputation systems, and community tabs.\n\nAccomplishments: We successfully translated dating app engagement mechanics into a functional trading marketplace that solves real environmental problems. The database schema we built scales efficiently despite complex many-to-many relationships across users, items, and interactions. Our matching algorithm framework uses collaborative filtering techniques that improve recommendations over time as it learns from user behavior. We created comprehensive user flows covering every edge case and state transition. Most importantly, we proved that making sustainability effortless and fun can drive meaningful behavioral change without sacrificing functionality.\n\nWhat we learned: We gained hands-on experience with recommendation systems, feature engineering, and multi-dimensional scoring algorithms. The project reinforced how critical database indexing and query optimization are for apps with complex relational data. We learned to adapt UX patterns between domains while preserving their psychological effectiveness‚Äîchanneling the dopamine hit of swiping toward sustainable behavior. Building for trades rather than sales taught us to think differently about incentive structures, mutual value creation, and transaction completion. We also learned that accessible design and inclusive features (like size-inclusive tags) aren't just nice-to-haves‚Äîthey're essential for building platforms that serve diverse communities.\n\nWhat's next: We're completing the MVP and launching a beta to gather real user data for algorithm refinement. Next comes machine learning integrationÔºõcollaborative filtering for better recommendations and computer vision for automated item tagging and style recognition. We're building features like profile viewing, bundling multiple items in trades, like history tracking, and community tabs. The technical roadmap includes horizontal scaling with caching layers, materialized views for feed generation, and message queues for real-time notifications. We'll implement trust and safety features including reputation scoring and trade verification. Long-term, this matching framework could extend beyond fashion to furniture, electronics, books‚Äîany marketplace where discovery beats search and sustainability matt",
    "hackathon": null,
    "prize": "Winner Finalist",
    "techStack": "javascript, react, sqlite, tailwind",
    "github": "https://github.com/enyazvn/HackCamp",
    "youtube": "https://www.youtube.com/watch?v=VqgQut_r_Zo",
    "demo": "https://docs.google.com/presentation/d/13whQi02Gx5GyU1AaskrtMNUMuHkk-tTvtzTM8AFgTeE/edit?usp=sharing",
    "team": "Enya Zeng, Arseniy Dolgov, Kiana Faden, Kylie Seto",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/renew-d"
  },
  {
    "title": "NeuroAccess: AI-Powered Cognitive Equity Platform",
    "summary": "NeuroAccess is an AI-powered platform designed to enhance cognitive equity by leveraging real-time EEG signal processing. The project aims to provide personalized insights into cognitive health and accessibility, enabling users to better understand and manage their cognitive capabilities.\n\nKEY ACHIEVEMENTS\nNeuroAccess distinguished itself by integrating advanced AI techniques with edge computing for efficient real-time analysis of EEG data. Its focus on explainable AI ensures that users can comprehend the insights provided, promoting transparency and trust in the technology. Winning the hackathon signifies the project's innovative approach and impactful potential in addressing cognitive disparities.\n\n- Edge Computing: Enables real-time processing of EEG data directly on devices, reducing latency and bandwidth usage.\n- EEG Signal Processing: Utilizes MNE-Python for advanced analysis of brainwave signals, ensuring accurate interpretation of cognitive states.\n- Explainable AI: Implements frameworks that allow users to understand AI-driven insights, fostering user engagement and trust.\n- Cross-Platform Development: Built with Flutter and React for a seamless user experience across multiple platforms.\n- Machine Learning Frameworks: Employs PyTorch and TensorFlow for developing robust models that can adapt and learn from user data.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in ensuring the accuracy and reliability of EEG signal interpretation, particularly in diverse user populations. Additionally, maintaining user privacy and data security while processing sensitive cognitive data would have posed significant hurdles. Developing a user-friendly interface that effectively communicates complex AI insights could also be challenging.\n\nFUTURE POTENTIAL\nNeuroAccess has the potential to evolve into a comprehensive cognitive health platform that includes features such as personalized cognitive training programs, integration with wearable devices, and community support forums. Collaborations with healthcare professionals could enhance its credibility and application, while expanding its reach into educational and therapeutic domains could further promote cognitive equity.",
    "hackathon": null,
    "prize": null,
    "techStack": "edge-computing, eeg-signal-processing, explainable-ai, figma, firebase, flutter, mne-python, node.js, python, pytorch, react, tensorflow",
    "github": "https://github.com/senushidinara/neuroaccess-",
    "youtube": "https://www.youtube.com/watch?v=4DL3CPq0sgw",
    "demo": "https://691a4697edf05dc357139391--guileless-dieffenbachia-ab248d.netlify.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/neuroaccess-ai-powered-cognitive-equity-platform"
  },
  {
    "title": "Big Boy With A Fez",
    "summary": "\"Big Boy With A Fez\" is an interactive installation that combines physical and digital elements, allowing users to engage with an animated figure equipped with a fez. Utilizing sensors and programming, the project creates a playful and immersive experience, where the character responds to user interactions in real-time.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its innovative use of hardware and software integration, creating a unique interactive experience that captivated judges and participants alike. Winning the overall hackathon and the Interactive Media Arts/Entertainment category showcases its creativity, execution, and engagement factor.\n\nKey technical implementations include the use of Arduino for hardware control, infrared sensors for user interaction, and Python for programming the logic behind the character's responses. The integration of a TFT display enhances visual feedback, making the interactive experience more engaging.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as synchronizing the hardware components with the software, ensuring responsive interactions, and troubleshooting the physical construction of the cardboard figure. Additionally, optimizing the user experience while managing the limited resources available during the hackathon could have posed difficulties.\n\nFUTURE POTENTIAL\nThe project has significant potential for evolution by incorporating more advanced sensors or machine learning algorithms to enhance interactivity. Future iterations could also explore broader themes and narratives, integrating storytelling elements or expanding the range of user interactions, potentially leading to applications in education or entertainment.\n\nWhat it does: He's a big boy that wears a fez! He can speak and move his head, and his mouth moves with it! He's controlled by a remote. He's powered by an Arduino Mega 2560, plus a servo, an ST7735 TFT display, and an IR receiver!\n\nInspiration: I was inspired by Doctor Who, specifically the eleventh doctor, who wore a fez. His suit was inspired by Kingpin from Spider-Man: Into the Spiderverse. I was also inspired by the cardboard boxes.\n\nHow it was built: I built it using Arduino code to control everything except sound, which is controlled by a Python script over serial on my MacBook.\n\nChallenges: I didn't have very much time to make him! I had probably 6 hours total. He's more complicated than you would think.\n\nAccomplishments: There was a point I didn't think I'd have a project because I was so behind. But I have a project! Huzzah!\n\nWhat we learned: I've learned to have an idea next time going into this, and to use my time wisely!\n\nWhat's next: He's going to continue to be a big boy wearing a fez. The Python file was completely generated with AI (ChatGPT). The Arduino code was not and is released under the GNU GPLv3.",
    "hackathon": null,
    "prize": "Winner Interactive Media Arts/Entertainment",
    "techStack": "arduino, cardboard, display, ir, python, servo, tft",
    "github": "https://github.com/danieliscrazy/BigBoyWithAFez",
    "youtube": "https://www.youtube.com/watch?v=oP9MieDAlJU",
    "demo": null,
    "team": "Daniel Davidson",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/big-boy-with-a-fez"
  },
  {
    "title": "Zeural",
    "summary": "Zeural Project Summary\n\nZeural is an innovative project that leverages various technologies to create a platform or tool, though specific functionalities are not detailed. It likely focuses on enhancing user experience or streamlining processes through a web-based interface.\n\nKEY ACHIEVEMENTS: This project distinguished itself in the hackathon by winning multiple awards, including a participation prize and recognition as a top contender among the 24-85 range. Its success suggests a strong concept, effective execution, and possibly a unique approach to solving a problem or addressing a need.\n\nZeural utilizes a robust tech stack including CSS, HTML, and JavaScript for front-end development, while leveraging Firebase for real-time database and authentication functionalities. The use of GitHub workflows indicates a commitment to continuous integration and deployment, which enhances the project‚Äôs reliability and scalability.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating various technologies seamlessly, ensuring a smooth user experience, and managing real-time data interactions with Firebase. Additionally, debugging and optimizing the application for performance may have posed significant hurdles, especially under time constraints typical of hackathons.\n\nFUTURE POTENTIAL: Zeural has the potential to evolve into a more comprehensive platform by expanding its feature set, enhancing user engagement, and possibly incorporating machine learning elements using Python. Future iterations could focus on user feedback, scalability, and integration with additional APIs or services to broaden its applicability and user base.\n\nWhat it does: Zeural is an open-source learning platform that combines: Educational content: AI/ML tutorials, programming lessons, and research summaries.\nFun engagement: Mythology stories to refresh learners between intense topics.\nResearch integration: Cron job fetching the latest arXiv papers, with chatbot explanations.\nAccessibility & customization: Theme options for learners with different needs and upcoming multilingual support.\nCommunity contributions: Anyone can add content or improve the platform.\n\nInspiration: I noticed that learners often get bored when studying complex topics for long periods. To keep engagement high, I integrated mythology stories in between lessons‚Äîfun, short stories that let learners take mental breaks while staying curious. Additionally, I wanted to make research papers more accessible, since my first experience reading them was challenging. This inspired me to add a chatbot below each research paper to help learners understand the content more effectively.\n\nHow it was built: Frontend: React.js, TypeScript, Tailwind CSS\nBackend: GitHub Jekyll and Firebase.\nImplemented Jekyll so that it is easier for others to contribute\nDatabase: MongoDB / PostgreSQL ‚Äî stores content, user progress, and contributions.\nContent Management: Markdown-based with Git versioning for easy contribution.\nSpecial features: Dynamic research paper fetching, chatbot explanations, theme section, and an upcoming Zeural learner community.\n\nChallenges: Balancing learning content with engaging, fun experiences.\nIntegrating research papers dynamically while keeping them easy to understand.\nDesigning a frontend that keeps learners motivated for long periods.\nPlanning for community contributions and a scalable platform.\n\nAccomplishments: Successfully integrated mythology stories to maintain learner engagement.\nBuilt a chatbot-assisted research paper reader for better understanding of complex papers.\nCreated a fully open-source platform where contributors can improve content and features.\nDeveloped a frontend that‚Äôs visually engaging and user-friendly.\n-Search bar for better navigation\n\nWhat we learned: -How to make learning better and make it in a way those to learn can contribute later to the same. How to combine diverse content types into a cohesive platform.\nHandling real-time data fetching and chatbot integration.\nImportance of user engagement and interface design in education.\nThe value of community-driven, open-s\n\nWhat's next: Implement multilingual support to reach a global audience.\nLaunch a dedicated Zeural learner community for collaboration and knowledge sharing.\nExpand content across more domains while maintaining engagement and interactivity.\nContinuously improve the chatbot and research integration for easier understanding of papers.",
    "hackathon": null,
    "prize": "Winner Participation Prize; Winner Top 24-85",
    "techStack": "css, firebas, firebase, githubworkflows, html, javascript, jekyll, python",
    "github": "https://github.com/zeural1/zeural1.github.io",
    "youtube": "https://www.youtube.com/watch?v=s4CHMLAAV9Y",
    "demo": "http://zeural1.github.io/",
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/zeural"
  },
  {
    "title": "PolkaShield",
    "summary": "PolkaShield is a blockchain project built using Rust and Substrate, aimed at enhancing security and privacy in decentralized applications (dApps) within the Polkadot ecosystem. Its primary focus is to provide robust protection mechanisms against vulnerabilities and attacks that dApps may encounter in a multi-chain environment.\n\nKEY ACHIEVEMENTS\nPolkaShield distinguished itself by winning both the overall hackathon and the third prize across all themes. Its success can be attributed to innovative security features, effective user experience design, and a well-executed demonstration that showcased its potential impact on the dApp ecosystem.\n\nNotable technical implementations of PolkaShield include the use of Rust for high-performance development and Substrate for rapid blockchain deployment. The project likely features advanced cryptographic techniques to secure transactions and user data, as well as interoperability solutions that allow seamless communication across different blockchains within the Polkadot network.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring comprehensive security across diverse dApp functionalities, addressing scalability issues in a multi-chain environment, and navigating the complexities of integrating with existing Polkadot parachains. Additionally, maintaining user trust while implementing security measures could pose a difficulty.\n\nFUTURE POTENTIAL\nPolkaShield has significant potential to evolve into a leading security framework for dApps, potentially expanding its functionalities to include automated security audits, real-time threat detection, and compliance with regulatory standards. Future development could also focus on partnerships with existing dApps and blockchain projects to create a more secure ecosystem.\n\nWhat it does: PolkaShield introduces a decentralized, cryptographically verifiable access-control layer for Web2 applications. It includes: A custom Polkadot chain that stores roles, permissions, and audit logs\nA dashboard where admins can create roles and assign permissions\nA public API gateway that lets Web2 apps verify access using a simple call like:\n\n/verify-access?user=<address>&role=editor\n\nA demo ‚ÄúEditor Portal‚Äù that shows how permissions unlock or restrict actions in real time Users own their permissions through wallet signatures, and developers get a reliable, tamper-proof way to manage access.\n\nInspiration: Every Web2 application, from school portals to admin dashboards to platforms, relies on a centralized access-control model, often reduced to a single database flag like is_admin = true.\nThat tiny line silently governs permissions, authority, and trust. But because it lives inside a private server, it creates a dangerous single point of failure.\nIf the database goes down, gets corrupted, or is compromised, the entire access-control system collapses. I kept asking myself: What if permissions were user-owned instead of server-owned?\nWhat if they were tamper-proof, verifiable, and portable across applications? When I discovered the Polkadot SDK‚Äôs modular blockchain design, the solution became obvious.\nWe could decentralize access control itself, not as another token or DeFi concept, but as rea\n\nHow it was built: PolkaShield is composed of four coordinated components:\n\nWhat's next: A clean UI for: Connecting wallets\nCreating roles\nAssigning roles\nViewing logs",
    "hackathon": null,
    "prize": "Winner Third prize - All Themes",
    "techStack": "rust, substrate",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=ExB94irX7-o",
    "demo": null,
    "team": "Fred Gitonga",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/polkashield-og8vky"
  },
  {
    "title": "Involvee Times",
    "summary": "PROJECT SUMMARY: Involvee Times\n\nInvolvee Times is a platform designed to enhance user engagement and participation in various events or activities. By leveraging advanced technologies, it aims to streamline the process of organizing and joining events, fostering a more connected and interactive community.\n\nKEY ACHIEVEMENTS: The project distinguished itself by effectively addressing user needs for engagement and community building, which resonated with judges. Its innovative approach and user-friendly design contributed to its recognition as a winner, along with the high-quality audio prize of Sony WH-1000XM4, highlighting its potential in enhancing user experience.\n\nThe project utilizes a robust tech stack, including exa for backend management, OpenAI for intelligent features, PostgreSQL for reliable data storage, and React combined with TypeScript for a responsive and scalable front-end. The use of tRPC facilitates seamless communication between the frontend and backend, enhancing overall performance.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as managing data integrity and performance with PostgreSQL, ensuring real-time interactions in the user interface, and integrating OpenAI functionalities effectively. Additionally, user adoption and engagement metrics would need to be continuously monitored and improved.\n\nFUTURE POTENTIAL: Involvee Times has significant room for growth, including expanding its feature set to include personalized event recommendations, advanced analytics for event organizers, and integration with social media platforms. Further development could focus on enhancing community features, such as forums or live discussions, and exploring partnerships with organizations to broaden its use case.\n\nWhat it does: Involvee cuts the friction, finding bills that will directly impact the user, finding events the user could make an impact at, and helping the user voice their opinion.\n\nInspiration: We've experienced firsthand how difficult it is to get involved with local community. Whether it is trying to volunteer, learn about bills that will directly impact you, as well as other general state legislation, there's no good way to find information for you specifically.\n\nHow it was built: Involvee Times uses full-stack Typescript, with React for the frontend, and Express for the backend. tRPC is used to communicate between the two, with Exa and GPT-4o for the search, Twilio and Sendgrid for emailing users, and Postgres for the database. OpenStates was used for the state legislation data.\n\nWhat's next: Our first goal would be to improve the search, to find more options, especially options that are hidden behind pages of search results. We also would like to have more specific filters for users regarding email updates, with the goal to add the ability to filter via tags, categories, etc.",
    "hackathon": null,
    "prize": "Winner Sony WH-1000XM4",
    "techStack": "exa, openai, postgresql, react, trpc, typescript",
    "github": "https://github.com/Bardemic/sous-teach",
    "youtube": "https://www.youtube.com/watch?v=xDhwR7GmhKQ",
    "demo": null,
    "team": "Brandon Pieczka, Luke Patterson, jbaccam Baccam",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/_-tzw4gb"
  },
  {
    "title": "The Voyage",
    "summary": "The Voyage is an immersive web application that utilizes 3D graphics and animations to create an engaging user experience. It leverages modern web technologies to provide a visually captivating journey, potentially showcasing storytelling or educational content in an interactive format.\n\nKEY ACHIEVEMENTS\nThis project stood out for its innovative use of 3D rendering and smooth animations, which enhanced user engagement and interactivity. Winning the overall prize indicates that the project not only excelled in technical execution but also delivered a compelling user experience that resonated with the hackathon judges.\n\nThe project incorporates several advanced technologies including:\n- Three.js for rendering 3D graphics in the browser.\n- GSAP for high-performance animations, enabling smooth transitions and interactions.\n- Drei and postprocessing for adding effects and enhancing visual quality.\n- React and TailwindCSS for building a responsive and modern user interface, ensuring a seamless user experience.\n- TypeScript for improved code quality and maintainability.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to performance optimization, especially when rendering complex 3D models and animations. Ensuring compatibility across different devices and browsers could also have posed difficulties, as well as implementing an intuitive user interface that balances functionality with aesthetics.\n\nFUTURE POTENTIAL\nThe Voyage has significant potential for future development, including expanding its narrative capabilities or integrating augmented reality features for an even more immersive experience. It could evolve into an educational tool, a storytelling platform, or a marketing asset for brands looking to engage users in unique ways through digital storytelling. Additionally, collaboration with artists or storytellers could enhance its content diversity and appeal.\n\nWhat it does: An interact-able 3D website that allows users to explore the planets and collect data by performing tasks. The data collected about planets are stored and can be accessed at any time. The user is in the first person view and is a pilot flying through the void of space.\n\nInspiration: Browsing the NASA website and being able to explore the different planets, and there was a 3D model of a planet that I could explore. I wondered if I could make an immersive experience that can allow users to explore space from the comfort of their own home.\n\nHow it was built: Using React three fiber and GSAP to blend 3D and 2D ui elements together. The model was created by hand in blender, and camera controls and lighting were added to it in React. Then the UI is set over the 3D canvas which allows for interactivity.\n\nChallenges: A lot of issues came from dialogues not showing the correct text so this was solved by creating a dialogue system that relied on knowing the current planet and keeping track of the dialogue index.\n\nAccomplishments: The project overall, adding all the elements together to create that immersive space exploration experience. Being able to replicate a space ship style vibe with the hologram styling and ambience sound effects really made sitting in that cockpit just a bit more realistic.\n\nWhat we learned: How to combine 3D and UI elements to create web interactivity, as well as adding sound and animations for a more engaging user experience.\n\nWhat's next: Adding the rest of the planets, more ways to collect data, and maybe even expanding further than the solar system.",
    "hackathon": null,
    "prize": "Winner Overall Winner",
    "techStack": "drei, gsap, postprocessing, react, tailwindcss, three.js, typescript",
    "github": "https://github.com/HubertYu03/TheVoyage",
    "youtube": "https://www.youtube.com/watch?v=-eux9uhE4J0",
    "demo": "https://the-voyage-seven.vercel.app/",
    "team": "Hubert Yu",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/the-voyage"
  },
  {
    "title": "EqualEyes",
    "summary": "EqualEyes Project Summary\n\nEqualEyes is an innovative project designed to enhance accessibility and inclusivity in digital environments. It employs advanced technologies to provide users with tailored experiences that accommodate various needs, ensuring that everyone can engage with digital content effectively.\n\nKEY ACHIEVEMENTS: EqualEyes distinguished itself by winning both the overall hackathon prize and the Best Pitch award, showcasing its compelling vision and clear communication of impact. The project likely demonstrated a strong user-centric approach, highlighting its potential to address critical accessibility issues in an engaging manner.\n\nThe project was built using Claude, along with standard web technologies such as CSS, HTML, and JavaScript. Notable technical implementations may include dynamic content adjustments based on user feedback, intuitive UI design for ease of navigation, and efficient use of APIs for real-time accessibility enhancements.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring compatibility across various devices and platforms, addressing diverse user needs in terms of accessibility, and maintaining performance while integrating complex features. Additionally, effectively gathering user feedback for continuous improvement could have been a hurdle.\n\nFUTURE POTENTIAL: EqualEyes has substantial potential for evolution, including expanding its features to support more accessibility standards, integrating machine learning for personalized user experiences, and collaborating with organizations to promote wider adoption. Future iterations could also focus on creating community-driven content to continuously adapt and improve accessibility resources.\n\nWhat it does: Our Chrome extension allows you adjust certain features, such as contrast and text of the website based on standards inspired by the Web Content Accessibility Guidelines (WCAG). These guidelines aim to define ‚Äúaccessibility‚Äù with tangible, testable design choices to consider diverse needs.\nBecause every website is designed differently, and everyone has unique needs, one solution does not fix all. So, EqualEyes offers different options that can be toggled at the user's discretion: Dark mode\n\nDarkens the background and makes the text light, so you get a softer, but higher-contrast view that‚Äôs easier on the eyes.\n\nHigh contrast\n\nTurns the background pure black and the text white to maximize contrast.\nThis is a more intense option for people who need really strong color distinction to read c\n\nInspiration: We wanted to create something that improves accessibility or inclusivity while considering the context of ourselves and our peers. As students, we are constantly staring at our laptops, and often studying at night in the dark, while our laptop screen continues to blare at us. But beyond our own privileged experiences growing up able-bodied, we wanted to consider the unique but very relevant struggles of others, such as individuals who are visually impaired or are simply more sensitive to poorly designed website displays. That is where our idea was born! We wanted to build a Chrome extension allows users to adjust certain features to enhance visibility, and in turn, accessibility.\n\nHow it was built: Using JavaScript, HTML, CSS, and online tutorials/debugging with Claude. Lots of struggle. Lots of headache.\n\nChallenges: A big challenge we ran into while creating this project is that most of us were using languages we had never coded in before until Learn Day. So, it was kind of stressful and a big learning curve getting accustomed to the syntax and conventions. Additionally, we had minimal experience in web design or development in general. Taking what we learn solely in school/on our own and applying it to a real-world circumstance was... daunting.\nWe had other features (such as using AI to create descriptions to be read by alt-text for images that do not already have it, to be inclusive of visually-impaired users) we wanted to implement that included APIs, but after around 5-6 hours of attempting, it still wasn't working out, so we decided to focus on other features.\n\nAccomplishments: We are proud of creating a functioning website that is relatively aesthetically pleasing. We had to be very meticulous and persistent in order to link the files we created separately and standardize their styles. Additionally, we are proud of the logo we made as we feel it added a touch of personality and cuteness to the design. Most importantly, we are proud of our developer for making the actual Chrome extension that allows users to toggle different buttons to apply the different effects (the ones that enhance contrast and font). She did not sleep. She had a nosebleed at 7am. She needs to sleep.\n\nWhat we learned: We were exposed to new technical skills this weekend such as web design and development. We gained significant experience (compared to our null starting point...) in new, or slightly familiar languages (JavaScript, HTML, CSS). Additionally, it was satisfying to see the application of what we learned at LearnDay (Saturday morning) to BuildNight (just a few hours later!).\n\nWhat's next: We want to add more features to enhance visibility and accessibility for an even more diverse pool of needs. This looks like building on the alt-text using AI descriptions idea. We would also like to look at more criteria for the standards given by WCAG, so we can make all websites on the internet equally accessible. The future of EqualEyes also looks like building on current limitations, such as if the background or text is an image or something more complex than a solid color. In the end, we would like for EqualEyes to be used on a larger scale, daily, to improve the lives, eye-health, and access to information (e.g. graphs, other informational images) for all people.",
    "hackathon": null,
    "prize": "Winner Best Pitch",
    "techStack": "claude, css, html, javascript",
    "github": "https://github.com/jjessicatsai/EqualEyes",
    "youtube": "https://www.youtube.com/watch?v=uIzLU9dT590",
    "demo": null,
    "team": "jessica tsai, Angela Li, Sara A, Tara A",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/equaleyes"
  },
  {
    "title": "Tuffr",
    "summary": "Tuffr Project Summary\n\nTuffr is an innovative project that leverages AI and edge computing to enhance real-time data processing and analysis. By utilizing advanced computer vision techniques, it aims to provide efficient solutions for various applications, potentially in areas like surveillance, automation, or smart environments.\n\nKEY ACHIEVEMENTS: Tuffr distinguished itself by winning multiple accolades, including the overall winner of the hackathon and specific prizes for the best use of Nodal CLI and Qualcomm EdgeAI. Its ability to effectively integrate these technologies showcased exceptional creativity and practicality, appealing to both technical and business perspectives.\n\nThe project is built with a robust stack that includes Adafruit for hardware integration, Cloudflare for secure and scalable web infrastructure, and OpenCV for advanced image processing. The use of PlatformIO emphasizes a strong focus on embedded systems development, while React provides a dynamic user interface, ensuring a seamless user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to hardware-software integration, particularly in optimizing the performance of edge devices for real-time processing. Additionally, ensuring data privacy and security while utilizing cloud services could have posed significant hurdles during development.\n\nFUTURE POTENTIAL: Tuffr has substantial potential for evolution, with opportunities to expand its application scope into diverse sectors such as smart cities, healthcare, and industrial automation. Future iterations could include more sophisticated AI models, enhanced user interfaces, and broader hardware compatibility, potentially leading to commercial viability and partnerships.\n\nWhat it does: Tuffr lets you choose a workout and sends that information to our AI agent. Using a computer vision model, it tracks your movements and gives you real-time feedback on your form, along with suggestions for improvement. On the left side, you‚Äôll also find a chatbot you can ask for guidance, whether you want to know what to do next or get tips to improve your lift.\n\nInspiration: We set out to make scientific training accessible to everyone. No one should ever feel embarrassed or out of place in a gym; everyone deserves the chance to learn, grow, and train with confidence. To solve this, we created Tuffr to help people analyze their lifts and make sure they can train without injury.\n\nHow it was built: We built Tuffr with a React frontend enhanced by smooth animations using Lenis. Our backend is powered by a Python-based AI agent and chatbot. For computer vision, we use OpenCV with YOLO to detect key points and calculate angles that represent the user‚Äôs form. This information is then sent to the frontend, where we display the user‚Äôs form score in real time.\n\nChallenges: We've had various problems with the ECG electrode configuration and the other software bugs, and we overcame these issues very quickly and were able to create a cohesive product that was able to successfully measure and give data on a person's workout. To go more in-depth, the ECG pads output a very low-strength voltage signal, which we had to amplify using op amps to get appropriate readings.\n\nAccomplishments: Overall our team successfully built the project and was able to give analytics on how the workout went. We increased accuracy for the computer vision model.\n\nWhat we learned: We learned a lot about computer vision and how we can apply AI to lifting and healthcare.\n\nWhat's next: We want to increase the accuracy even more and allow for more sensitive outputs on the form progression bar.",
    "hackathon": null,
    "prize": "Winner Best Use of Nodal CLI; Winner Best Use of Qualcomm EdgeAI",
    "techStack": "adafruit, cloudflare, opencv, openserver, platformio, react",
    "github": "https://github.com/The-Bettr-Tech/FrontEnd",
    "youtube": "https://www.youtube.com/watch?v=ZHPY_1P-ulU",
    "demo": "https://tuffr.tech/",
    "team": "Will Pelech, Eric Dong",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/tuffr"
  },
  {
    "title": "StationScope",
    "summary": "StationScope Summary\n\nStationScope is a project designed to enhance the accessibility and understanding of public transportation data. By integrating various geocoding and mapping technologies, it provides users with an intuitive interface to explore transit stations, demographic data, and service patterns.\n\nKEY ACHIEVEMENTS: The project stood out by winning multiple awards, including the overall winner and categories for education and the Aristotle Challenge. Its innovative use of AI for data analysis and visualization, combined with a focus on user experience in navigating transportation systems, contributed to its recognition.\n\nStationScope employs a diverse tech stack, including CSS for styling, JavaScript and TypeScript for interactive web features, Python and R for data analysis, and integration with Google Maps and MapQuest for geospatial functionalities. The incorporation of US Census Bureau data allows for demographic insights, enhancing the project‚Äôs analytical depth.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data accuracy and integration from multiple sources, ensuring real-time updates for transit information, and creating a user-friendly interface that can handle complex datasets without overwhelming users.\n\nFUTURE POTENTIAL: StationScope could evolve by incorporating real-time transit updates, expanding its geographic coverage, and enhancing AI capabilities for predictive analytics. Additionally, partnerships with transportation agencies could facilitate access to more enriched data, while user feedback could drive further refinements to improve usability and functionality.\n\nWhat it does: It is a fully functional dynamic app. You type in the desired address in the first landing screen, and Google Maps API helps you find a valid address. The app identifies the closest station to your input address as well as maybe additional close stations. Behind the scenes, the app pulls demographic summary stats using a U.S. Census Bureau API for all stations on the same lines as the closest station. We use census tracts partially or fully within a 1/4 mile radius of each subway station and the 2019-2023 5 Year ACS. This information is fed into the bot as reliable quantitative data (less prone to the hallucinations large market AI models can have with numbers). You are prompted to click to on a chat button, after which you can see an interactive map to get a good sense of geographical lay\n\nInspiration: Finding apartments as an intern or new grad moving to NYC is a daunting task. \nI know it. I lived it 4 months ago when I came to the city for an econ consulting role. \nI would have many pages open on Zillow, late nights compiling spreadsheets, and trying my best to avoid unnecessary transfers but failing. I found ChatGPT not very helpful at suggesting me neighborhoods. Since I had little idea of NYC geography, it did not give me a good sense of distance from work without opening up several other tabs of Google maps. (Real experience from Max and speaking with other new grads in NYC) This is why we created StationScope, an interactive chatbot robust against AI hallucinations by pulling reliable data from the U.S. Census Bureau and MTA for quantitative data measures. It is a custom AI with w\n\nHow it was built: To develop the app, we had two main workstreams: the frontend + data modeling in R which Max handled, and the agentic bot and map, which Daren handled. The goal is to educate the new mover to NYC about their options and make a structured recommendation, much like a consultant would. Anyone can understand the inputs and outputs to this app, even though the infrastructure is a complex web of Javascript, R, and API links and endpoints.\n\nChallenges: Coordinating our differing expertises: Daren has more skills in fullstack development, Max has more skills in data science. Smoothly integrating the APIs took some time and a lot of iteration. We slept for 2 hours.\n\nAccomplishments: This is Max's first hackathon!\n\nWhat we learned: Always plan for more time than it seems. Bugs pop up out of nowhere all the time. R backend plus JSON is an unusual combination but it works with a very helpful R library called \"plumber.\" Commit and push often!!!\n\nWhat's next: Making the AI model even more robust. It turned out pretty well, we think, having only one day (and long night) to work on it, though. We also want to finalize the report output and potentially host the site for the public to use.",
    "hackathon": null,
    "prize": "Winner Education; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "css, geocoding, google-maps, javascript, json, mapquest, python, r, typescript, us-census-bureau",
    "github": "https://github.com/hellomaxlee/hack-nyu",
    "youtube": "https://www.youtube.com/watch?v=hL5gwPF4RK0",
    "demo": null,
    "team": "Maxwell Lee, Daren Hua",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/stationscope"
  },
  {
    "title": "Study Desk Assistant",
    "summary": "The Study Desk Assistant is an innovative project designed to enhance the study environment by monitoring and optimizing conditions for better focus and productivity. Utilizing a combination of sensors and artificial intelligence, the system can provide real-time feedback on variables such as temperature, humidity, and noise levels, assisting users in creating an ideal study atmosphere.\n\nKEY ACHIEVEMENTS\nThis project stands out due to its comprehensive integration of various sensors (DHT11 for temperature and humidity, KY-037 for sound detection, and a photoresistor for light levels) with advanced AI capabilities via ChatGPT. Its practical application, user-centric design, and real-time analytics contributed to its recognition as a hackathon winner, showcasing its potential to enhance studying efficiency.\n\nNotable technical implementations include the utilization of Arduino and Raspberry Pi for hardware control and data processing, respectively, along with C++ and JavaScript for software development. The project effectively integrates Node-RED for visual programming, allowing for seamless connections between sensors and outputs, while ChatGPT provides intelligent recommendations based on real-time data.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring accurate sensor calibration and managing the integration of multiple technologies within a cohesive system. Additionally, optimizing the user interface for ease of use and ensuring reliable performance in various environmental conditions would have been critical hurdles.\n\nFUTURE POTENTIAL\nThe Study Desk Assistant could evolve into a more advanced platform by incorporating machine learning algorithms to personalize study recommendations based on user behavior over time. Future iterations could also include cloud connectivity for remote monitoring and control, integration with other smart devices, and expanded functionalities like gamification of study sessions to further engage users.\n\nWhat it does: The Study Desk assistant utilizes sensor readings (temperature, humidity, light, sound) and data and displays them to a dashboard easily accessible by the user to get a quick look at their environment, also sending phone notifications and emails to the user when there are major changes in the sensor readings to better alert them and giving tips on how their can improve their environment with the end goal of giving insight to the user and making them more productive and focused.\n\nInspiration: Many students struggle with being focused and productive and unbeknownst to them, it can be tied back to their study environment. Barley any reliable tools have been make for student with the sole purpose of being able to notify students of their study environment with temperature, humidity, light and sound levels, whilst also sending notifications and giving tips on how ot better improve their environment.\n\nHow it was built: I began by utilizing sensors, DHT11 for the temperature and humidity, Photoresistor for the light level, and a KY-037 for the sound level. I have connected these sensors to an Arduino Uno R3 and used Serial Communication to take the sensor readings and parse them into a JSON format in C++, which then was accessed by my Raspberry Pi 5. Using Node-Red on the Raspberry Pi and connecting my Arduino Uno to it, Node-Red was able to access the Serial Communication I had built earlier and used the parsed JSON format to extract specific sensor values which were outputted as clean graphs easily accessible by the user. I have also accessed a local server, which allowed me to send notifications to my phone and send emails about major changes in the environment whilst giving me tips.\n\nChallenges: The one of the biggest challenge I had run into was the creation of the phone notification and email tips system. Learning how to properly access the sensor values in a way that would check them whilst also keeping track of major changes of the sensors over time to give proper tips that were not repetitive made me think outside the box so that I would not get extreme spammage of notifications just from an extremely small change detected.\n\nAccomplishments: Though I had built multiple projects dealing with sensors and graphs, this was the first time I had properly incorporated all of them together into one seamless app. This was also my first time playing around with a notification and email system, and I am proud to say that it has worked much better than I could have ever expected, being able to access a server and give the user insight leverage this project to a whole nother level.\n\nWhat's next: I would love to add Alexa integration to this project, eliminating the need of the distraction of looking at another app in the middle of your work to get the sensor values of your room. Having an Alexa connected to this would allow someone to simply ask their Alexa for the sensor values and it can give them a quick summary so they can continue their work without being distracted. Notifications would also work much better, as an Alexa would eliminate the need of looking at your phone for update which could lead to another doom scroll session (I am very guilty of this).",
    "hackathon": null,
    "prize": "Winner Top 4-23",
    "techStack": "arduino, ardunio-uno, c++, chatgpt, dht11, javascript, ky-037, node-red, photoresistor, raspberry-pi",
    "github": "https://github.com/LloydTheCoder/Study-Desk-Assistant",
    "youtube": null,
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/iot-desk-assistant"
  },
  {
    "title": "Understandly",
    "summary": "Understandly Project Summary\n\nUnderstandly is a web-based application designed to enhance user comprehension of complex topics through interactive content and visual aids. By leveraging engaging design and intuitive navigation, the platform aims to simplify learning and make information more accessible to a wider audience.\n\nKEY ACHIEVEMENTS: Understandly stood out by effectively combining aesthetics with functionality, creating a user-friendly interface that enhances learning experiences. Its innovative approach to breaking down complex subjects likely contributed to its recognition as a winner in the hackathon, showcasing both creativity and practicality.\n\nThe project utilizes CSS for styling, HTML for structure, and JavaScript for interactivity, enabling dynamic content presentation. Notable implementations may include responsive design for various devices, seamless animations for transitions, and perhaps the use of APIs to pull in relevant data or resources.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user engagement and content accuracy, ensuring that the information presented was both simplified and still comprehensive. Additionally, technical challenges may have arisen from optimizing the application for performance and cross-browser compatibility.\n\nFUTURE POTENTIAL: Understandly has the potential to evolve into a comprehensive educational platform by incorporating features such as user-generated content, collaborative learning tools, and integration of AI to personalize learning experiences. Expanding to mobile applications and multilingual support could also broaden its reach and impact.",
    "hackathon": null,
    "prize": null,
    "techStack": "css, html, javascript",
    "github": "https://github.com/ChirculeteMihai/Understandly",
    "youtube": "https://www.youtube.com/watch?v=9rsFjbyaxmY",
    "demo": "https://chirculetemihai.github.io/Understandly/",
    "team": "Chirculete Mihai",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/understandly"
  },
  {
    "title": "BOBUX BARGAINS",
    "summary": "Project Summary: Bobux Bargains\n\nBobux Bargains is a creative project designed within the Roblox ecosystem that likely focuses on providing users with engaging, gamified shopping experiences. The project integrates AI elements, possibly allowing users to interact with cutting-edge AI agents to discover or negotiate virtual bargains, enhancing the overall user experience.\n\nKEY ACHIEVEMENTS: Bobux Bargains stood out by winning multiple prizes, including the \"Best Funny Haha Hack\" and the \"Aristotle Challenge: Most Cutting-Edge AI Agent.\" This indicates that the project not only entertained users with humor but also showcased innovative use of AI technology, setting it apart from other hackathon entries.\n\nThe project was built using Luau, a scripting language for Roblox, and leveraged tools such as OpenRouter for AI functionalities. The integration of AI agents likely facilitated dynamic interactions within the game, enhancing player engagement through personalized shopping experiences.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the AI agents provided relevant and engaging interactions without overwhelming users. Additionally, optimizing the user interface for a seamless shopping experience within the Roblox platform could have posed design and technical hurdles.\n\nFUTURE POTENTIAL: Bobux Bargains has significant potential for evolution by expanding its features to include more sophisticated AI-driven interactions, partnerships with brands for virtual goods, or even integrating blockchain technology for secure transactions. It could also explore community-driven content creation, allowing users to contribute to the bargains offered, thereby enhancing engagement and user retention.\n\nWhat it does: Bobux bargains simulates a real world economy including supply and demand and financial transactions, on Roblox! There are two kinds of AI agents at play in our project, Buyers and Sellers. Buyers simulate buyers of goods and services in the real world who want to find the best price available for a product in a good and services market. Sellers are businesses that have the product buyers want and aim to earn the most profit out of each transaction. Each AI agent is fully customizable and can use a variety of AI models using the Open Router API. This allows the application to test the reasoning and negotiation capabilities of each model. Bobux Bargains is also scalable, allowing more accurate tests and economic modelling.\n\nInspiration: Whenever we think about finance software, we think of boring, unintuitive software, with exorbitant licensing costs and little accessibility or ease to the general public to understand. Oftentimes, processes are also hard to visualize. Therefore, we want to combine the power of AI agents and common variety game engines to simulate a market.\n\nHow it was built: First, our environment of choice is Roblox and Roblox Studio, as the games produced on the platform are highly accessible to anyone with a device. Furthermore, it is an easy platform to quickly prototype and develop 3D environments with player models, making it easy to attach our agents to an NPC. The agents print their conversations to the Roblox game NPC chat. For our actual agents, we are using OpenRouter to allow us to easily pick different LLMs to experiment with and potentially build agents with different personalities. To develop our prompts, our application keeps track of historical negociations that the buyer has conducted and feed them back to the language model, which means the buyer can remember deals it has made and make decisions based on past experience to conduct future dea\n\nChallenges: Our challenges actually mainly lies with building the simulation environment itself\nGetting automated Roblox characters to communicate with each other\nGetting elements of the game to actually interact with each other in the physical space in a way that makes sense\nDealing with race conditions between element rendering and agent / other API calls within the game engine\nVersion control\n\nAccomplishments: First and foremost, we are really proud of our concept and design, which also scales to various other AI Agent simulations and tasks. We are also proud to have built a full-stack application within the time constraint and within such a unique environment.\n\nWhat we learned: A key learning includes how to simulate negotiation. We often intuitively know how to conduct negotiations ourselves at least at a basic level, but trying to instruct the AI agents to conduct sensible negotiations requires a deeper understanding of the mechanics of sale negotiation. Furthermore, we also learned how to build games and work with 3D spaces effectively. For the most part, this our first part ever building in 3D environments, so from collision to movement, we now have a better understanding of 3D game engines.\n\nWhat's next: We want to expand BOBUX BARGAINS to have more buyer and seller agents, while also allowing players to negotiate with other NPCs and players. With 10 BILLION agents, we can spend $450 000 PER SECOND in OpenRouter credits and simulate the BEST and MOST ACCURATE economic model of the real-world marketplace there is.",
    "hackathon": null,
    "prize": "Winner Best Funny Haha Hack; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "luau, openrouter, roblox, robloxstudio",
    "github": "https://github.com/jennnniferkuang/Bobux-Bargains",
    "youtube": "https://www.youtube.com/watch?v=xkyIFGGAyg0",
    "demo": "https://www.roblox.com/games/140073423730757/Boxub-Bargains",
    "team": "Seller registry API, Jennifer Kuang, Amelia Song, David Hang",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/bobux-bargains"
  },
  {
    "title": "Zenith",
    "summary": "Zenith Project Summary\n\nZenith is a web application designed to provide users with an interactive and engaging platform, likely focused on a specific niche or community service. While specific functionalities are not detailed, the project emphasizes user engagement and accessibility, making it particularly suitable for beginners in technology.\n\nKEY ACHIEVEMENTS: Zenith stood out at the hackathon by winning the \"Freshly Unearthed: Best Beginner Hack\" award, indicating that it effectively catered to novice users while demonstrating creativity and innovation. Its recognition as a winner suggests that it met a need in its intended audience, offering a user-friendly experience that resonated well with judges.\n\nThe project was built using a combination of modern web technologies, including CSS for styling, Firebase for backend services, and JavaScript for interactivity, showcasing a solid understanding of full-stack development. The use of Git and GitHub for version control indicates a collaborative approach to coding, enhancing team productivity and project management.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to integrating various technologies seamlessly, particularly ensuring the backend services from Firebase worked efficiently with the frontend components. Additionally, as a beginner project, there may have been hurdles in user experience design and functionality testing to ensure a smooth user interaction.\n\nFUTURE POTENTIAL: Zenith has the potential to evolve into a more comprehensive platform by incorporating advanced features such as user authentication, enhanced data analytics, or community-building tools. As the team gains more experience, they could expand its scope to include mobile compatibility or integrate additional services, making it a versatile tool for a broader audience.\n\nWhat it does: Zenith is mainly a task tracker/to-do list. When it is your first time on the site it will auto-populate with a few wellness focused tasks but you can easily delete those and add your own and assign them points values. You gain points as you complete tasks as you work to build a rocket ship. Once you have enough points to launch you will gain another module in your space station \"Zenith\" that you are able to see by clicking on the space-station button on the navbar. Shooting across the sky in this view are self care comets, if you click on it, a pop-up with a wellness tip will appear. Once you launch, your selected tasks are deselected, you can either delete them if it was a one time task or keep it if it is a reoccurring task (good for weekly assignments or daily wellness goals). Don't wo\n\nInspiration: Both of us are on the the satellite development program at UMD (project THEIA) and are very passionate about space. We both also have a heavy work load and do not prioritize our physical and mental wellness as well as we should. We wanted to make a website that would allow us to keep track of all of our tasks while still giving us wellness reminders (for when we inevitably do not add them to our task list) and also incorporate our love for space. Both of us joined this hackathon to find a community of women with similar interests to us, when you find the right community of people it makes it easier to navigate spaces where you are the outlier. Throughout our journeys in coding and with space development, both of us know what it is like to be the only or one of only a few women in a room. W\n\nHow it was built: We are using Firebase for hosting our site since one of our members has used it on a previous project and is familiar with the application. For front-end development, we are using HTML and CSS and JavaScript to add our functionality and interactive elements on the back-end. Since there were two of us working together, we used Git to maintain version control and to allow for simultaneous coding on different branches without merge conflicts.\n\nChallenges: In the beginning we ran into a lot of merge conflicts because both of us had limited experience working with Git merges before. After some troubleshooting, we were able to get a better idea of how to fix them and generally avoided merge conflicts for the rest of the hackathon. Also, between the two of us one of us does not have a lot of JavaScript experience and the other does not have a lot of experience with any of the coding languages that we used for this project, so there was a learning curve when figuring out how everything needs to be structured in the code.\n\nAccomplishments: For one member this was their first hack and for the second, it was her second time doing a project but her first time being a main coder for the project. When we first started, we were unsure of how much functionality we would be able to implement. We ended up placing a lot of features into the \"If we have time at the end\" category. We were able to knock out basically that entire category and more. We would keep finding new and better features to add to enhance the project. Since coming here, we were unsure if we would even have something to present. We are very proud to that we finished and are demoing our project.\n\nWhat we learned: Both of us built upon our current knowledge of Git. We now are better at debugging merge conflicts and how to avoid them. We also learned how different branches work and how to merge those branches to the main (and double checking to make sure you pull first). We also learned a lot more about web development especially when it comes to dynamic and interactive web applications. Additionally, we learned to incorporate moving elements, including layering a moving interactive background that covers the entire page over other interactive elements on the page.\n\nWhat's next: For Zenith we would like to add more variety and customization in the modules that are being added to your space station. It will make it more exciting to see your Space Station grow if there are different ways that it can be built. Additionally, instead of just seeing the outside, being able to actually see the different interiors of the Space Station modules is a future feature we would like to implement. The interior design could also be your choice based on what the module is, to add more customization for the user. We would also like to make it where there is a second section on the home page, specifically for wellness where a user can input as many goals as they want and everyday the list will randomly populate with a few of them. This will make it where it is more exciting to keep u",
    "hackathon": null,
    "prize": "Winner [Technica] Freshly Unearthed: Best Beginner Hack (College)",
    "techStack": "css, firebase, git, github, html, javascript",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=r_CqMXF6ia8",
    "demo": "https://zenith-sky.work/",
    "team": "Madison Lawson, Bri Rosado",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/zenith-kxw084"
  },
  {
    "title": "DocSimplified",
    "summary": "DocSimplified Project Summary\n\nDocSimplified is a web application designed to simplify document management and processing using advanced generative AI and machine learning technologies. It leverages these technologies to enhance user interaction with documents, likely streamlining tasks such as editing, summarization, and organization for improved efficiency.\n\nKEY ACHIEVEMENTS:  \nDocSimplified stood out in the hackathon by winning the \"Most Transformative Hack Leveraging Generative AI/ML\" prize from Intuit. This recognition highlights its innovative use of generative AI to fundamentally improve how users interact with documents, showcasing a significant leap in processing capabilities and user experience.\n\nThe project employs a robust technology stack, including dropzone.js for file uploads, Eleven Labs for voice synthesis, OpenAI for natural language processing, and React for a dynamic user interface. The integration of Supabase for backend services and Tailwind for responsive design further enhances its functionality and user experience.\n\nPOTENTIAL CHALLENGES:  \nThe team likely faced challenges such as ensuring data privacy and security when handling sensitive documents, optimizing the performance of AI algorithms for real-time processing, and creating a user-friendly interface that balances complexity with usability. Additionally, integrating multiple technologies seamlessly can pose significant technical hurdles.\n\nFUTURE POTENTIAL:  \nDocSimplified has strong potential for evolution into a comprehensive document management system that could incorporate features like collaborative editing, advanced analytics for document usage, and integration with existing enterprise solutions. Future developments could also explore expanding its AI capabilities to include more languages and dialects, enhancing accessibility and user reach.\n\nWhat it does: DocSimplified is a multilingual AI-powered web application that simplifies government documents. Users upload any fillable PDF, and the app automatically detects form fields, summarizes each section in clear steps, translates the content into their preferred language, and guides them through filling it out. It even includes text-to-speech for users with low vision or reading difficulties. The goal: make complex forms feel intuitive and approachable.\n\nInspiration: Government forms are notoriously confusing ‚Äî even for families who have lived in the U.S. for decades. Three out of four of our team members come from immigrant households, and we‚Äôve all watched our parents struggle with taxes, visa renewals, and federal documentation because of language barriers, dense terminology, and inaccessible formatting. We wanted to build a tool that makes essential government services understandable for everyone, regardless of literacy level or language proficiency.\n\nHow it was built: Frontend: React and Tailwind for a clean, accessible UI; pdf-lib for rendering and editing fillable PDFs.\nBackend: Supabase for authentication, user profiles, and document storage.\nAI Processing: OpenAI 4o API for summarization, language translation, and adaptive explanations tuned to the user‚Äôs education level.\nAccessibility: ElevenLabs for high-quality text-to-speech output in multiple languages.\nWe tied all of these components together to create a smooth workflow from upload ‚Üí analysis ‚Üí guided form-filling ‚Üí saving progress.\n\nChallenges: Our initial PDF viewer couldn‚Äôt read embedded form fields, forcing us to switch to a more reliable parsing method.\nOCR-based image uploads were too time-intensive to implement during the hackathon.\nEarly OpenCV bounding-box detection only recognized perfect rectangles, while many real forms used underlines or irregular shapes.\nIntegrating multiple third-party APIs while keeping the interface fast and accessible required lots of debugging and design iteration.\n\nAccomplishments: Delivering a fully functional, end-to-end document simplification and form-filling experience.\nBuilding a genuinely accessible tool with translation, easy summaries, and text-to-speech baked in.\nCreating a polished and intuitive UI despite the technical complexity underneath.\nStaying true to our mission of helping immigrant and low-literacy communities ‚Äî the people who inspired this project in the first place.",
    "hackathon": null,
    "prize": "Winner [Intuit] Most Transformative Hack Leveraging Generative AI/ML",
    "techStack": "dropzone.js, elevenlabs, node.js, openai, pdf-lib, react, supabase, tailwind, vercel, vite",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=UwHuJXa4LVw",
    "demo": "https://www.docsimplified.us/",
    "team": "Bassil Shalaby, Dvij Raicha, William Conner, Shanmukha Pothukuchi",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/docsimplified"
  },
  {
    "title": "StudyBuddy Pro",
    "summary": "StudyBuddy Pro Summary\n\nStudyBuddy Pro is an innovative educational tool designed to enhance collaborative studying for students. By leveraging interactive features and visual aids, it allows users to organize study sessions, share materials, and track their progress effectively.\n\nKEY ACHIEVEMENTS: This project stood out due to its intuitive user interface and seamless integration of various tools that facilitate collaborative learning. Winning both the main prize and the People‚Äôs Choice award highlights its broad appeal and effectiveness in addressing real-world educational challenges.\n\nThe project employs a robust tech stack, including React for dynamic user interfaces and Tailwind for responsive design. Notably, it incorporates libraries such as Recharts for data visualization and html2canvas/jspdf for exporting study materials, showcasing a blend of performance and usability.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to ensuring cross-platform compatibility and optimizing performance for real-time collaboration. Additionally, user engagement and retention strategies would have required careful planning and execution.\n\nFUTURE POTENTIAL: StudyBuddy Pro has significant potential for evolution by integrating AI-driven personalized learning paths, expanding features for gamified learning experiences, and possibly incorporating community-driven content sharing. This could further enhance its value in the educational technology landscape.\n\nWhat it does: StudyBuddy Pro is a comprehensive study management platform that combines: Assignment Management: Track all assignments with subjects, due dates, estimated hours, and difficulty levels\nAI-Powered Scheduling: (Planned feature) Generate intelligent study schedules that break down large tasks into manageable daily sessions\nPomodoro Timer: Built-in productivity timer with automatic progress tracking linked to specific assignments\nVisual Analytics: Interactive charts showing weekly study hours, subject distribution, and assignment progress\nSmart Notifications: Deadline alerts (3 days, 1 day, same day), daily study reminders, and inactivity nudges\nGamification System: 8 achievement badges to encourage consistent study habits (Early Bird, Night Owl, Perfect Week, etc.)\nExport Functionality: Gener\n\nInspiration: 87% of students procrastinate on assignments, and 90% struggle with time management. As a student myself, I've experienced the overwhelming feeling of juggling multiple deadlines without clear visibility on progress. I wanted to build a solution that not only tracks assignments but actively helps students develop better study habits through intelligent scheduling, real-time feedback, and motivation.\n\nHow it was built: Tech Stack: Frontend: React 18 + TypeScript for type-safe, component-based UI\nBuild Tool: Vite for lightning-fast development and hot module replacement\nStyling: Tailwind CSS + Radix UI (shadcn/ui) for beautiful, accessible components\nData Visualization: Recharts for interactive charts (weekly hours, subject distribution, progress tracking)\nAnimations: Framer Motion for smooth transitions and engaging interactions\nData Persistence: Browser localStorage for client-side data storage\nNotifications: Browser Notification API for deadline and study reminders\nExport: jsPDF and html2canvas for generating downloadable reports\nDevelopment Platform: Lovable.dev for rapid prototyping and deployment\nHosting: Lovable for fast, reliable deployment Development Process: Research Phase: Analyzed student pai\n\nChallenges: 1. Timer-Assignment Integration\nThe biggest challenge was linking Pomodoro timer sessions to specific assignments and automatically updating progress. I had to design a robust data structure that tracked: Session timestamps\nDuration (work vs break sessions)\nAssignment IDs\nCompletion status Solution: Created a session logging system in localStorage with real-time progress calculation based on completed vs estimated hours. 2. Real Study Streak Calculation\nCalculating an accurate \"study streak\" required handling edge cases: What counts as a study day? (At least one completed work session)\nHow to handle missed days gracefully\nTimezone considerations for accurate date comparisons Solution: Implemented a grace period system and date normalization to accurately track consecutive study days. 3. Ch\n\nAccomplishments: ‚úÖ Built a fully functional app in 48 hours with 6+ major features\n‚úÖ Real-time progress tracking that eliminates manual data entry\n‚úÖ Beautiful, responsive design that works seamlessly on mobile and desktop\n‚úÖ Data-driven insights with 3 different chart types for comprehensive analytics\n‚úÖ Professional export functionality for sharing progress with parents/teachers\n‚úÖ Gamification system with 8 achievement badges to keep students engaged\n‚úÖ Accessibility-first design with keyboard navigation, ARIA labels, and dark mode\n‚úÖ Zero-backend MVP - entire app runs client-side with localStorage\n‚úÖ Interactive presentation website built with the same tech stack\n‚úÖ Real student validation - tested with peers who confirmed it solves real problems\n\nWhat we learned: Technical Skills: Advanced React patterns (custom hooks, context API, prop drilling avoidance)\nTypeScript best practices for type-safe development\nData visualization with Recharts (responsive charts, tooltips, legends)\nBrowser APIs (Notifications, localStorage, date-fns for date manipulation)\nPDF/CSV export implementation from scratch\nFramer Motion for production-quality animations\nTailwind CSS utility-first styling philosophy\nAccessibility best practices (ARIA, keyboard navigation, color contrast) Product Design: The importance of user research (talking to students revealed pain points I hadn't considered)\nSimplicity beats feature bloat (focused on core features that matter most)\nVisual feedback is critical (progress bars, charts, and badges keep users engaged)\nGamification works (achieve",
    "hackathon": null,
    "prize": "Winner People‚Äôs Choice",
    "techStack": "css, date-fns, framer, html2canvas, jspdf, lovable, lucide, motion, radix, react, recharts, router, sonner, tailwind, typescript, ui, vite",
    "github": "https://github.com/linfordlee14/studybuddypro",
    "youtube": "https://www.youtube.com/watch?v=48DeaEq4Ltw",
    "demo": "https://studybuddyprov-presentations.vercel.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/studybuddypro"
  },
  {
    "title": "BuckBounty",
    "summary": "BuckBounty is a fintech project designed to leverage advanced AI technologies to enhance financial insights and decision-making for users. By integrating multiple APIs and leveraging machine learning, it aims to provide real-time data analysis, personalized recommendations, and streamlined financial transactions.\n\nKEY ACHIEVEMENTS\nBuckBounty stood out in the hackathon for its innovative use of AI and robust financial data integration, earning recognition as the overall winner, as well as accolades for being the most cutting-edge AI agent. Its unique combination of user-centric design and advanced analytics positioned it as a frontrunner in both fintech and AI categories.\n\nThe project utilized a diverse tech stack, including FastAPI for backend development, Next.js and React for frontend, and several APIs such as Plaid and Google Gemini for financial data and AI capabilities. The use of FAISS for vector databases and sentence-transformers enhances data retrieval and processing efficiency, while Redis optimizes performance with caching strategies.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating various APIs and ensuring data consistency across platforms. Additionally, maintaining user privacy and security in handling sensitive financial information would have required careful attention. Implementing a seamless user experience while managing complex backend processes may have also posed difficulties.\n\nFUTURE POTENTIAL\nBuckBounty has significant potential to evolve into a comprehensive financial advisory platform that incorporates machine learning for predictive analytics, enhancing user engagement and decision-making. Future iterations could focus on expanding its user base, integrating more financial services, or exploring partnerships with established fintech companies to broaden its market reach.\n\nWhat it does: BuckBounty is an AI-powered personal finance assistant that: Maximizes savings through credit card reward optimization (analyzes your spending patterns and recommends the best cards for each category)\nFinds hidden deals via automated coupon scraping from Gmail, Honey, and Rakuten\nBuilds wealth by converting your savings into personalized investment portfolios with 10-year projections\nProvides intelligent insights using a multi-agent AI system (MARK orchestrates specialized agents for coupons, financial news, and market analysis)\nOffers voice-enabled chat with ElevenLabs text-to-speech for hands-free financial management\nIntegrates with Plaid for automatic transaction syncing and real-time spending analysis\nAnalyzes prediction markets through PolyMarket integration with risk assessment and\n\nInspiration: We noticed a common problem: people leave thousands of dollars on the table every year through suboptimal credit card usage, missed deals, and lack of financial awareness. Traditional budgeting apps just show you numbers, but they don't actively help you make money. We wanted to build an AI-powered financial copilot that doesn't just track spending‚Äîit turns every transaction into an opportunity to save, optimize, and build wealth.\n\nHow it was built: Frontend: Next.js 14 with TypeScript for a modern, responsive interface\nShadCN UI components with custom glass morphism effects\nRecharts for interactive spending visualizations (radar charts, pie charts)\nVanta.js for immersive 3D animated backgrounds\nWeb Speech API + ElevenLabs for voice interactions Backend: FastAPI server with multiple specialized AI agents using MCP (Multi-agent Coordination Protocol)\nGoogle Gemini for natural language processing and embeddings\nRAG (Retrieval-Augmented Generation) with dual vector indexing:\n\nFAISS FLAT index for current month (fast exact search)\nHNSW index for historical data (efficient approximate search)\n\nRedis caching layer for sub-50ms response times on repeated queries\nWeb scraping agents (BeautifulSoup + Selenium) for automated coupon aggregation\n\nChallenges: Vector search scalability: Initially, semantic search on 10,000+ transactions was slow. We solved this by implementing a dual-index system‚ÄîFLAT for recent data, HNSW for historical‚Äîwith automatic migration.\nLLM response caching: Gemini calls were expensive and slow (~2.5s per request). We built a Redis-based caching layer that reduced identical queries to <50ms, a 50x improvement.\nMerchant name matching: User transactions have messy merchant names (e.g., \"STARBUCKS #12345 SEATTLE WA\"). We implemented fuzzy matching with embeddings to accurately match @Starbucks mentions to actual transaction data.\nReal-time agent coordination: Coordinating multiple AI agents (MARK, BountyHunter1, BountyHunter2) without conflicts required building a custom MCP server with status tracking and task queuing.\nC\n\nAccomplishments: 50x performance improvement through intelligent Redis caching\nMulti-agent AI orchestration that actually works in production\nDual-index RAG system handling 10,000+ transactions with sub-second search\nAutomated wealth building that converts credit card savings into actionable investment portfolios with real fund recommendations\nBeautiful UI/UX with glass morphism, 3D backgrounds, and smooth animations\nVoice-enabled interface that makes finance management accessible hands-free\n@ mention feature that provides instant merchant-specific insights\nProduction-ready security with Plaid OAuth and proper API key management\n\nWhat we learned: RAG architecture: Implementing FAISS with dual indexing taught us the tradeoffs between exact and approximate search. FLAT is perfect for small, current datasets while HNSW scales to millions of vectors.\nLLM optimization: Caching isn't just about speed‚Äîit saves real money. Our Redis layer reduced API costs by 70%.\nMulti-agent systems: Coordination is hard. We learned to design agents with clear responsibilities and avoid circular dependencies.\nFinancial domain knowledge: Credit card reward structures, investment portfolio theory, and prediction markets‚Äîbuilding BuckBounty required deep research into personal finance.\nUX matters: The most powerful AI is useless if the interface is confusing. We iterated heavily on the dashboard and chat experience.\n\nWhat's next: Machine learning for budget predictions: Train models on transaction history to predict future spending and warn users before they overspend.\nTax optimization agent: Automatically identify tax deductions from transactions (charitable donations, business expenses, medical costs).\nAutomated bill negotiation: Use AI to call service providers (phone, internet, insurance) and negotiate lower rates.\nSocial features: Allow users to compare anonymized spending patterns with peers and compete on savings leaderboards.\nMobile app: React Native app with push notifications for real-time deal alerts and bill reminders.\nOpen banking expansion: Support for international banks beyond Plaid's coverage using Teller, Finicity, and MX.\nCryptocurrency integration: Track crypto portfolios alongside traditional a",
    "hackathon": null,
    "prize": "Winner Fintech; Winner Aristotle Challenge: Most Cutting-Edge AI Agent",
    "techStack": "beautiful-soup, elevenlabs-api, faiss-vector-database, fastapi, google-gemini-api, next.js-14, plaid-api, polymarket-api, python, react-18, recharts, redis, selenium, sentence-transformers, shadcn-ui, stripe, tailwind-css, typescript, vanta.js, web-speech-api, yahoo-finance-api, zustand",
    "github": "https://github.com/NandanHemanth/BuckBounty",
    "youtube": "https://youtu.be/rCsw2WkivC0",
    "demo": "https://youtu.be/rCsw2WkivC0",
    "team": "Nandan Hemanth",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/buckbounty"
  },
  {
    "title": "Saathi",
    "summary": "Saathi is a web application designed to streamline document management and collaboration, leveraging interactive data visualizations and efficient file processing. It integrates various tools to enable users to create, edit, and manage documents seamlessly while providing analytical insights through visual charts.\n\nKEY ACHIEVEMENTS\nSaathi distinguished itself by winning the hackathon and the Best Documentation prize, indicating a strong emphasis on usability and clarity in its presentation. Its comprehensive approach to document management and collaboration sets it apart, showcasing innovative use cases that enhance productivity and user experience.\n\nThe project utilizes a robust tech stack including Django for backend development, PostgreSQL for database management, and Chart.js for dynamic data visualizations. Additionally, it employs Python libraries such as PyMuPDF and python-docx for document processing, while Supabase provides backend-as-a-service capabilities. Tailwind CSS is used for stylish and responsive front-end design, ensuring an appealing user interface.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as integrating various technologies seamlessly, optimizing performance for document processing, and ensuring compatibility across different file formats. Additionally, thorough testing for usability and functionality within a limited timeframe would have been critical to the project‚Äôs success.\n\nFUTURE POTENTIAL\nSaathi could evolve by incorporating advanced AI features for document analysis and automated content generation, enhancing user experience. Expanding its collaborative features to include real-time editing and feedback could attract more users. Additionally, integrating third-party services for enhanced functionality and exploring mobile app development could further broaden its user base and application in diverse industries.\n\nWhat it does: At first glance, Saathi may seem just another chatbot that can generate quizzes. But what if it actually reduces cognitive dependency on AI and can help humans grow? From calculus to code, Saathi is built to guide you, nudge you, and explain to you the fundamental concepts, not solve the problem for you and hamper your learning. It pushes you to apply STEM concepts through quizzes that test active recall, not passive reading. And when you see your progress visualized over time, the momentum builds, making you actually want to keep going.\n\nInspiration: With the rise of vibe coding, almost everyone‚Äîincluding me, at some point‚Äîare or have been stuck in this AI hell where the lines of knowing something and actually being able to do it blur. It's like you know the content, the code, the logic, but actually writing it...just feels impossible. Saathi was built not to show you the destination or to drag you to the finish line, but to keep you walking. Something like this could save me‚Äîand probably many others‚Äîa lot of the hours wasted begging LLMs to teach instead of handing over full answers. Automating learning isn't about taking away the mental strain, because that is how you learn.\n\nHow it was built: Frontend: Built with Tailwind CSS. The static HTML/CSS layouts were generated using Stitch to speed up UI development, allowing me to focus on the logic. Chart.js was used to make the beautiful and interactive charts. Backend: I chose Django as the Python framework for this project as I am the most comfortable with it. Along with that, libraries like Markdown It, PyMuPdf,  Python-docx, and Functools have been utilized for displaying the LLM's response in a structured manner, reading and extracting text from pdf and .docx files, and making wrapper functions for views where login is required respectively. The authentication is handled using Supabase whereas the user's profile and quizzes are stored in a local Postgres DB. AI: Saathi itself is powered by a Gemini 2.5 flash model with a system\n\nChallenges: *Supabase's RLS policies: *\nIt  prevent unauthenticated users from inserting data into tables like profiles. But new users need a profile row created before they can authenticate. This meant new users couldn't create profiles because they weren't authenticated, but couldn't authenticate without a profile existing.\n-> Instead of using Supabase for cloud storage, I connected Postgres SQL to Django, and made a local database with the tables \"Profiles\" and \"Quizzes\". This allowed for hassle-free row inserts and smooth authorization *Quiz Parsing: *\nNo matter what prompt was used, Gemini did not always follow it and occasionally returned textual response which broke the quiz generation since using json.loads() failed instantly. Manually parsing and structuring into dictionary containing lists c\n\nAccomplishments: Building all of this solo: Of course the project making could be way smoother if I had a teammate, but looking back and realizing I did all of this (code, documentation, demo, filling out the Devpost project details, etc.) all by myself actually makes me proud.\n*Participating: * Yes, even participating was a really huge challenge for me as I was so nervous about this hackathon, people seemed way more experienced and skilled, a lot of people had a clear idea and strong team and initially it was all just so intimidating, especially participating solo, but I am glad that I did it:)\n Deployement:  It was my first time deploying a web app and I had no idea what I was doing, but with a little bit of Youtube tutorials and asking Claude to explain me the bits and pieces I didn't get, I DID IT.\n\nWhat we learned: *Find a team, with the same timezone: * Time zone, no matter what you do, always is a bit of a barrier sometimes when it comes to meetings and ideation, regardless a team helps a lot. Not only with coding but also with ideating and the emotional support that comes along with one.\n*Use Flask: * As much as I love Django and am really comfortable with it, it is such a pain to set it up sometimes, of course the database management and authentication gets really easy, but in a hackathon where you have to build and present in 48 hours, Django might not always be the best choice. \n*Write a script for the demo: * I thought I could just record the demo video without any script or prep and say what comes to mind, then I ended up re recording like 15 times and realized why I need one. \n*How to deploy\n\nWhat's next: Spaced repetition, flashcard generation, automatic resource finder, and integration with FocusAI, another EdTech project I have that can go well with Saathi. For more information, make sure to read the documentation",
    "hackathon": null,
    "prize": "Winner Best Documentation (Sponsored by Gitbook)",
    "techStack": "chart.js, django, gemini-api, javascript, postgresql, pymupdf, python, python-docx, supabase, tailwind",
    "github": "https://github.com/Aruniaaa/CSG-Hackathon",
    "youtube": "https://www.youtube.com/watch?v=D3-EYI0bZ5o",
    "demo": "https://saathi-production-1513.up.railway.app/",
    "team": "Charu Singhania",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/saathi-f291s5"
  },
  {
    "title": "InvestyBesty",
    "summary": "InvestyBesty is a financial technology application designed to empower users with investment knowledge and tools. By integrating various APIs, it provides real-time financial data, analytics, and educational resources, aiming to enhance users' investment decision-making skills.\n\nKEY ACHIEVEMENTS\nInvestyBesty stood out by winning the T. Rowe Price Investor Education Challenge, demonstrating its effectiveness in promoting investor education. Its ability to combine real-time data with user-friendly interfaces likely contributed to its appeal, making it a valuable resource for both novice and experienced investors.\n\nThe project utilizes a robust tech stack, including Next.js for server-side rendering and React for dynamic UI components, ensuring a responsive user experience. The integration of multiple APIs (e.g., alpha-vantage-api, financial-modeling-prep-api, sec-edgar-api) allows for comprehensive financial data aggregation, while Tailwind CSS and CSS4 enhance the application's visual design. The use of TypeScript adds type safety, improving code reliability and maintainability.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges in seamlessly integrating various data sources into a cohesive user interface while maintaining performance. Additionally, ensuring data accuracy and handling the complexities of financial regulations could pose significant hurdles. Scalability and security concerns associated with handling sensitive financial information may also be critical issues.\n\nFUTURE POTENTIAL\nInvestyBesty has the potential to evolve into a comprehensive investment platform by incorporating features like personalized investment recommendations, social trading capabilities, and advanced analytical tools. Expanding its educational content and community features could further enhance user engagement and retention, positioning it as a go-to resource for investment learning and strategy.\n\nWhat it does: InvestIQ is a full-stack, AI-powered platform delivering validated, commercially-ready financial mastery and compliance.\nValidated ML & i18n: Through a multi-language Beta Pilot Program (powered by active i18n), we validated the predictive accuracy of our Advanced ML model for personalized lesson sequencing using real user data.\nRegulatory Compliance & Audit: The platform includes a working Compliance Layer and has already completed Preliminary External Auditing in preparation for seed funding and future regulatory filing.\nData Simplification: Advanced Visualization tools translate complex sentiment-analysis and predictive-model outputs into clear insights.\nHolistic Application: Includes Paper Trading, Behavioral Finance Coaching, and debt-management guidance‚Äîsupporting the 47% of American\n\nInspiration: Generic financial education fails learners, costing the average American an estimated $1,015 each year in poor decisions. The result is an equity crisis‚ÄîGen Z scores only 38% on financial-literacy assessments. We built InvestIQ to fix this using Generative AI: a dynamic, personalized mentor that turns low confidence into real financial capability.\n\nHow it was built: We designed an enterprise-grade system for security, validation, and global scalability.\nValidation: Ran a full Pilot Program using the active i18n framework to test platform stability and predictive-model accuracy against live interactions.\nCompliance & Security: Built the Regulatory Compliance Layer and engaged in External Auditing to secure the codebase and prepare for market entry.\nAI/ML Core: Integrated Generative AI for content and Advanced ML for predictive sequencing, supported by visualization tools that render complex outputs simply.\nAPI & Integration: Integrated with brokerage APIs and developed an NLP sentiment-analysis model.\n\nChallenges: Audit & Pilot Data: Preparing for External Auditing and securely managing multi-language pilot data for ML validation.\nPredictive Accuracy: Achieving high predictive accuracy required extensive real-world data and multiple retraining cycles.\nThe Jargon Trap: Simplifying technical financial concepts while maintaining accuracy proved consistently challenging.\n\nAccomplishments: Market Validation: Successfully launched a multi-language Beta Pilot Program, proving viability, stability, and global scalability.\nInvestment Readiness: Achieved External Audit readiness and built Advanced Visualization to turn complex ML outputs into actionable insights.\n\nWhat we learned: FinTech innovation must be validated and secure before launch. Through the Pilot Program and External Auditing, we proved that our GenAI-driven personalization is not only effective but stable and ready for regulatory scrutiny‚Äîdemonstrating true market readiness.\n\nWhat's next: Seed Funding & Full Regulatory Filing: Leverage audit results and pilot data to raise seed capital and pursue formal regulatory approval for real-money transactions.\nStrategic Partnerships: Target global banks and financial institutions to deploy our compliance-ready, multi-language platform.\nAdvanced Psychometrics: Expand behavioral-finance coaching with deeper psychometric testing to further personalize user pathways.",
    "hackathon": null,
    "prize": "Winner [T. Rowe Price] Investor Education Challenge",
    "techStack": "alpha-vantage-api, big-data, cloudflare, css4, financial-modeling-prep-api, gemini, i18, llms, nextjs, pnpm, python, react19, reacthook, recharts, sec-edgar-api, tailwind, typescript, vultr",
    "github": "https://github.com/amansahu205/Technica-2025",
    "youtube": "https://www.youtube.com/watch?v=lZ9JGw4fMuU",
    "demo": "https://75ae0832.technica-2025.pages.dev/login/",
    "team": "Aman Sahu, Dhanush P Sukruth, Supriya Kadam, Nikhil Mulgir",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/ohmyfunds-xyz"
  },
  {
    "title": "It's Getting Fishy...",
    "summary": "\"It's Getting Fishy...\" is an innovative project that likely combines music and interactive gameplay, utilizing Ableton for audio production, Lua for scripting, and Playdate as the gaming platform. The project aims to create a unique experience that engages users through audio-visual interactions, possibly involving rhythm or sound-based challenges.\n\nKEY ACHIEVEMENTS\nThe project stood out in the hackathon by creatively integrating music and gaming in a way that resonates with users, showcasing originality and technical skill. Winning 1st place indicates that the judges recognized its potential for user engagement and entertainment, as well as the team's effective execution of their concept.\n\nNotable technical implementations likely include the use of Ableton for dynamic audio generation, Lua for efficient game logic and interactive elements, and Playdate's unique hardware capabilities to enhance user interaction. The combination of these tools suggests a seamless integration of sound and gameplay, allowing for real-time audio manipulation based on player actions.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges such as ensuring smooth synchronization between audio and gameplay mechanics, optimizing performance on the Playdate platform, and addressing any limitations of the tools used. Additionally, creating an engaging user experience that balances complexity and accessibility could have posed difficulties.\n\nFUTURE POTENTIAL\nThis project has significant potential for evolution, including expanding its gameplay mechanics, incorporating user-generated content, or developing additional levels or challenges. Future iterations could explore further integration with different music genres or collaborative multiplayer features, potentially leading to a broader audience and increased replayability.\n\nInspiration: We wanted to take advantadge of the unique crank mechanism in the Playdate. While thinking of different ideas, we decided to go with a fishing game, as we thought it would be fun with the crank and very intuitive.\n\nChallenges: None of us had touched Lua before, so it was quite the learning curve!\nAt first, we couldn't load the game into the console! We realized the console was running an older software version, so we had to find a way to update the software.\nOur game was having very poor performance when running on actual hardware, we had to go back and refactor some of the code to make it more smooth and performant\n\nAccomplishments: We made everything ourselves in a very short amount of time. The code, the music, the art, the sound effects. All from us.\n\nWhat we learned: We learned how to make games, a very different approach to development as what we are all accustomed to. \nWe learned to code with specific hardware in mind, being aware of its limitations, its constraints, etc\nWe used our foundations of programming to quickly learn a new language (Lua) and apply it to a challenge.\n\nWhat's next: We want to see how we can release the game to the public! We will look into releasing the game on itch.io and seeing if other Playdate owners can play it on their consoles!",
    "hackathon": null,
    "prize": "Winner 1st Place",
    "techStack": "ableton, lua, playdate",
    "github": "https://github.com/santipadron/codejam15",
    "youtube": "https://www.youtube.com/watch?v=0IwvcU246H8",
    "demo": null,
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/it-s-getting-fishy"
  },
  {
    "title": "Arbitrage",
    "summary": "Arbitrage is a project designed to facilitate real-time market transactions by leveraging arbitrage opportunities across different platforms. It utilizes advanced technologies to simulate an agent marketplace, optimizing trade decisions and maximizing profit margins through efficient data analysis and execution.\n\nKEY ACHIEVEMENTS\nThe project distinguished itself by winning multiple prestigious awards, including the overall hackathon winner and specific accolades for excellence in the Visa Challenge and Best Use of Qualcomm EdgeAI. These achievements reflect its innovative approach, effective use of technology, and potential market impact.\n\nArbitrage is built using a robust tech stack that includes Docker for containerization, FastAPI for high-performance web service development, and PostgreSQL for reliable database management. The project also incorporates advanced tools like lm-studio and openrouter for machine learning and data processing, as well as TypeScript and React for a responsive front-end experience. The use of WebSockets allows for real-time data updates, enhancing user interaction and trade responsiveness.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating multiple technologies seamlessly, ensuring the accuracy and speed of data processing, and managing the complexities of real-time market fluctuations. Additionally, maintaining security and compliance in financial transactions could have posed significant hurdles.\n\nFUTURE POTENTIAL\nArbitrage has the potential to evolve into a comprehensive trading platform that could incorporate machine learning algorithms for predictive analytics, expanding its capabilities to include more complex trading strategies. Future iterations could also explore partnerships with financial institutions, enhancing its market reach and functionality. Additionally, integrating AI for automated trading could further streamline operations and improve user experience.\n\nInspiration: The project began with a simple question: what happens when negotiation is treated as an conversational process rather than a numerical optimization problem?\nMarket simulations typically rely on scoring functions, game-theoretic formulas, or engineered heuristics. Real human bargaining rarely works that way. Sellers hide motives, buyers infer intentions, and decisions emerge from dialogue, not equations. This gap‚Äîbetween real negotiation and algorithmic negotiation‚Äîinspired the creation of Arbitrage, a system where autonomous agents engage in realistic, chat-based bargaining driven entirely by LLM reasoning.",
    "hackathon": null,
    "prize": "Winner Visa Challenge - Agent Marketplace Simulation; Winner Best Use of Qualcomm EdgeAI",
    "techStack": "docker, fastapi, git, lm-studio, openrouter, postgresql, python, react, sqlalchemy, typescript, websockets",
    "github": "https://github.com/rohan-g0re/Hack_NYU/tree/main",
    "youtube": "https://www.youtube.com/watch?v=z2_8AWR_qU8",
    "demo": null,
    "team": "Pradeep Kulkarni, Sahil Sarnaik, Rohan Gore",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/multi-agent-marketplace-simulator"
  },
  {
    "title": "RotLocker",
    "summary": "RotLocker Project Summary\n\nRotLocker is an innovative application designed to enhance user security by managing and rotating sensitive credentials automatically. It simplifies the process of storing and updating passwords, ensuring that users maintain secure access to their accounts without the hassle of manual updates.\n\nKEY ACHIEVEMENTS: The project distinguished itself by winning the \"Best Domain Name\" prize from GoDaddy Registry, indicating a strong branding and user engagement strategy. Its focus on automating credential management addresses a critical need in cybersecurity, contributing to its recognition as a hackathon winner.\n\nBuilt using Android Studio and Kotlin, RotLocker leverages modern mobile development techniques for an intuitive user interface and experience. The integration of Claude CLI enhances its functionality, potentially allowing for seamless command-line interactions and automated processes, which could improve user efficiency.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges such as ensuring the security of sensitive information, managing user data privacy, and developing a robust backend to support automated credential rotation. Additionally, achieving a user-friendly interface while maintaining strong security measures may have posed design complexities.\n\nFUTURE POTENTIAL: RotLocker could evolve into a comprehensive personal security suite by incorporating features like biometric authentication, multi-factor authentication, and integration with various online services. Expansion into enterprise solutions could also be considered, allowing businesses to manage employee credentials securely and efficiently.\n\nInspiration: Device addiction has quietly but steadily crept into our daily lives‚Äîand it‚Äôs only getting worse. As of 2025, the average American spends over 5 hours and 16 minutes on their phone each day. This growing dependence is closely linked to rising rates of depression, anxiety, and declining physical and emotional well-being. Looking to proven strategies for overcoming addiction, we turned to one of the most effective tools: accountability. When people support each other, meaningful change becomes possible. RotLocker is built on that principle.\n\nWhat's next: Right now, RotLocker is functional but early in its development. The UI is still being refined, and the app currently works only on Android. In the coming months, we plan to polish the design and bring RotLocker to iOS as well.",
    "hackathon": null,
    "prize": "Winner [MLH] Best Domain Name from GoDaddy Registry",
    "techStack": "android-studio, claude-cli, github, kotlin",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=7gBWeZ2DXOc",
    "demo": "https://block-the-rot-for.us/",
    "team": "Ilya Shchelokov, qwack",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/rotlocker"
  },
  {
    "title": "./space_re:ACT",
    "summary": "The project ./space_re:ACT is an immersive technology experience that leverages interactive storytelling within a 3D environment. It likely combines elements of virtual reality or augmented reality to engage users, allowing them to explore and interact with a digital universe in innovative ways.\n\nKEY ACHIEVEMENTS\n./space_re:ACT stood out by winning multiple prestigious awards, including the Creative Immersive Technology 1st Place Prize and Best Business Potential. These accolades suggest that the project not only showcased creativity and technical prowess but also demonstrated a viable business model that appeals to potential investors and stakeholders.\n\nThe project utilized a blend of advanced technologies including Blender for 3D modeling, Unity for game development, and C# for programming, ensuring a robust and interactive user experience. Additionally, ElevenLabs likely contributed to voice synthesis or AI-driven interactions, enhancing the immersive quality of the experience.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to integrating various technologies seamlessly, ensuring user engagement, and optimizing performance across different devices. Additionally, balancing technical complexity with usability to create an accessible experience for a wide audience could have posed significant hurdles.\n\nFUTURE POTENTIAL\n./space_re:ACT could evolve into a broader platform offering customizable immersive experiences for various applications, such as education, training, or entertainment. There is potential for expansion with additional content, features that leverage AI for personalized experiences, and partnerships with businesses looking to utilize immersive technology for marketing or training purposes.\n\nWhat it does: ./space_re:ACT is an immersive, interactive, and intuitive game that tests a player's ability to react to verbal and visual instructions in a high-pressured environment. Their task to push buttons and pull levers  to successfully reach the destination of the Space Mission. Tasks become increasingly difficult, with more colors and directions to track and memorize. Failure to accurately maintain the ship exposes the player to dangerous debris, such as meteorites, that collides with the ship and ultimately ends the game.\n\nInspiration: As avid VR/AR game players, athletes, and CS students, we saw VR has the potential to not only entertain, but the train. While current VR applications exist to help develop and maintain coordination and quick reactions, most on the market serve little purpose other than sight-seeing. Those that serve as clinical tools lack engagement, and frame recovery as a chore, not self-improvement.\n\nHow it was built: In the front-end, we built most of the models and assets using Blender. We personally model everything from scratch. For the game engine, we are using Unity with the XR toolkit. While importing all the models and assets into Unity, we use C# to implement the back end, creating physics, buttons, and levers. As a four-person team, we are using GitHub Desktop to collaborate and achieve vision control.\n\nAccomplishments: We actually finished the project in 42 hours! We implement everything from scratch, and as we mentioned before, we encountered many challenges. However, we created a cool VR game in just 42 hours.\n\nWhat we learned: Problems Solving Skills\nCollaboration with a new team\nUnity\nC#\nModeling in Blender\n## What's next for ./space_re:ACT\nModes/difficulty levels to help players GROW\ndifferent modes/versions for different targets - one more on memory, one more on speed\nmore missions to increase immersive engagement and curiosity\nsome tasks outside of the Rocketship\nmore interactable objects in the Rocketship",
    "hackathon": null,
    "prize": "Winner Creative Immersive Technology 1st Place Prize; Winner Best Business Potential - S",
    "techStack": "blender, c#, elevenlabs, github, unity",
    "github": "https://github.com/rayyyrayy/DandyHacks-SpaceSimulator",
    "youtube": "https://www.youtube.com/watch?v=izFmv4YLVLk",
    "demo": null,
    "team": "Earvin Chen, Daniel Lin, Rain Xia",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/roket"
  },
  {
    "title": "US Law Reference",
    "summary": "US Law Reference is a mobile application designed to provide users with easy access to legal information and resources related to U.S. laws. By utilizing a user-friendly interface, the app aims to empower individuals to better understand their legal rights and navigate the complexities of the legal system.\n\nKEY ACHIEVEMENTS\nThe project stood out due to its practical utility and user-centric design, which effectively addresses a common need for accessible legal information. Its recognition as a winner in multiple categories at the hackathon highlights its innovative approach and the team's ability to deliver a functional and impactful solution.\n\nBuilt using Dart and Flutter, the project features a sleek and responsive user interface that enhances user experience across various devices. The integration of SQLite for local data storage ensures quick access to legal references and resources, allowing for offline functionality and improved performance.\n\nPOTENTIAL CHALLENGES\nThe team likely faced challenges related to ensuring the accuracy and comprehensiveness of legal information, given the complexity and variability of laws across different jurisdictions. Additionally, they may have encountered difficulties in designing an intuitive navigation system that caters to users with varying levels of legal knowledge.\n\nFUTURE POTENTIAL\nThe project could evolve by incorporating features such as personalized legal advice through AI-driven chatbots, real-time updates on legal changes, and user-generated content for community support. Expanding to include resources for legal professionals and integrating with existing legal databases could significantly enhance its value and reach.\n\nWhat it does: US Law Reference provides an offline, searchable library of the U.S. Code, the Constitution, and DMV practice tests. Users can browse titles, read full legal sections, study amendments, take quizzes, and track their progress‚Äîall in one app.\n\nInspiration: Students often struggle to access U.S. legal information in a simple, organized way. Existing apps are cluttered, online-only, or incomplete. I wanted a fast, offline tool that makes learning U.S. laws and DMV rules easy for everyone.\n\nHow it was built: I built the app using Flutter and Dart, with a local SQLite database for storing law sections, constitutional text, quiz history, and DMV questions. The UI uses Material Design 3 and follows a clean MVVM structure. All legal content is embedded for offline access.\n\nChallenges: Extracting and formatting real U.S. legal text into a structured database\nManaging large datasets efficiently on-device\nFixing quiz randomization bugs and answer duplication\nDesigning a clean UI that still handles long legal sections\nEnsuring good performance without requiring internet access\n\nAccomplishments: Fully offline access to 38 U.S. Code titles and the entire Constitution\nA clean, intuitive UI that makes dense legal text easier to read\nA complete DMV practice test system with progress tracking\nSmooth search functionality across all legal sections\nSuccessfully publishing a working APK usable on real Android devices\n\nWhat we learned: How to structure and optimize large SQLite datasets in Flutter\nHow to manage state and navigation in a growing mobile app\nHow to extract, clean, and format public legal data\nThe importance of UI/UX when displaying long-form text\nHow to design quizzes, timers, and statistics tracking systems\n\nWhat's next: Expanding to all 54 U.S. Code titles\nAdding case law summaries and examples for easier learning\nAdding bookmarks, favorites, and note-taking\nImproving search with filters and keyword highlighting\nBringing the app to iOS\nPublishing to Google Play for easy installation",
    "hackathon": null,
    "prize": "Winner Participation Prize; Winner Top 24-85",
    "techStack": "dart, flutter, sqlite",
    "github": "https://github.com/AndrewTr0612/USLawReferenceAndroidApp",
    "youtube": "https://www.youtube.com/watch?v=eml56tZ-PgA",
    "demo": null,
    "team": null,
    "date": "2025-11-17",
    "projectUrl": "https://devpost.com/software/us-law-reference"
  },
  {
    "title": "Pomodoro Park Casino - Productivity Paradise",
    "summary": "Pomodoro Park Casino - Productivity Paradise\n\nPomodoro Park Casino is a unique productivity application that gamifies the Pomodoro Technique, allowing users to manage their time effectively while engaging in a themed virtual casino environment. Users earn rewards and unlock features as they complete tasks, blending productivity with an enjoyable gaming experience.\n\nKEY ACHIEVEMENTS: The project stood out for its innovative approach to enhancing productivity through gamification. It effectively merged the structured focus of the Pomodoro Technique with a fun, engaging interface, which resonated with users and judges alike, securing recognition as both a winner and finalist.\n\nBuilt using React and TypeScript with Vite for fast development and performance, the project likely showcases responsive design, real-time task tracking, and interactive gaming elements. The use of TypeScript would enhance code quality and maintainability, while Vite's rapid build times would contribute to a smooth development experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in balancing productivity features with gaming elements to ensure neither aspect detracted from the other. Additionally, creating a seamless user experience that is both intuitive and engaging might have required extensive user testing and iteration.\n\nFUTURE POTENTIAL: This project has the potential to evolve into a comprehensive productivity suite, integrating features like collaborative task management, personalized productivity analytics, and community challenges. Further development could include mobile app implementation and integration with popular productivity tools to widen its user base and enhance functionality.\n\nWhat it does: Pomodoro Park is a gamified study timer that blends the productivity of the Pomodoro Technique with the fun of a casino. Here's how it works: Set Your Session: The user inputs their desired study time (e.g., 2 hours), a study block duration (e.g., 25 minutes), and a base break time (e.g., 5 minutes).\nStudy Mode: The app runs the study timer. The user focuses, knowing a unique break is coming.\nBreak Time!: When the study timer hits zero, the break timer begins, and the user is presented with the \"Pomodoro Park\" casino.\nPlace Your Bets: The user can choose to play one of three casino games: Blackjack, Roulette, or a Slot Machine.\nWin or Lose: The outcome of the game directly impacts their break! A win might add time to the break, while a loss will shorten the break. (Or, they can just take t\n\nInspiration: The Pomodoro Technique is a time management method that uses a timer to break down work into focused intervals, traditionally 25 minutes in length, separated by short breaks. While it is a proven productivity powerhouse, but we've always felt the break component was boring. The standard 5 minute break leads endless doomscrolling on your phone too often. So, we asked ourselves: how could we make the break feel like a real reward? We wanted to build a system where you not only earned your break, but also holds you accountable. When your break timer ends, so does your ability to play at the Pomodoro Park casino.  The idea of a high-energy, fun \"casino\" where you could \"gamble\" for more break time lead to the creation of \"Pomodoro Park.\"\n\nHow it was built: This project is built with: Vite\nTypeScript\nReact\nshadcn-ui\nTailwind CSS\n\nChallenges: Our biggest challenge was scope and time management. We were passionate about the idea and initially wanted to add even more games and features. We had to focus and cut back to what was achievable.\n\nAccomplishments: We're also proud of the UI polish. We created an app that is clean, intuitive, and, most importantly, fun to use. The overall look of the application feels smooth and rewarding.\n\nWhat's next: More Games: We'd love to add more quick, casino games, to provide users with maximum enjoyment of the casino.\nUser Accounts & Stats: We plan to add a proper backend to allow users to create accounts, track their study hours, and see stats on their game winnings.\nCustomization: More themes! Imagine a customizable casino where you can choose the background, colours, etc., to your liking.",
    "hackathon": null,
    "prize": "Winner Finalist",
    "techStack": "react, typescript, vite",
    "github": "https://github.com/jamesjhjung/study-break-casino.git",
    "youtube": "https://www.youtube.com/watch?v=KX9xhb2YUuE",
    "demo": "https://pomodoro-casino.lovable.app/",
    "team": "James Jung, Lester Cheng, Jacky Zhong, Denis Chen",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/productivity-paradise"
  },
  {
    "title": "CrisisShield",
    "summary": "CrisisShield Project Summary\n\nCrisisShield is a web application designed to provide real-time information and resources during emergencies and crises. It aims to facilitate communication and coordination among individuals and organizations affected by disasters, ensuring timely access to critical support and updates.\n\nKEY ACHIEVEMENTS: The project stood out by effectively integrating AI technology to analyze and disseminate relevant crisis information quickly. Its user-friendly interface built with Bootstrap and React, as well as its successful demonstration of real-time data handling, contributed to its recognition as a hackathon winner.\n\nNotable technical implementations include the use of TypeScript for enhanced code reliability and maintainability, and Next.js for optimized server-side rendering and performance. The AI component likely leverages machine learning algorithms to process and interpret data from various sources, providing users with actionable insights.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to data accuracy and timeliness, as information in crisis situations can change rapidly. Additionally, ensuring the platform's scalability to handle potentially high traffic during emergencies, as well as integrating diverse data sources, may have posed technical hurdles.\n\nFUTURE POTENTIAL: CrisisShield could evolve into a comprehensive platform by incorporating features such as user-generated reports, geolocation services for personalized alerts, and partnerships with emergency response organizations. Expanding its reach to include multilingual support and accessibility features could enhance its usability across diverse communities, making it an essential tool in crisis management.\n\nWhat it does: CrisisShield is a comprehensive AI-powered crisis management platform that helps small businesses predict, prepare for, and recover from disasters. Core Capabilities: AI Threat Prediction: Analyzes real-time weather patterns, economic indicators, and regional risks to provide personalized threat assessments with probability scores and early warnings\nIntelligent Emergency Planning: Generates customized crisis response plans in seconds using AI, tailored to specific business types, locations, and situations with step-by-step actions\nRecovery Tracking: Guides businesses through 6 recovery stages with visual progress monitoring, milestone tracking, and operational capacity metrics\nCrisis Management: Provides real-time AI guidance during active emergencies with context-aware recommendations and\n\nInspiration: Small businesses are the backbone of economies worldwide, yet they're the most vulnerable when disaster strikes. We were inspired by the devastating reality that 90% of small businesses in developing countries have no disaster preparedness plan, and 60% never reopen after a crisis. We saw families losing their life's work to floods, pandemics, and economic crashes‚Äînot because they didn't care, but because enterprise-grade crisis management tools were too expensive, too complex, and simply not designed for them. We wanted to change that. CrisisShield was born from a simple belief: every business deserves a fighting chance. Crisis preparedness shouldn't be a luxury reserved for large corporations. With AI and modern cloud technology, we can democratize access to life-saving tools and level t\n\nHow it was built: We built CrisisShield using a modern, scalable tech stack designed for speed, reliability, and real-time performance: Frontend & Framework: Next.js 15 with React 18 and TypeScript for type-safe, server-side rendered pages\nReact Bootstrap and Bootstrap 5 for responsive, accessible UI components\nCustom CSS for brand-specific styling and animations AI & Intelligence: Cerebras Cloud SDK for lightning-fast AI processing and threat analysis\nCustom AI prompts for emergency plan generation and crisis guidance\nReal-time data aggregation from multiple threat intelligence sources Backend & Database: Supabase (PostgreSQL) for real-time database with row-level security\nCustom database schema with 8+ interconnected tables\nOptimized queries for fast threat detection and plan retrieval Authentication & Se\n\nChallenges: 1. AI Response Speed vs. Quality Challenge: Balancing fast AI responses with comprehensive, actionable plans\nSolution: Implemented Cerebras for ultra-fast inference while optimizing prompts for quality output 2. Real-time Threat Data Integration Challenge: Aggregating data from multiple sources with different formats and reliability levels\nSolution: Built a flexible data aggregation layer with fallback mechanisms and confidence scoring 3. Database Schema Complexity Challenge: Designing a schema that handles multiple crisis types, recovery stages, and business profiles\nSolution: Created a normalized schema with flexible JSON fields for extensibility while maintaining query performance 4. User Experience During Crisis Challenge: Making the interface simple enough to use during high-stress em\n\nAccomplishments: Technical Achievements: ‚úÖ Built a full-stack, production-ready application in hackathon timeframe\n‚úÖ Integrated cutting-edge AI (Cerebras) for real-time threat analysis and plan generation\n‚úÖ Implemented 8+ major features with real-time database synchronization\n‚úÖ Created a scalable architecture that can handle millions of users\n‚úÖ Achieved sub-second AI response times for emergency plan generation\n‚úÖ Designed and implemented a complex database schema with proper relationships User Experience: ‚úÖ Created an intuitive interface that works during high-stress situations\n‚úÖ Built mobile-responsive designs that work on any device\n‚úÖ Implemented accessibility features for inclusive design\n‚úÖ Designed clear visual progress tracking for recovery stages Impact Potential: ‚úÖ Addressed a real problem affecting\n\nWhat we learned: Technical Learnings: How to integrate and optimize AI models for real-time applications\nBest practices for real-time database design with Supabase\nEffective patterns for Next.js 15 server and client components\nStrategies for handling complex state management in crisis scenarios\nTechniques for building responsive dashboards with Bootstrap Product Learnings: The importance of simplicity during high-stress situations\nHow to balance feature richness with usability\nThe value of visual progress indicators for user motivation\nThe need for offline capabilities in disaster scenarios (future work) Domain Learnings: Deep understanding of crisis management workflows\nThe specific needs of small businesses in developing regions\nThe complexity of disaster recovery processes\nThe importance of funding acce",
    "hackathon": null,
    "prize": "Winner Finalist Certificate + Custom .xyz Domain",
    "techStack": "ai, bootstrap, next, react, typescript",
    "github": "https://github.com/AqilaRifti/CrisisShield",
    "youtube": "https://www.youtube.com/watch?v=oJmFA3dTxM0",
    "demo": "https://crisisshield.netlify.app/",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/crisisshield"
  },
  {
    "title": "SynapseHub",
    "summary": "SynapseHub Project Summary\n\nSynapseHub is an innovative platform designed to facilitate seamless data integration and collaboration among users, leveraging real-time data from Google Spreadsheets. It aims to enhance productivity by providing a cohesive environment where users can manage and analyze data effectively, supported by advanced AI functionalities.\n\nKEY ACHIEVEMENTS: The project stood out by combining user-friendly interfaces with powerful backend functionalities, resulting in a robust tool that enhances data-driven decision-making. Its recognition as a winner in multiple categories reflects its impactful design and utility in addressing common data management challenges.\n\nSynapseHub employs a diverse tech stack, including Flask for backend development, Llama-3.3-70b for advanced AI processing, and Google Spreadsheets for dynamic data management. The integration of SMTP for communication and SQLite for local data storage further strengthens its functionality, while the responsive design built with HTML5, CSS3, and JavaScript ensures a smooth user experience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges in ensuring data security and privacy, especially when integrating with external services like Google Spreadsheets. Additionally, optimizing the performance of real-time data processing and managing the complexity of AI implementations could have posed significant hurdles.\n\nFUTURE POTENTIAL: SynapseHub has the potential to evolve into a comprehensive data collaboration platform, integrating more data sources and expanding its AI capabilities. Future iterations could include enhanced analytics features, support for more file formats, and collaborative tools that foster teamwork, positioning it as a go-to solution for data-driven organizations.\n\nWhat it does: SynapseHub is a comprehensive platform where student entrepreneurs can transform ideas into reality through visibility, collaboration, and expert guidance.\n\nInspiration: I'm a 15-year-old student entrepreneur. I've experienced the frustration firsthand: üí≠ Hidden Potential: Ideas remain trapped in classrooms and small friend groups\nüîá Zero Visibility: No platform to reach beyond immediate circles\n‚ùå No Validation: Students can't tell if their ideas are viable or needed\nüë• Isolation: Finding co-founders with complementary skills is nearly impossible\nüìö Learning Gap: No structured way to develop entrepreneurship skills\nüòû Lost Motivation: Without feedback and support, brilliant concepts die silently The truth is simple: 90% of student startup ideas fail not because they‚Äôre bad, but because there‚Äôs no infrastructure to nurture them. We‚Äôre told to ‚Äúinnovate,‚Äù yet given no tools to actually do it.\n\nChallenges: Styling Consistency: Fast development caused spacing and color mismatches; fixed in final hours\nTime Management: Prioritized core features over a full mentorship module\nResponsive Layout: With limited time, we focused on desktop stability; mobile required more restructuring\nFiltering Bugs: Filters weren‚Äôt updating dynamically ‚Üí JS rewrite\nRouting Conflicts: Flask template paths broke and required refactoring\nTime Pressure: We dropped several planned features to ship a solid MVP\n\nWhat's next: ‚óè Move from SQLite to PostgreSQL for larger data sets.\n‚óè Implement real-time chat using Flask-SocketIO for team collaboration.\n‚óè Deploy AI-powered idea matching to connect students with similar interests.\n‚óè Mobile-first design ‚Üí native Flutter app in phase 2.\n‚óè Long-term ‚Üí Partnership with edtech companies and incubators for funding &\ninternships.",
    "hackathon": null,
    "prize": "Winner Participation Prize; Winner Top 4-23",
    "techStack": "css3, flask, google-spreadsheets, groq, html5, javascript, llama-3.3-70b, python, smtp, sqlite, vs-code",
    "github": "https://github.com/aditya3272-arya/SynapseHub-Student-Entrepreneur-Platform/tree/main",
    "youtube": "https://www.youtube.com/watch?v=AQj3iniSYs8",
    "demo": "https://docs.google.com/document/d/1Ps2Ec83H0Q0rgds43Tf3m43ixBJ5lRnfdVX2XJGVA-k/edit?usp=sharing",
    "team": null,
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/synapsehub-w2gl6z"
  },
  {
    "title": "Pancake Pals",
    "summary": "Pancake Pals Project Summary\n\nPancake Pals is a mobile application designed to enhance social interactions through cooking, specifically focusing on pancake-making. Users can share recipes, cooking tips, and even host virtual pancake parties, fostering a sense of community around a fun and engaging activity.\n\nKEY ACHIEVEMENTS: The project stood out by combining a unique social element with a popular cooking theme, appealing to both food enthusiasts and those looking for social engagement. Winning both the overall hackathon and the \"Best Mobile Hack\" category indicates exceptional execution and innovative features that resonated with judges and users alike.\n\nBuilt with Dart and Flutter, Pancake Pals likely showcases smooth cross-platform functionality, allowing for a seamless user experience on both iOS and Android devices. Key implementations may include real-time communication features for virtual gatherings and an intuitive interface for recipe sharing and exploration.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to user engagement and retention, ensuring that the app remains appealing over time. Technical difficulties in real-time collaboration features and optimizing performance for various devices could have also posed hurdles during development.\n\nFUTURE POTENTIAL: Pancake Pals could evolve into a broader culinary platform, expanding beyond pancakes to include various cooking themes and community-driven events. Future iterations might incorporate gamification elements, partnerships with culinary influencers, or integration with smart kitchen devices to enhance the cooking experience further.\n\nWhat it does: Pancake Pals tracks your screen time and turns it into a daily challenge. The less time you spend on your phone, the more breakfast points you earn. You can level up your pancake buddy, check your friends‚Äô progress, and compete to stay off your phone the longest.\n\nInspiration: We wanted to make screen time fun instead of guilty. Everyone has tried to ‚Äúuse their phone less‚Äù and failed ‚Äî so we thought, why not turn it into a game? Breakfast characters, competition with friends, and a cozy vibe felt like the perfect mix to motivate people without pressure.\n\nHow it was built: We built the app using Flutter + Dart so it could run cross-platform. We used Android Usage Stats to read screen time, and Firebase for storing user progress and friend leaderboard data. All UI assets and icons were custom-designed to match the breakfast theme.\n\nChallenges: It was our first time working with flutter and dart, so it took us longer than expected to get set up with it. Screen time permissions were way harder than expected ‚Äî dealing with Android SDK versions, NDK errors, and manifest permissions took a long time. Syncing friend data without lag and getting the emulator to cooperate were also big battles.\n\nAccomplishments: We got real screen-time tracking working and fully connected it with the reward system and characters. The UI turned out super cozy and satisfying, and people who tested the app said it actually motivated them to stay off their phones.\n\nWhat we learned: We learned a ton about Android permission APIs, Flutter build configurations, and debugging emulator chaos. We also learned that small UI details make a huge difference in user motivation ‚Äî a cute pancake really does make you use your phone less.\n\nWhat's next: We want to add more breakfast characters, weekly tournaments, and streak bonuses. Long term, we‚Äôd love to launch on both Android and iOS and maybe bring in features like calming mini-games or phone-free quests.",
    "hackathon": null,
    "prize": "Winner Best Mobile Hack",
    "techStack": "dart, flutter",
    "github": "https://github.com/ashleychching/pancake_pals",
    "youtube": "https://www.youtube.com/watch?v=OJWKgV2G9w4",
    "demo": "https://www.figma.com/proto/IIV5EcP56f20XcrtNjz9Pe/Pancake-Pals?node-id=1-4&t=fINy9LKeaYhgkVtP-0&scaling=scale-down&content-scaling=fixed&page-id=0%3A1&starting-point-node-id=1%3A4",
    "team": "Ashley Chan, Eddie Matthews, Yasmin",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/pancake-pals"
  },
  {
    "title": "Night Shift",
    "summary": "Night Shift Hackathon Project Summary\n\nNight Shift is an innovative project that leverages reinforcement learning to create a dynamic environment for training AI agents, likely focused on optimizing decision-making in complex scenarios. The project aims to simulate nighttime operations, potentially in areas like logistics or gaming, where intelligent navigation and task management are crucial.\n\nKEY ACHIEVEMENTS: The project stood out by winning both the overall hackathon and the Best Machine Learning Hack award, indicating a strong execution and innovative use of machine learning techniques. Its success may stem from a well-implemented reinforcement learning model that demonstrated significant capability in real-time problem-solving.\n\nNight Shift utilized advanced libraries such as Gymnasium for creating the training environment, NumPy for numerical operations, and Stable-Baselines3 for implementing state-of-the-art reinforcement learning algorithms. The integration of Pygame likely provided an engaging visual interface for the simulation, enhancing user interaction and feedback.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges around optimizing the reinforcement learning algorithms for efficiency, balancing exploration versus exploitation in training, and ensuring the simulation was both realistic and scalable. Additionally, debugging and fine-tuning the interaction between the various libraries could have presented technical hurdles.\n\nFUTURE POTENTIAL: Night Shift has significant room for evolution, including expanding its application to various industries such as autonomous vehicles, emergency response systems, or even gaming. Future enhancements could involve integrating more complex environments, improving the user interface, and incorporating real-world data to refine the AI's decision-making capabilities. Additionally, collaborative features could be introduced, allowing multiple agents to operate within the same environment, thereby simulating real-world teamwork scenarios.\n\nWhat it does: Night Shift is a \"Digital Twin\" of an ICU in crisis. It has two modes: \"Retro\" Mode: A top-down Pygame simulator where you, the human, try to manage the ICU. You must manually allocate nurses, beds, and ventilators to save patients as their severity increases.\n \"Modern\" AI Mode: We run the exact same simulation, but this time it's controlled by an AI we trained using reinforcement learning. It acts as a Decision Support System, executing the optimal strategy it learned from millions of simulations to save the maximum number of lives. It's a \"flight simulator for nurses\" that doubles as a powerful planning tool for hospital administrators.\n\nInspiration: We were inspired by the intense, high-stakes decisions that hospital staff make every single night. A charge nurse on the \"Night Shift\" is constantly playing a game of high-stakes triage: who gets the last ventilator? Who gets the only available nurse? This human-driven optimization is incredibly difficult. We wanted to see if a modern AI, trained on millions of scenarios, could discover optimal, life-saving strategies that even the most experienced human might miss.\n\nHow it was built: The project is built entirely in Python with two main components: The Simulation Engine: A core SimICU class that models all the logic: patient arrivals (Poisson process), severity degradation, and resource constraints.\nThe AI Environment: We wrapped our engine in a Gymnasium-compatible environment. This included a normalized observation_space (patient severity, wait times, free resources) and a MultiDiscrete action space (patient_id, action_type).\nThe AI Agent: We used Stable-Baselines3 to train a Proximal Policy Optimization (PPO) agent on our environment.\nThe \"Retro\" UI: We used Pygame to build the human-playable \"Retro\" game, which calls the same underlying simulation engine.\n\nChallenges: The AI did not work. At all. Our first agent had a 1.1% success rate‚Äîit was acting randomly and everyone was lost. The biggest challenge was debugging the environment (the teacher), not the agent (the student). State Normalization: The AI saw numbers like tick=1000 and free_beds=5 and thought the tick count was 200x more important. We had to normalize all state inputs to a [0, 1] range so it could understand the relative importance of each factor.\nReward Shaping: A single +100 reward for a save wasn't enough; the AI didn't know which of its 500 actions led to the win. We had to add frequent, small penalties for every tick a patient waited, teaching it a sense of urgency.\nInvalid Actions: The agent wasted thousands of steps trying to act on non-existent patients. We had to add large, immedi\n\nAccomplishments: We're incredibly proud of turning that 1.1% success rate into a high-performing agent. Debugging an RL environment is notoriously difficult, and we successfully navigated state normalization and complex reward shaping. Most importantly, we're proud of building a project that is more than a \"game\"‚Äîit's a high-fidelity \"Digital Twin\" that can be directly applied to a real-world healthcare problem.\n\nWhat we learned: We learned that \"DigitalTwins\" are incredibly powerful. Our \"Retro\" sim isn't just a game; it's a baseline for human performance. Our \"Modern\" AI isn't just an opponent; it's a Decision Support System. By building this, we learned how to model a complex, real-world system and, more importantly, how to teach an AI to master it. The challenges taught us that the quality of the environment and the clarity of the reward are far more important than the agent's algorithm.\n\nWhat's next: This is just the beginning. The next steps are focused on real-world deployment: Explainable AI (XAI): Build a dashboard that shows why the AI is making its recommendations in plain English. A doctor will never trust a black box, so we need to add transparency.\n Higher Fidelity: Deepen the model by adding patient archetypes (e.g., cardiac vs. respiratory) and specialized resources, making the simulation even more realistic.\n EMR Integration: Create a mock-up showing how Night Shift could plug into a real hospital's EMR system and provide live, actionable recommendations to staff.",
    "hackathon": null,
    "prize": "Winner Best Machine Learning Hack",
    "techStack": "gymnasium, numpy, pygame, python, stable-baselines3",
    "github": null,
    "youtube": "https://www.youtube.com/watch?v=nKaOxXwdDYI",
    "demo": null,
    "team": "Justin Zhang, courteneyS Sit",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/night-shift"
  },
  {
    "title": "AccessiGesture",
    "summary": "AccessiGesture Project Summary\n\nAccessiGesture is a project aimed at enhancing accessibility through gesture recognition technology. By leveraging computer vision techniques, it interprets hand gestures to facilitate interactions for users with disabilities, allowing them to control devices or software applications more intuitively.\n\nKEY ACHIEVEMENTS: The project stood out due to its innovative approach to accessibility, providing a practical solution that addresses a significant need in the community. Winning multiple prizes, including \"Winner\" and \"Future Startup,\" highlights its potential market impact and the judges' recognition of its originality and utility.\n\nAccessiGesture utilizes advanced libraries such as Mediapipe for real-time hand tracking and OpenCV for image processing, ensuring accurate gesture recognition. The integration of PyInstaller allows for easy deployment of the application, while Tkinter provides a user-friendly interface, making the technology accessible to a broader audience.\n\nPOTENTIAL CHALLENGES: The team likely faced challenges related to ensuring the robustness of gesture recognition under varying lighting conditions and backgrounds. Additionally, optimizing the application for different hardware specifications and ensuring ease of use for individuals with varying abilities would have posed significant hurdles.\n\nFUTURE POTENTIAL: AccessiGesture has the opportunity to evolve into a comprehensive accessibility tool, potentially expanding its gesture library and integrating with various platforms and devices. Future developments could also include machine learning enhancements for improved accuracy and adaptability, as well as partnerships with organizations focused on disability advocacy to broaden its reach.\n\nWhat it does: AccessiGesture runs as a lightweight, unobtrusive application that translates a user's hand movements into direct, real-time cursor control. We meticulously designed a set of gestures to be both ergonomic and comprehensive, allowing for the full spectrum of mouse operations. Cursor Movement: An open hand gesture ([1, 1, 1, 1, 1]) puts the system into \"mouse mode.\" The application maps the hand's position within a user-defined \"active zone\" on the screen to the entire desktop, allowing for smooth, 1-to-1 cursor movement.\nLeft Click & Drag: A pinch between the thumb and index finger triggers a \"left-click-down\" event. This was a critical design choice: by holding this pinch, the user can naturally drag files, highlight text, or interact with any drag-and-drop interface. Releasing the pinch i\n\nInspiration: In a world that runs on digital interaction, the mouse and keyboard remain a fundamental barrier for millions. We were inspired by the daily challenge faced by individuals with motor disabilities, arthritis, repetitive strain injuries (RSI), or other conditions that make using a traditional mouse difficult or even painful. We asked ourselves: why should digital accessibility be a luxury, gated behind expensive, specialized hardware? Our project, AccessiGesture, is our answer. It‚Äôs built on the philosophy that accessibility should be built-in, not bolted-on. We envisioned a tool that could transform any standard webcam‚Äîa device already built into most computers‚Äîinto a high-fidelity, intuitive, and ergonomic gesture controller. We wanted to create something that didn't just work, but felt em\n\nHow it was built: The project was built entirely in Python, which served as the \"glue\" for a stack of powerful, specialized libraries, each chosen for a specific purpose to create a seamless, real-time experience. OpenCV: This was our high-speed, low-latency \"eye.\" We used it for the initial camera capture, frame-by-frame processing, and image manipulation. Crucially, OpenCV also served as our visual debugging tool, allowing us to draw the hand landmarks and our program's current \"state\" (e.g., \"PINCH\") directly onto the video feed.\n Google MediaPipe (Hands): This is the core machine-learning engine and the \"brain\" of our recognition. We leveraged its incredibly high-fidelity, pre-trained model, which provides 21 distinct 3D landmarks for the hand in real-time. This level of detail is what allowed us to mov\n\nChallenges: Our development process was a story of hitting walls and finding breakthroughs. Our first challenge was Framework Selection. We initially explored a sophisticated web-based solution using Next.js, drawn to the idea of a modern, browser-based tool. However, we quickly ran into the sandboxed limitations of the web. Gaining the simple, direct, low-latency access to the webcam and, more importantly, the system-level cursor control we needed was a massive, complex hurdle. We made the hard pivot to a native Python application, which proved to be the right decision, offering us the raw power and simplicity we needed. Our primary technical hurdle was Gesture-Clash Resolution. Our first prototypes were a mess of false positives. A \"fist\" might be misread as a \"thumbs-up.\" A hand moving into positio\n\nAccomplishments: This project was a journey of firsts, but a few accomplishments stand out: The Visual Debugger: We are incredibly proud of the real-time diagnostic overlay. By drawing our program's state (gesture_detected, fingers_list) onto the camera feed, we created an essential engineering tool that allowed us to see our logic working (or failing) and iterate with incredible speed.\nThe Standalone App: We didn't just build a script. We built a full desktop application with a robust UI, packaged as a standalone executable. This was a huge step in learning how to deliver a real product to users.\nMastering the Stack: We dove headfirst into a professional-grade development environment, learning to integrate complex libraries like OpenCV and MediaPipe within VS Code.\nThe Ultimate Test: Beating *osu!* We kne\n\nWhat we learned: This project was a masterclass in the architecture of real-time computer vision applications. Stateful Program Design: We learned the critical importance of managing \"state.\" A user's intent is not just in a single frame; it's a process. We had to build logic to track is_pinching vs. was_pinching to distinguish between a discrete \"click\" and a continuous \"drag.\"\n Human-Computer Interaction (HCI): We learned that gesture selection is a deep, complex trade-off. A gesture must be intuitive for the user, ergonomic to hold, and mathematically unambiguous for the machine. This core conflict is the central challenge of HCI design.\n Performance Profiling: We learned that in real-time apps, \"good enough\" code isn't. Every line matters. We learned how to hunt for performance bottlenecks and understa\n\nWhat's next: Trainable Models: Our next big leap is to move from our hard-coded (heuristic) gesture rules to a proper machine-learned model. This would not only make detection more robust but would also allow users to train the app to recognize their own unique set of custom gestures.\nExpanding the Vocabulary: Right now, we only speak \"mouse.\" We want to expand AccessiGesture's features to include keyboard shortcuts (like Ctrl-C, Ctrl-V) and a pop-up virtual keyboard, transforming it from a mouse replacement into a complete, hands-free operating system navigator.",
    "hackathon": null,
    "prize": "Winner Future Startup",
    "techStack": "mediapipe, opencv, pyinstaller, python, tkinter",
    "github": "https://github.com/dlt87/Accesigesture",
    "youtube": "https://www.youtube.com/watch?v=bQuyfCt01F8",
    "demo": null,
    "team": "nathan liou, David Ting, Payton Yu, Alejandro Imperial",
    "date": "2025-11-16",
    "projectUrl": "https://devpost.com/software/accessigesture"
  }
]